<?xml version="1.0" encoding="utf-8"?><items lastModified='1279224670'><item>
<title><![CDATA[In The World: Breath of life]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/itw-ventilator-0715.html</link>
<story_id>15516</story_id>
<featured>1</featured>
<description><![CDATA[Low-cost portable ventilator could be a lifesaver for people in remote locations and for hospitals in the developing world.]]></description>
<postDate>Thu, 15 Jul 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100713112347-1.png</thumbURL>
<smallURL width='140' height='92'>http://web.mit.edu/newsoffice/images/article_images/w140/20100713112347-1.jpg</smallURL>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100713112347-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[A working prototype of the mechanical ventilator developed by MIT students. They are working on a new version designed for easy manufacturability and maintenance.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100713112347-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Graduate student Abdul al Husseini demonstrates the features of the mechanical ventilator he and his fellow students designed.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In India a few years ago, a 30-year-old farmer was admitted to a hospital in the city of Lucknow, and over the next few days developed paralysis that prevented him from being able to breathe on his own. The kind of mechanical ventilator that would be used in most major medical facilities to keep him alive wasn’t available, but the hospital did have a bag-valve mask — a small, inexpensive plastic medical hand pump that could be used to force air carefully into the patient’s lungs. The farmer’s mother and brother took turns, squeezing the device 15 times a minute nonstop for 18 full days and nights, which kept him alive until a mechanical ventilator became available. Thirty days after being admitted to the hospital, the patient recovered enough to be weaned off the ventilator, and two weeks later he was sent home.<br /><br />This is thought to be a record for the length of time a person has been kept alive by manually operated ventilation, and it underscores the need for simple, inexpensive, widely available equipment that could do the job without requiring such extraordinary and heroic efforts. <br /><br />Now a team of students from MIT has devised a better way to keep patients breathing in places that lack standard mechanical ventilators, or during times of emergency such as pandemics or natural disasters, when normal hospital resources may be overextended. They have designed a system that uses the same widely available manual pump — the same type used for the farmer in India. The new system encases the pump in a plastic box with a battery, motor and controls to take the place of the manual compression process.<br /><br />There is a substantial need for such devices in many developing nations, especially in rural areas that have no access to existing ventilator technology. Dr. Jussi Saukkonen of Boston University Medical Center, who originally proposed the concept of the low-cost ventilator and worked with the MIT team, says that “it’s likely there would be millions of cases worldwide” that could benefit from such a device. In addition, a U.S. government study in 2005 found that in a worst-case pandemic scenario, this country alone might need more than 700,000 mechanical ventilators, while only 100,000 are now in use. <br /><br />The kind of ventilators used in modern hospitals can cost up to $30,000, but the newly developed device can be produced for about $100, says Abdul Mohsen Al Husseini, a graduate student in mechanical engineering and one of the students who developed the system. While there are some situations where it can’t perform all the same functions as the more expensive versions, for 98 percent of cases, this simple inexpensive device could do the job, he says.<br /><br />“These manual devices are available everywhere,” Al Husseini says. “Our approach is to adapt them, since they’re already there.”<br /><br /><strong>Simple system</strong><br /><br />The simple system has a curved plastic cam that compresses the device and then releases. It has just three control knobs; these adjust the total volume of air delivered in each breath, the number of breaths per minute, and the ratio of time between inhaling and exhaling.<br /><br />Al Husseini explains that the mechanical system could not only eliminate the need for a person to operate the device manually in an emergency — it could also be safer. “There’s a danger, with manual ventilation, of overpressurizing” the patient’s lungs, which can cause serious damage, he says. The new system includes a gauge that stops the flow before the pressure gets too high.<br /><br />The idea began as a class project in an MIT mechanical engineering course called Precision Machine Design, in which doctors from Boston-area hospitals present problems awaiting solutions, and the students choose which ones to address. A first prototype was developed in that class, and some of the students refined the design and produced a second prototype in a follow-up class, Development of Mechanical Products. They filed for a patent and presented a paper on the system to the Design of Medical Devices Conference in April of this year. Now some of the students are preparing to do further testing and develop the idea so it can be licensed for manufacturing.<br /><br />One of the students, Amelia Servi, traveled to Nicaragua this summer to analyze the need for such a device and how to bring it to market as a real product, as part of her thesis research for her master’s degree.<br /><br />Dr. Augustine Choi, chief of pulmonary and critical care medicine at Brigham and Women's Hospital, who was not involved in this project, says there is an “unmet medical need” for a low-cost ventilator, and if one becomes available “it will have impact throughout the world, especially in parts of the world where regular ventilators are not available.” He adds that a number of low-cost ventilators with different mechanisms have been proposed, so a key issue is to prove the reproducibility and quality assurance of this system.<br /><br />The development of the device was supported in part by the Center for Integration of Medicine and Innovative Technology (which receives some funding from the U.S. Department of Defense). The work in Nicaragua is supported by MIT’s Public Service Center. <br /><br /><em><a href="http://web.mit.edu/newsoffice/topic/in-the-world.html">In The  World</a> is a column that explores the ways members of the MIT  community are developing technology — from the appropriately simple to  the cutting edge — to help meet the needs of communities around the  planet, especially those in the developing world. If you have  suggestions for future columns, please e-mail <a href="mailto:newsoffice@mit.edu">newsoffice@mit.edu</a>.</em><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Protein linked to aging may boost memory and learning ability]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/sirtuins-0714.html</link>
<story_id>15512</story_id>
<featured>0</featured>
<description><![CDATA[Discovery could lead to new drugs to fight Alzheimer’s and other neurological diseases.]]></description>
<postDate>Wed, 14 Jul 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100712145624-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100712145624-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100712145624-1.jpg</fullURL>
<imageCaption><![CDATA[Crystallographic structure of yeast sir2 complexed with ADP and a histone H4 peptide.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='472'>http://web.mit.edu/newsoffice/images/article_images/20100712145624-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[Li-Huei Tsai, director of MIT's Picower Institute for Learning and Memory]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Over the past 20 years, biologists have shown that proteins called sirtuins can slow the aging process in many animal species.<br /><br />Now an MIT team led by Professor Li-Huei Tsai has revealed that sirtuins can also boost memory and brainpower — a finding that could lead to new drugs for Alzheimer’s disease and other neurological disorders.<br /><br />Sirtuins’ effects on brain function, including learning and memory, represent a new and somewhat surprising role, says Tsai, the Picower Professor of Neuroscience and an investigator of the Howard Hughes Medical Institute. “When you review the literature, sirtuins are always associated with longevity, metabolic pathways, calorie restriction, genome stability, and so on. It has never been shown to play a role in synaptic plasticity,” she says.<br /><br />Synaptic plasticity — the ability of neurons to strengthen or weaken their connections in response to new information — is critical to learning and memory. Potential drugs that enhance plasticity by boosting sirtuin activity could help patients with neurological disorders such as Alzheimer’s, Parkinson’s and Huntington’s diseases, says Tsai.<br /><br /><strong>A protein with many roles</strong><br /><br />Sirtuins have received much attention in recent years for their life-span-boosting potential, and for their link to resveratrol, a compound found in red wine that has shown beneficial effects against cancer, heart disease and inflammation in animal studies. <br /><br />MIT Biology Professor Leonard Guarente discovered about 15 years ago that the SIR2 gene regulates longevity in yeast. Later work revealed similar effects in worms, mice and rats.<br /><br />More recently, studies have shown that one mammalian version of the gene, SIRT1, protects against oxidative stress (the formation of highly reactive molecules that can damage cells) in the heart and maintains genome stability in multiple cell types. SIRT1 is thought to be a key regulator of an evolutionarily conserved pathway that enhances cell survival during times of stress, especially a lack of food.<br /><br />In 2007, Tsai and her colleagues showed that sirtuins (the proteins produced by SIR or SIRT genes) protect neurons against neurodegeneration caused by disorders such as Alzheimer’s. They also found that sirtuins improved learning and memory, but believed that might be simply a byproduct of the neuron protection.<br /><br />However, Tsai’s new study, funded by National Institutes of Health, the Simons Foundation, the Swiss National Science Foundation and the Howard Hughes Medical Institute, shows that sirtuins promote learning and memory through a novel pathway, unrelated to their ability to shield neurons from damage. The team demonstrated that sirtuins enhance synaptic plasticity by manipulating tiny snippets of genetic material known as microRNA, which have recently been discovered to play an important role in regulating gene expression.<br /><br />Specifically, the team showed that sirtuins block the activity of a microRNA called miR-134, which normally halts production of CREB, a protein necessary for plasticity. When miR-134 is inhibited, CREB is free to help the brain adjust its synaptic activity.<br /><br />Mice with the SIRT1 gene missing in the brain performed poorly on several memory and learning tests, including object-recognition tasks and a water maze.<br /><br />“Activation of sirtuins can directly enhance cognitive function,” says Tsai. “This really suggests that SIRT1 is a very good drug target, because it can achieve multiple beneficial effects.”<br /><br />Raul Mostoslavsky, assistant professor of medicine at Harvard Medical School, says the findings do suggest that activating SIRT1 could benefit patients with neurodegenerative diseases. “However, we will need to be very cautious before jumping to conclusions,” he says, “since SIRT1 has (multiple) effects in multiple cells and tissues, and therefore targeting specifically this brain function will be quite challenging.”<br /><br />Tsai and her colleagues are now studying the mechanism of SIRT1’s actions in more detail, and are also investigating whether sirtuin genes other than SIRT1 influence memory and learning.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Richard Binzel on astronomers’ powerful new tool]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/3q-binzel-pan-staars-0713.html</link>
<story_id>15511</story_id>
<featured>0</featured>
<description><![CDATA[Pan-STARRS, a telescope designed to reveal the ‘unexpected surprises’ in our solar system, including possible threats to Earth, just became fully operational.]]></description>
<postDate>Tue, 13 Jul 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100712160344-2.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100712160344-2.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100712160344-2.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Pan-STAARS]]></imageCredits>
<imageCaption><![CDATA[The first Pan-STARRS telescope, PS1, obtained this image of the galaxy M51 and its companion, NGC5195, while it was undergoing commissioning and debugging in 2008. Astronomers announced last month that PS1, which will scan the heavens each night in search of potential threats to Earth, is now fully operational.]]></imageCaption>
</image>
<body><![CDATA[<em>Last month, it was announced that the first Pan-STARRS (Panoramic Survey Telescope &amp; Rapid Response System) telescope, PS1, is fully operational. The system is designed to search for “killer” asteroids and comets by mapping large portions of the sky each night to look for moving objects in our solar system. Based in Hawaii, Pan-STARRS features the world’s largest digital camera — a 1,400-megapixel device designed by researchers at MIT Lincoln Laboratory. Follow-up observations based on those images will allow astronomers to track moving objects and calculate their orbits to determine any potential threats to Earth. Richard Binzel, professor of planetary sciences in MIT’s Department of Earth, Atmospheric and Planetary Sciences, discusses Pan-STARRs with </em>MIT News<em>. A member of NASA’s Task Force for Planetary Defense, Binzel believes that Pan-STARRS’ “constant watch” will not only rule out possible threats from near-Earth objects (NEOs) over time, but will also reveal unknown galaxies and details about faraway planets. </em><br /><br /><strong>Q.</strong> How does the technology of Pan-STARRS compare to that of the Hubble Space Telescope that was launched into orbit in 1990 to study objects in space?<br /><br /><strong>A.</strong> Pan-STARRS and Hubble are about as different as different can be in the way they go about their space studies. Hubble is designed to focus intensely on a very small piece of the sky to unravel the physical mysteries of very carefully pre-selected targets. Pan-STARRS is designed to see as much of the sky as possible as quickly as possible by imaging a huge chunk of the sky every 30 seconds before moving on to a different chunk of the sky. By repeating the image of each part of the sky every few minutes and then comparing those images, Pan-STARRS is designed to detect rapid changes that could indicate a moving object. <br /><br />It is not known in advance what each Pan-STARRS image will reveal, but there is always something to be discovered with each new look. In most cases, these discoveries will be moving objects — small bodies in our solar system that are following their own orbital paths around the sun. Most are asteroids in the main belt between Mars and Jupiter. Some are in the outer solar system in the new zone of bodies now known to reside beyond Neptune called the Kuiper Belt. Small solar system bodies that are found to be moving the fastest — because they are closer to both the sun and Earth — are the so-called “near-Earth objects” (NEOs). These bodies can be both asteroids and comets whose orbits bring them within the vicinity of Earth, and some have the potential to be on collision course with Earth. By repeatedly imaging these objects over several hours and many nights, their orbital paths can be determined with increasing precision. Fortunately, almost all NEOs can be immediately ruled out for having any potential hazard to Earth. But for those that do show some remaining chance of a future collision, dedicated follow-up tracking by Pan-STARRS or other telescopes can find out for sure. So far, no object with a certain impact having hazardous consequences has been discovered or is known.<br /><br /><strong>Q.</strong> What are the limitations of this telescope? Can it be used to find exoplanets — planets that orbit a star other than the sun — or other objects outside our solar system?<br /><br /><strong>A.</strong> Pan-STARRS is most strongly specialized toward finding changes in the sky, but unraveling the cause of those changes will largely fall to specialized telescopes to analyze light from different parts of the spectrum or with much higher resolution. Pan-STARRS’ strength is being sensitive to any changes it sees anywhere in its field of view, and that, of course, also includes distant stars and galaxies beyond our solar system. While our sun is a very stable light source, many stars pulsate in their brightness in the early and late stages of their lives. Pan-STARRS will be the most sensitive survey ever performed to detect these changes over very short (minutes to hours) to very long (days and years) intervals of time. Pan-STARRS can also detect abrupt, but regularly spaced drops in the brightness of stars that can be a telltale sign of exoplanets. These drops occur when the orbit of the planet happens to carry the planet into the line of sight between us and its own star. Even though we cannot see the planet directly, we can tell that it is there, allowing follow-up observations by Pan-STARRS and other telescopes to learn more about these newly discovered exoplanets. Even farther away, enormous stellar explosions called supernovae can sometimes bring faint galaxies into view that may have never before been seen. All told, the universe is a bizarre and dramatic place, and with the constant watch that Pan-STARRS is giving us, there are plenty of unexpected surprises ahead.<br /><br /><strong>Q.</strong> Even with technology like that used in Pan-STARRS, is it possible that we could fail to detect potentially dangerous asteroids or comets?<br /><br /><strong>A.</strong> No single system is complete and can be sure to catch every object that is out there. Pan-STARRS’ task is to reduce risk by cataloging as many potentially hazardous asteroids and comets as possible. As Pan-STARRS starts out, nearly every discovered asteroid and comet will be a new object that has never before been seen or catalogued. As the survey continues for many years or even a decade, it will begin to “rediscover” objects that are already in the catalog. When Pan-STARRS reaches the point where 90 percent of the asteroids and comets it detects are already in its catalog, we can estimate that about 90 percent of potentially hazardous asteroids and comets have been found. This allows us to know with certainty that the overall risk to Earth is reduced because most objects that we could not know for sure whether they might be hazardous will be safely ruled out. Most importantly, specific objects that might pose a future risk will be positively identified, and resources to fully assess that risk can be focused on these objects to determine whether they are “friend or foe.” The odds favor that nearly all will be ruled out as foes, but in identifying them as friends, these objects with the near-Earth orbits have the possibility to be very easily reached for scientific exploration by robotic or human space missions.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Fibers that can hear and sing]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/acoustic-fibers-0712.html</link>
<story_id>15507</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers pass a milestone on the path to sophisticated fibers that interact with their surroundings in new ways.]]></description>
<postDate>Mon, 12 Jul 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100715121110-1.png</thumbURL>
<smallURL width='140' height='149'>http://web.mit.edu/newsoffice/images/article_images/w140/20100715121110-1.jpg</smallURL>
<fullURL width='368' height='392'>http://web.mit.edu/newsoffice/images/article_images/20100715121110-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Research Laboratory of Electronics at MIT/Greg Hren Photograph]]></imageCredits>
<imageCaption><![CDATA[The Fink lab has demonstrated that it can manufacture acoustic fibers with flat surfaces, like those shown here, as well as fibers with circular cross sections. The flat fibers could prove particularly useful in acoustic imaging devices.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100709161125-5.jpg</fullURL>
<imageCredits><![CDATA[Photo: Research Laboratory of Electronics at MIT/Greg Hren Photograph]]></imageCredits>
<imageCaption><![CDATA[Yoel Fink, flanked by two of the graduate students in his lab who helped develop the new fibers, Sasha Stoyarov and No&eacute;mie Chocat.
]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[For centuries, "man-made fibers" meant the raw stuff of clothes and ropes; in the information age, it's come to mean the filaments of glass that carry data in communications networks. But to Yoel Fink, an associate professor of materials science and principal investigator at MIT's Research Lab of Electronics, the threads used in textiles and even optical fibers are much too passive. For the past decade, his lab has been working to develop fibers with ever more sophisticated properties, to  enable fabrics that can interact with their environment.<br /><br />In the August issue of <em>Nature Materials</em>, Fink and his collaborators announce a new milestone on the path to functional fibers: fibers that can detect and produce sound. Applications could include clothes that are themselves sensitive microphones, for capturing speech or monitoring bodily functions, and tiny filaments that could measure blood flow in capillaries or pressure in the brain. The paper, whose authors also include Shunji Egusa, a former postdoc in Fink's lab, and current lab members Noémie Chocat and Zheng Wang,<a href="http://www.nature.com/nmat/journal/vaop/ncurrent/abs/nmat2792.html" target="_blank"> appeared on <em>Nature Materials</em>' website on July 11</a>, and the work it describes was supported by MIT's Institute for Soldier Nanotechnologies, the National Science Foundation and the U.S. Defense Department's Defense Advanced Research Projects Agency.<br /><br />Ordinary optical fibers are made from a "preform," a large cylinder of a single material that is heated up, drawn out, and then cooled. The fibers developed in Fink's lab, by contrast, derive their functionality from the elaborate geometrical arrangement of several different materials, which must survive the heating and drawing process intact.<br /><br /><strong>The right stuff</strong><br /><br />The heart of the new acoustic fibers is a plastic commonly used in microphones. By playing with the plastic's fluorine content, the researchers were able to ensure that its molecules remain lopsided — with fluorine atoms lined up on one side and hydrogen atoms on the other — even during heating and drawing. The asymmetry of the molecules is what makes the plastic "piezoelectric," meaning that it changes shape when an electric field is applied to it. <br /><br />In a conventional piezoelectric microphone, the electric field is generated by metal electrodes. But in a fiber microphone, the drawing process would cause metal electrodes to lose their shape. So the researchers instead used a conducting plastic that contains graphite, the material found in pencil lead. When heated, the conducting plastic maintains a higher viscosity — it yields a thicker fluid — than a metal would. <br /><br />Not only did this prevent the mixing of materials, but, crucially, it also made for fibers with a regular thickness. After the fiber has been drawn, the researchers need to align all the piezoelectric molecules in the same direction. That requires the application of a powerful electric field — 20 times as powerful as the fields that cause lightning during a thunderstorm. Anywhere the fiber is too narrow, the field would generate a tiny lightning bolt, which could destroy the material around it.<br /><br /><strong>Sound results</strong><br /><br />Despite the delicate balance required by the manufacturing process, the researchers were able to build functioning fibers in the lab. "You can actually hear them, these fibers," says Chocat, a graduate student in the materials science department. "If you connected them to a power supply and applied a sinusoidal current" — an alternating current whose period is very regular — "then it would vibrate. And if you make it vibrate at audible frequencies and put it close to your ear, you could actually hear different notes or sounds coming out of it." For their <em>Nature Materials</em> paper, however, the researchers measured the fiber's acoustic properties more rigorously. Since water conducts sound better than air, they placed it in a water tank opposite a standard acoustic transducer, a device that could alternately emit sound waves detected by the fiber and detect sound waves emitted by the fiber.<br /><br />In addition to wearable microphones and biological sensors, applications of the fibers could include loose nets that monitor the flow of water in the ocean and large-area sonar imaging systems with much higher resolutions: A fabric woven from acoustic fibers would provide the equivalent of millions of tiny acoustic sensors. <br /><br />Zheng, a research scientist in Fink's lab, also points out that the same mechanism that allows piezoelectric devices to translate electricity into motion can work in reverse. "Imagine a thread that can generate electricity when stretched," he says. <br /><br />Ultimately, however, the researchers hope to combine the properties of their experimental fibers in a single fiber. Strong vibrations, for instance, could vary the optical properties of a reflecting fiber, enabling fabrics to communicate optically.<br /><br />Max Shtein, an assistant professor in the University of Michigan's materials science department, points out that other labs have built piezoelectric fibers by first drawing out a strand of a single material and then adding other materials to it, much the way manufacturers currently wrap insulating plastic around copper wire. "Yoel has the advantage of being able to extrude kilometers of this stuff at one shot," Shtein says. "It's a very scalable technique." But for applications that require relatively short strands of fiber, such as sensors inserted into capillaries, Shtein say, "scalability is not that relevant." <br /><br />But whether or not the Fink lab's technique proves, in all cases, the most practical way to make acoustic fibers, "I'm impressed by the complexity of the structures they can make," Shtein says. "They're incredibly virtuosic at that technique."<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The aerosols conundrum]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/aerosols-0709.html</link>
<story_id>15506</story_id>
<featured>0</featured>
<description><![CDATA[Research shows that aerosols not only cool, but also heat the planet — a finding that may cloud the validity of climate-change models.]]></description>
<postDate>Fri, 09 Jul 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100712085215-0.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100712085215-0.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100712085215-0.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA]]></imageCredits>
<imageCaption><![CDATA[Many climate researchers assume that aerosols — microscopic particles in the atmosphere — help to cool the Earth. But new research from MIT shows that aerosols not only cool but also heat the planet, a finding that calls into question some key assumptions about climate change.  
]]></imageCaption>
</image>
<body><![CDATA[Just how much warmer Earth will become as a result of greenhouse-gas emissions — and how much it has warmed since preindustrial times — is much debated. In a 2007 report, the Intergovernmental Panel on Climate Change, an agency formed by the United Nations to assess climate change, said that the planet’s average surface temperature will rise by between 2 and 11.5 degrees Fahrenheit by 2100, with a best estimate at between 3.2 to 7.2 degrees F. However, the IPCC’s computer models have a record of overestimating warming: If the IPCC models were right, the planet should now be hotter than it is. <br /><br />The IPCC attributes the discrepancy to aerosols — microscopic particles in the atmosphere that are created by both nature (dust blown by desert winds) and human activity (liquid droplets created from fuel combustion). Because aerosols help cloud droplets form into icy particles and reflect sunlight back into space, they help to cool Earth and possibly mitigate warming caused by emissions. But Richard Lindzen, the Alfred P. Sloan Professor of Meteorology in MIT’s Department of Earth, Atmospheric and Planetary Sciences, is among those who question the accuracy of the IPCC models, and he has been critical of the aerosols argument. <br /><br />In a paper published last month in the <em>Proceedings of the National Academy of Sciences</em> Lindzen and his former postdoctoral researcher, Yong-Sang Choi, suggest that aerosols not only cool the Earth-atmosphere system — the system by which the atmosphere and oceans interact and affect the global climate — but also heat it. By describing the potential dual effects of aerosols, the research questions the IPCC’s models.  <br /><br />“Current climate models generally overpredict current warming and assume that the excessive warming is cancelled by aerosols,” the researchers say in their paper. “[Our research] offers a potentially important example of where the secondary effect is to warm, thus reducing the ability of aerosols to compensate for excessive warming in current models.” That is, the degree to which aerosols can compensate for model over-prediction of warming remains open, the research suggests. <br /><br />While Thomas Stocker, co-chair of the IPCC’s Working Group I that is examining the physical scientific aspects of the climate system and climate change, declined to comment on the study, he says Lindzen and Choi’s research is part of relevant peer-reviewed work that the group will assess in its Fifth Assessment Report about climate change to be published in 2013.<br /><strong><br />Pinning down aerosols</strong><br /><br />In their research, Lindzen and Choi analyzed data about cloud formation and dust aerosols, or tiny particles of sand and silicate in the atmosphere, that were collected by NASA’s Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO) satellite from June 2006 through May 2007. Their analysis revealed that there were about 20 percent fewer “super-cooled” cloud particles — droplets that are a mixture of water and ice, but reflect more sunlight than ice — in regions that had dust aerosols. Such a difference, Lindzen and Choi suggest, could warm the atmosphere in those regions.<br /><br />According to the researchers, the decrease in super-cooled particles occurs when aerosols travel to a layer of the atmosphere where the temperature is around minus 20 degrees Celsius, and they “effectively kill” super-cooled cloud droplets by causing them to form into ice. Fewer super-cooled cloud droplets would mean that clouds reflect less sunlight, which could have a warming effect on the climate. That effect, the researchers believe, needs to be incorporated into climate-change models. “The IPCC assumed that all the secondary effects of aerosols would be to increase reflectivity, so it has left out a very important factor that could lead to the opposite effect,” Lindzen says.<br /><br />The work is important to the global-warming debate because it sheds light on the uncertainties of climate sensitivity, which is the term the IPCC uses to describe the change that a doubling of carbon dioxide would have on global average temperatures (the IPCC’s 2007 report estimates that change to be between 3.6 and 8.1 degrees F by the end of the century, with a best estimate of about 5.4 degrees F). According to Yale climate scientist Trude Storelvmo, “aerosol effects on climate, particularly via their influence on clouds, currently represent the most uncertain forcing of climate change.” Although the IPCC models assume that aerosols cool the Earth-atmosphere system, she cautions that “unless we can quantify this supposed aerosol cooling counteracting the warming due to increasing greenhouse gases, we cannot say what the climate sensitivity of the Earth-atmosphere system is.”<br /><br />Because satellite data can be limited, she suggests that future research should include measurements of aerosol and cloud properties taken by instruments onboard aircraft that travel to the upper atmosphere. She thinks this combination could help address one question that remains unanswered in the paper: why few super-cooled clouds were detected over South America even though the satellite didn’t detect dust or carbon aerosols over that region. <br /><br />Lindzen agrees that climate scientists can’t rely solely on remote sensing techniques to get “solid, incontrovertible data” about aerosols and clouds. Even so, he is eager for the launch of better satellites and instruments so that he and his colleagues can gather as much data as possible about how clouds evolve “so that we can better pin down what aerosols do.” Until scientists figure out that missing piece of the climate change puzzle, it will be difficult to predict the effects of future warming.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Phonons]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/explained-phonons-0706.html</link>
<story_id>15497</story_id>
<featured>0</featured>
<description><![CDATA[When trying to control the way heat moves through solids, it is often useful to think of it as a flow of particles.]]></description>
<postDate>Thu, 08 Jul 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100701123309-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100701123309-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100701123309-1.jpg</fullURL>
<imageCaption><![CDATA[A computer simulation shows phonons, depicted as color variations, traveling through a crystal lattice. The lattice in this case is broken up by round rods whose spacing has been chosen to block the passage of phonons of certain wavelengths.]]></imageCaption>
</image>
<body><![CDATA[For the engineers who design cell phones, solar panels and computer chips, it’s increasingly important to be able to control the way heat moves through the crystalline materials — such as silicon — that these devices are based on. In computer and cell-phone chips, for example, one of the key limitations to increasing speed and memory is the need to dissipate the heat generated by the chips.<br /><br />To understand how heat spreads through a material, consider that heat — as well as sound — is actually the motion or vibration of atoms and molecules: Low-frequency vibrations correspond to sound, while higher frequencies correspond to heat. At each frequency, quantum mechanics principles dictate that the vibrational energy must be a multiple of a basic amount of energy, called a quantum, that is proportional to the frequency. Physicists call these basic levels of energy phonons. <br /><br />In a sense, then, “phonon” is just a fancy word for a particle of heat.<br /><br />Phonons are especially relevant in the behavior of heat and sound in crystals, explains Gang Chen, the Soderberg Professor of Power Engineering at MIT. In a crystal, the atoms are neatly arranged in a uniform, repeating structure; when heated, the atoms can oscillate at specific frequencies. The bonds between the individual atoms in a crystal behave essentially like springs, Chen says. When one of the atoms gets pushed or pulled, it sets off a wave (or phonon) travelling through the crystal, just as sitting down on one edge of a trampoline can set off vibrations through the entire surface.<br /><br />In practice, most materials are filled with a chaotic mix of phonons that have different frequencies and are traveling in different directions, all superimposed on each other, in the same way that the seemingly chaotic movements of a choppy sea can (theoretically) be untangled to reveal a variety of superimposed waveforms of different frequencies and directions.<br /><br />But unlike photons (the particles that carry light or other electromagnetic radiation), which generally don’t interact at all if they have different wavelengths, phonons of different wavelengths can interact and mix when they bump into each other, producing a different wavelength. This makes their behavior much more chaotic and thus difficult to predict and control. <br /><br />Just as photons of a given frequency can only exist at certain specific energy levels — exact multiples of the basic quanta —so, too, can phonons, Chen says. And when working on applied physics relating to the transfer of heat within solids, which is a specific focus of Chen’s research, thinking in terms of phonons has proved to be especially useful.<br /><br />For example, in the quest for better ways to dissipate heat from computer chips — a key requirement as chips get faster and pack in more components — finding ways to manipulate the behavior of the phonons in those chips, so the heat can be removed easily, is the key. Conversely, in designing thermoelectric devices to generate electricity from temperature differences, it’s important to develop materials that can conduct electricity (the motion of electrons) easily, but block the motion of phonons (that is, heat).<br /><br />“In some cases, you want strong conduction of phonons, and in some cases you want to reduce their propagation,” Chen says. “Sometimes they’re good guys, and sometimes they’re bad guys.”<br /><br /><input id="gwProxy" type="hidden" /><input id="jsProxy" onclick="if(typeof(jsCall)=='function'){jsCall();}else{setTimeout('jsCall()',500);}" type="hidden" />]]></body>
</item>
<item>
<title><![CDATA[When a little knowledge really is dangerous]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/financial-obfuscation-0707.html</link>
<story_id>15499</story_id>
<featured>0</featured>
<description><![CDATA[MIT Sloan professor on the ‘arms race’ between investors trying to understand financial products, and the firms trying to confuse them]]></description>
<postDate>Wed, 07 Jul 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100712085453-0.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100712085453-0.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100712085453-0.jpg</fullURL>
</image>
<body><![CDATA[As the global financial meltdown has revealed, there is no shortage of people who fail to understand their own investments. Perhaps better financial literacy among the public would help many people avoid such poor decisions.  <br /><br />But increasing knowledge among investors can actually lead to unintended negative consequences, claims Gustavo Manso, an associate professor at the MIT Sloan School of Management, in a new working paper. Indeed, Manso and his co-author, Bruce Carlin, an assistant professor of finance at UCLA, conclude that modest increases in know-how for some investors damage other customers by generating an “arms race” in which financial firms seek new profits by baffling all clients with ever-more arcane products. <br /><br />“Small increments of education can hurt everyone,” says Manso. This hardly means we should not pursue investor literacy, he says, but it does suggest that we consider the effects that different types of financial education can have. <br /><strong><br />Obfuscation versus education</strong><br /><br />In the paper, “Obfuscation, Learning, and the Evolution of Investor Sophistication,” to be published in the Review of Financial Studies, Manso and Carlin examine consumer financial education involving the mutual-fund industry, although they say their research also applies to the credit card and mortgage businesses, and more. Investors often ignore costly mutual-fund fees, the authors point out; similarly, many homeowners have failed to understand how adjustable-rate mortgages work, while consumers who do not grasp the fees and repayment schedules of credit cards have helped lead “to a record-setting amount of household debt” in the United States.<br /><br />With mutual funds, investors who focus exclusively on fund performance — annual returns — overlook the annual fees that can overshadow variations in fund results. “There are huge differences in mutual-fund fees,” says Manso. “They add up pretty quickly. But it’s hard for investors to figure them out, because they’re sometimes hidden in the fund’s prospectus.” A 1998 study by the economists Dale Domain and William Reichenstein found that fees account for 84 percent of the difference in performance among a group of major funds.<br /><br />In theory, this problem can be remedied in a variety of ways. The government could require financial firms to make more transparent disclosures. Investors can educate themselves about the subject, or learn from each other. Some economists have argued that financial literacy should be a basic part of a secondary-school education.<br /><br />Making financial literacy a part of schooling, however, would be costly and hard to implement nationally (at least in the United States, where schools have significant local controls). At the moment, gains in knowledge among investors are more likely to be incremental and limited to a portion of the investing population.<br /><br />“If you educate everyone, and everyone becomes sophisticated, you’d solve the problem,” says Manso. “Firms wouldn’t have incentives to play games any more. But that would be very expensive and hard to do.” <br /><br />Instead, Manso and Carlin assert, the uneven levels of financial knowledge among the public produce a dynamic relationship between financial firms and their customers. Firms frequently alter their offerings to increase profitability. “Some consumers are sophisticated, so they’re able to see through different offerings and pick out the best products, says Manso. “But in practice, there are a lot of unsophisticated investors, and they are the ones who are losing frequently. Even when they catch up a bit, there is still a lot of scope for firms to dynamically adjust their products.” <br /><br />Using past empirical studies, Manso and Carlin model a variety of situations in which investors increase their education levels, and firms respond. Some of these “arms-race” scenarios bring unwanted costs to all parties. Unsophisticated investors pay higher fees. Sophisticated investors invest more time and money finding better funds. The funds themselves spend increasing amounts of capital changing their products. And markets with highly complex products can scare off potential investors. “This not only degrades personal welfare,” the authors write, “but also effects the economy as a whole.” <br /><br /><strong>What is to be done?</strong><br /><br />If a little financial education is a dangerous thing, then what other options do we have to help the public avoid pitfalls when it comes to choosing sound investments? Manso suggests there are a couple of alternatives, with their own benefits and costs. <br /><br />One solution would be increased regulation to prevent obfuscation by financial firms, thus limiting the opportunities businesses have to exploit consumers. Indeed, the financial regulation bill being considered by Congress would likely increase the government’s ability to enact new rules, since it mandates the creation of a Consumer Financial Protection Agency, overseen by the Federal Reserve. Consumer advocates have hailed this part of the legislation.<br /><br />“We are going to see a fundamental change going forward,” says Gail Hillebrand, a senior attorney and manager of the Consumer Union’s Financial Services Campaign. “For the first time, we will have an agency whose job it is to stay on top of these developments.” In her view, “Financial literacy is a partial solution. We need to pair it with effective oversight.” <br /><br />However, as Manso notes, new laws might not anticipate innovations of the future. Moreover, despite all the dubious financial practices of recent years, tight regulations “could restrict legitimate innovation,” he says. <br /><br />Another option, Manso notes, would be widespread use of “libertarian paternalism,” that is, benign guidance for investors. Employees often work for firms whose 401(k) plans have several options. But the government could require firms to assign safe plans to employees as their default selection; workers would have to make a special effort to choose riskier retirement plans. This would eliminate the Enron-style problem in which employees tie up their retirement assets almost wholly in company stock. But the downside, Manso believes, is that libertarian paternalism reduces investor knowledge generally, since it minimizes the need for investors to understand their own finances.<br /><br />“The cost here is that it kills social learning,” says Manso. “People herd to a default option and they don’t do research any more.” <br /><br />This diminished incentive to learn may be the least of the various problems in the realm of investor education; it is something Manso and Carlin are continuing to research (in collaboration with another economist, Simon Gervais of Duke). Among investors, perhaps apathy is the cure for ignorance.  <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Computer automatically deciphers ancient language]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/ugaritic-barzilay-0630.html</link>
<story_id>15484</story_id>
<featured>0</featured>
<description><![CDATA[A new system that took a couple hours to decipher much of the ancient language Ugaritic could help improve online translation software.]]></description>
<postDate>Wed, 30 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100628141035-0.png</thumbURL>
<smallURL width='140' height='99'>http://web.mit.edu/newsoffice/images/article_images/w140/20100628141035-0.jpg</smallURL>
<fullURL width='368' height='260'>http://web.mit.edu/newsoffice/images/article_images/20100628141035-0.jpg</fullURL>
<imageCaption><![CDATA[An incidental challenge in building a computer system that could decipher Ugaritic (inscribed on tablet) was developing a way to digitally render Ugaritic symbols (inset).]]></imageCaption>
</image>
<body><![CDATA[In his 2002 book <em>Lost Languages</em>, Andrew Robinson, then the literary editor of the London <em>Times</em>’ higher-education supplement, declared that “successful archaeological decipherment has turned out to require a synthesis of logic and intuition … that computers do not (and presumably cannot) possess.” <br /><br />Regina Barzilay, an associate professor in MIT’s Computer Science and Artificial Intelligence Lab, Ben Snyder, a grad student in her lab, and the University of Southern California’s Kevin Knight took that claim personally. At the Annual Meeting of the Association for Computational Linguistics in Sweden next month, they will <a href="http://people.csail.mit.edu/bsnyder/papers/bsnyder_acl2010.pdf" target="_blank">present a paper</a> on a new computer system that, in a matter of hours, deciphered much of the ancient Semitic language Ugaritic. In addition to helping archeologists decipher the eight or so ancient languages that have so far resisted their efforts, the work could also help expand the number of languages that automated translation systems like Google Translate can handle.<br /><br />To duplicate the “intuition” that Robinson believed would elude computers, the researchers’ software makes several assumptions. The first is that the language being deciphered is closely related to some other language: In the case of Ugaritic, the researchers chose Hebrew. The next is that there’s a systematic way to map the alphabet of one language on to the alphabet of the other, and that correlated symbols will occur with similar frequencies in the two languages. <br /><br />The system makes a similar assumption at the level of the word: The languages should have at least some cognates, or words with shared roots, like <em>main</em> and <em>mano</em> in French and Spanish, or <em>homme</em> and <em>hombre</em>. And finally, the system assumes a similar mapping for parts of words. A word like “overloading,” for instance, has both a prefix — “over” — and a suffix — “ing.” The system would anticipate that other words in the language will feature the prefix “over” or the suffix “ing” or both, and that a cognate of “overloading” in another language — say, “surchargeant” in French — would have a similar three-part structure.<br /><br /><strong>Crosstalk</strong><br /><br />The system plays these different levels of correspondence off of each other. It might begin, for instance, with a few competing hypotheses for alphabetical mappings, based entirely on symbol frequency — mapping symbols that occur frequently in one language onto those that occur frequently in the other. Using a type of probabilistic modeling common in artificial-intelligence research, it would then determine which of those mappings seems to have identified a set of consistent suffixes and prefixes. On that basis, it could look for correspondences at the level of the word, and those, in turn, could help it refine its alphabetical mapping. “We iterate through the data hundreds of times, thousands of times,” says Snyder, “and each time, our guesses have higher probability, because we’re actually coming closer to a solution where we get more consistency.” Finally, the system arrives at a point where altering its mappings no longer improves consistency.<br /><br />Ugaritic has already been deciphered: Otherwise, the researchers would have had no way to gauge their system’s performance. The Ugaritic alphabet has 30 letters, and the system correctly mapped 29 of them to their Hebrew counterparts. Roughly one-third of the words in Ugaritic have Hebrew cognates, and of those, the system correctly identified 60 percent. “Of those that are incorrect, often they’re incorrect only by a single letter, so they’re often very good guesses,” Snyder says. <br /><br />Furthermore, he points out, the system doesn’t currently use any contextual information to resolve ambiguities. For instance, the Ugaritic words for “house” and “daughter” are spelled the same way, but their Hebrew counterparts are not. While the system might occasionally get them mixed up, a human decipherer could easily tell from context which was intended.<br /><br /><strong>Babel</strong><br /><br />Nonetheless, Andrew Robinson remains skeptical. “If the authors believe that their approach will eventually lead to the computerised ‘automatic’ decipherment of currently undeciphered scripts,” he writes in an e-mail, “then I am afraid I am not at all persuaded by their paper.” The researchers’ approach, he says, presupposes that the language to be deciphered has an alphabet that can be mapped onto the alphabet of a known language — “which is almost certainly not the case with any of the important remaining undeciphered scripts,” Robinson writes. It also assumes, he argues, that it’s clear where one character or word ends and another begins, which is not true of many deciphered and undeciphered scripts.<br /><br />“Each language has its own challenges,” Barzilay agrees. “Most likely, a successful decipherment would require one to adjust the method for the peculiarities of a language.” But, she points out, the decipherment of Ugaritic took years and relied on some happy coincidences — such as the discovery of an axe that had the word “axe” written on it in Ugaritic. “The output of our system would have made the process orders of magnitude shorter,” she says.<br /><br />Indeed, Snyder and Barzilay don’t suppose that a system like the one they designed with Knight would ever replace human decipherers. “But it is a powerful tool that can aid the human decipherment process,” Barzilay says. Moreover, a variation of it could also help expand the versatility of translation software. Many online translators rely on the analysis of parallel texts to determine word correspondences: They might, for instance, go through the collected works of Voltaire, Balzac, Proust and a host of other writers, in both English and French, looking for consistent mappings between words. “That’s the way statistical translation systems have worked for the last 25 years,” Knight says. <br /><br />But not all languages have such exhaustively translated literatures: At present, Snyder points out, Google Translate works for only 57 languages. The techniques used in the decipherment system could be adapted to help build lexicons for thousands of other languages. “The technology is very similar,” says Knight, who works on machine translation. “They feed off each other.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Emeritus: Engineering a new path]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/emeritus-merrill-0629.html</link>
<story_id>15486</story_id>
<featured>0</featured>
<description><![CDATA[MIT chemical engineer Edward Merrill helped steer his field toward biomedicine.]]></description>
<postDate>Tue, 29 Jun 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100628161339-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100628161339-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100628161339-1.jpg</fullURL>
<imageCaption><![CDATA[A 1972 file photo shows Professor Edward Merrill at work in his lab.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='180' height='250'>http://web.mit.edu/newsoffice/images/article_images/20100628161437-2.jpg</fullURL>
<imageCaption><![CDATA[Chemical engineering professor Paula Hammond]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<em>Editor’s note: This is part of a <a href="http://web.mit.edu/newsoffice/topic/emeritus.html" target="_blank">series of articles</a> linking the work of MIT’s emeritus faculty members with the current state of research in their given fields.</em><br /><br />When MIT chemical engineering professor Paula Hammond cleans out her lab every few years, her students occasionally find bottles of the chemical polyethylene oxide that date back to the 1980s. The bottles are relics of the previous lab occupant, Professor Emeritus Edward Merrill, and a reminder of his legacy in developing polymers for biomedical uses.<br /> <br />Merrill, one of a handful of researchers who pioneered the field of biomedical engineering in the 1960s and 70s, showed that polyethylene oxide, a polymer (long chain) of repeating units called ethers, is remarkably inert when in contact with blood, whereas most other materials cause blood to clot upon contact. He suspected this quality would make it ideal for biomedical applications such as drug delivery. <br /> <br />Today, polyethylene oxide is ubiquitous in the biomedical and pharmaceutical industries, and Hammond and others are developing the polymer into nanoparticles for cancer-drug delivery. Hammond’s “time bomb” nanoparticles can flow through the bloodstream without being absorbed by cells, until they reach a tumor and release their chemotherapy payload.<br /> <br />Polyethylene oxide, also known as polyethylene glycol (PEG), “turns out to be one of the most important, critical elements of designing the polymeric systems we believe can help us in targeting drugs for cancer,” says Hammond, the Bayer Professor of Chemical Engineering.<br /> <br />During his 60-year career at MIT, Merrill also used his expertise in polymer chemistry, especially the study of membranes, to make major contributions to the development of the artificial kidney and oxygenation of the blood during open-heart surgery. <br /> <br /><strong>‘Sticky stuff’</strong><br /><br />Although Merrill’s father was a chemical engineer, he was at first reluctant to follow in his father’s footsteps. At Harvard, he majored in chemistry and classics. His senior year, he planned to take a course in advanced physical chemistry, but wandered into the wrong classroom and ended up in a chemical engineering course taught by MIT Professor William Henry McAdams. “It kind of intrigued me, so instead of going to the correct classroom, I stayed on with it and did reasonably well in it,” recalls Merrill, now 86. <br /> <br />McAdams encouraged Merrill to come to MIT for graduate school, where he earned his PhD in chemical engineering in 1947. He did his thesis in polymer chemistry, an area he hadn’t encountered at Harvard. “It was an ignominious pursuit. Pure chemists would not touch it. It was sticky stuff, gooey stuff,” Merrill says.<br /> <br />Merrill then spent three years working at the Dewey &amp; Almy Chemical Company, where he studied rheology — the flow of polymer materials, such as rubber cement. He returned to MIT for good in 1950, joining the faculty as an assistant professor. <br /> <br />Because of his experience in rheology, researchers at Brigham and Women’s Hospital asked him to help them measure the viscosity of blood, which is how he got into biomedical engineering, around 1960. That work led him to devise a better way to oxygenate blood during open-heart surgery, using membranes permeable to oxygen and carbon dioxide. He also developed membranes to filter blood for artificial kidney machines. <br /> <br />At the time, only a handful of engineers around the United States were delving into biomedicine. Merrill’s colleagues told him that his work in blood rheology might undermine his shot at tenure. But the risk paid off, and Merrill went on to launch biomedical engineering as a major focus of MIT’s chemical engineering department, influencing hundreds if not thousands of MIT students. <br /> <br />“While it may be common to have a medical focus in chemical engineering curricula now, it sure wasn’t in 1957, or even when I came to MIT in 1970,” Robert Langer, MIT Institute Professor, said at a May 14 symposium honoring Merrill’s 60 years on the MIT faculty. “When you look at the people he’s touched and the people they’ve touched, it’s truly remarkable. It’s hard for me to think of anyone who’s had a greater impact.”<br /> <br /><strong>The Cheshire Cat</strong><br /> <br />Hammond first met Merrill as a senior at MIT in 1983, when she took his class in polymer chemistry. She appreciated Merrill’s focus on chemistry at the molecular level, which was a change from the usual chemical engineering focus on materials as a continuum.<br /> <br />“As an undergrad, the reason I wanted to become a chemical engineer was because I loved chemistry,” Hammond recalls. “When I started to take my chemical engineering courses I missed the sense that something molecular was going on. I loved chemistry and I wanted to see it again. So I took as an elective one of Professor Merrill’s classes in polymers and just got fascinated.”<br /> <br />In 1988, Hammond returned to MIT to get her PhD in chemical engineering and took two more polymer chemistry classes with Merrill. “Physical chemistry of polymers can be particularly difficult to conceptualize but he was able to make these concepts incredibly clear and fascinating at the same time,” she says. <br /> <br />Recognizing the difficulty of the material he was teaching, especially thermodynamics of polymers, Merrill often used an analogy from one of his favorite works, “Alice in Wonderland,” to hearten his students. In the story, the Cheshire Cat often climbs up a tree and then slowly disappears, leaving only the grinning mouth.<br /> <br />“Some of these things are very difficult concepts. It takes a lot of thinking, a lot of soaking in, before you can get it,” Merrill says. “I said to the students, this is going to be like the Cheshire Cat’s grin. You’re going to go out of the room and you’ll have this vague idea that you saw something, but the rest of the idea will be gone.”<br /> <br />The students eventually got it, though, says Hammond, who co-taught two courses with Merrill in the late 1990s, soon after she joined the MIT faculty. “It was really amazing to be in the classroom with him and to learn how he engages the students,” she says. When they began teaching together, Hammond asked Merrill for his course notes, which turned out to be three or four sheets stapled together. “I remember thinking, ‘Oh my God, he does all of this just streaming from his head,’” says Hammond. “It amazed me.”<br /> <br />Throughout his career, Merrill, who officially retired from MIT in 2001, made it a point to keep up with what was happening in industrial chemical engineering, and to convey that aspect of the field to his students. He often designed his labs to incorporate biomaterials such as bone cement, used to anchor artificial joints. Because of that exposure to real-world applications, “we end up with chemical engineers coming out of our department with this perspective that I think is unique,” says Hammond.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Christine Ortiz is appointed dean for graduate education]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/graduate-dean-ortiz.html</link>
<story_id>15483</story_id>
<featured>0</featured>
<description><![CDATA[The DMSE professor says she is honored to be chosen to work with and on behalf of MIT’s graduate students.]]></description>
<postDate>Mon, 28 Jun 2010 17:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100628123400-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100628123400-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100628123400-1.jpg</fullURL>
<imageCaption><![CDATA[Christine Ortiz, professor of materials science and engineering, has been appointed the Institute’s next dean for graduate education]]></imageCaption>
</image>
<body><![CDATA[Christine Ortiz, professor of materials science and engineering, has been appointed the Institute’s next dean for graduate education, effective Aug. 1, Chancellor Phillip L. Clay announced Monday.<br /><br />“Professor Ortiz brings considerable experience to graduate student issues,” Clay said in a letter to graduate students, faculty and staff. “Her development and leadership of major projects at MIT and leadership in her profession have been recognized by her peers and in numerous awards.”<br /><br />Ortiz, whose research in MIT’s Department of Materials Science and Engineering (DMSE) focuses on the structure and mechanics of biological materials, will replace Steven R. Lerman, who announced in March that he would become provost and executive vice president for academic affairs at George Washington University. He had been dean for graduate education since 2007.<br /><br />In her new role, Ortiz will collaborate with students, faculty and staff across the Institute on issues related to graduate education and research and will focus on increasing graduate-student opportunities for academic, professional and personal development. She will also facilitate the advancement and information exchange of graduate curricula, formulate new ways to grow the graduate-student community and strive to provide a better understanding and enhancement of the climate and level of diversity in the graduate student population.<br /> <br />“I am deeply grateful for the honor and opportunity to work on behalf of and as an advocate for MIT’s extraordinary graduate student population,” said Ortiz, adding that she hopes to build on the “outstanding achievements” of Lerman and his staff. “I am, to this day, continually awed by the intellectual depth, creativity, work ethic and unbridled enthusiasm of MIT graduate students. They are truly at the core of what makes MIT such a remarkable institution.”<br /><br />A member of the MIT faculty since 1999, Ortiz has served as a member or chair on several department, school and institute committees, including those that focus on undergraduate and graduate education, mentoring, international strategy and diversity. She is a member of MIT’s Initiative on Faculty Race and Diversity and is often invited to speak at panels and workshops geared to improving the experiences of underrepresented minority students and faculty members. In 2009, she received a Dr. Martin Luther King Jr. Leadership Award for recognition of service that reflects the late civil rights leader's ideals and vision.<br /><br />As chair of the DMSE Departmental Committee on Graduate Students since 2008, Ortiz helped lead an extensive review and revision of the department’s graduate curriculum. She is also the founding and current faculty director of the MIT International Science and Technology Initiatives (MISTI)-Israel international exchange program.<br /><br />“Professor Ortiz will bring creative energy to her work with graduate students and faculty,” Clay said. “In selecting her, we were impressed with the thoughtfulness that characterizes how she deals with students and faculty.”<br /><br />Ortiz, described by Clay as “a prolific researcher,” received her BS from Rensselaer Polytechnic Institute and both her MS and PhD from Cornell University, all in the field of materials science and engineering.<br /><br />As leader of MIT’s Ortiz Bionanomechanics Laboratory, which currently has 16 students and postdoctoral associates, Ortiz studies how the nanoscale properties of high-strength, lightweight biological materials could be transferred to synthetic materials. <br /><br />Her leadership achievements at MIT and in her profession have been recognized by her peers and in awards, including the National Science Foundation Presidential Early Career Award for Scientists and Engineers, which was presented to her by former President George W. Bush. In 2008, she won the National Security Science and Engineering Faculty Fellow Award from the Department of Defense. <br /><br />Ulric Ferner, president of the Graduate Student Council, welcomed the news of Ortiz’s appointment. “She has an impressive background of both professional and service work at MIT, and this puts her in an ideal position to be dean. This includes extensive curriculum development, the MIT Students for Israel program and countless community building initiatives,” Ferner said. “She brings a host of creative ideas and strong leadership to this office, and we look forward to working with her over the coming years.”<br /><br />Daniel Hastings, dean for undergraduate education, is also “delighted” that Ortiz is joining the chancellor’s team as the dean for graduate education. “I know she is committed to the welfare of our students, and I look forward to continuing to work with her,” he said.<br /><br />The search advisory committee was chaired by Steven C. Graves and in addition to Ferner, members included Professors Martin L. Culpepper, John A. Ochsendorf and Maria Zuber and Associate Deans Karen K. Gleason and Kai von Fintel.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT’s Koch Institute in strategic partnership with Ortho-McNeil-Janssen Pharmaceuticals]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/koch-ortho-mcneil-janssen.html</link>
<story_id>15482</story_id>
<featured>0</featured>
<description><![CDATA[Organizations will collaborate in multiple areas of oncology research and technology development.]]></description>
<postDate>Mon, 28 Jun 2010 14:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100628094911-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100628094911-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100628094911-1.jpg</fullURL>
<imageCredits><![CDATA[Image: ELLENZWEIG]]></imageCredits>
<imageCaption><![CDATA[An artist's rendering of the new Koch Institute building.]]></imageCaption>
</image>
<body><![CDATA[The David H. Koch Institute of Integrative Cancer Research at MIT announced a major strategic partnership with <a href="http://www.ortho-mcneil.com/ortho-mcneil/" target="_blank">Ortho-McNeil-Janssen Pharmaceuticals, Inc.</a>, and its affiliates, called TRANSCEND, whereby the parties will begin to collaborate in multiple areas of oncology research and technology development. <br /><br />“Interdisciplinary collaboration is central to the mission and culture of the new Koch Institute as it is to MIT. By collaborating with a recognized medical innovation leader like Ortho-McNeil-Janssen Pharmaceuticals, Inc., the Koch Institute community will have enhanced opportunities to deliver the results of our research and technology to cancer patients and their families,” commented MIT President Susan Hockfield. <br /><br />The partnership will begin with a five-year (with an option to renew) sponsored research effort involving the interdisciplinary faculty, students and staff from MIT’s Koch Institute. The teams will begin the collaboration by working in the areas of cancer diagnostics, cancer biology pre-malignancies, genetic models of disease, studying profiles of the tumor microenvironment while a Joint Scientific Steering Committee composed of MIT faculty members and Ortho-McNeil Janssen employees will jointly oversee TRANSCEND. <br /><br />Tyler Jacks, Director of the Koch Institute, stated “The Koch Institute is developing novel approaches to address the unmet needs of cancer patients. By working closely with experts at Ortho-McNeil-Janssen Pharmaceuticals, we expect to advance our discoveries and new technologies rapidly to benefit individuals affected by this disease. Our faculty and trainees are energized by the new translational options made uniquely available through the TRANSCEND program."<br /><br />The <a href="http://web.mit.edu/ki/" target="_blank">David H. Koch Institute for Integrative Cancer Research</a> is a cancer research center affiliated with the Massachusetts Institute of Technology located in Cambridge, Mass. The Institute is one of seven National Cancer Institute-designated basic research centers in the U.S. The Koch Institute was launched in October 2007 and is physical home to approximately 25 MIT faculty members from both the Schools of Engineering and Science and convenes more than 1,000 researchers from across the MIT campus. The Koch Institute fosters and funds interdisciplinary collaborations in five key research areas: nanotherapeutics, detection and monitoring, metastasis, mapping drug sensitivity and resistance pathways, and cancer immunology.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[An Internet 100 times as fast]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/100x-internet-0628.html</link>
<story_id>15480</story_id>
<featured>0</featured>
<description><![CDATA[A new network design that avoids the need to convert optical signals into electrical ones could boost capacity while reducing power consumption.]]></description>
<postDate>Mon, 28 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100625164520-0.png</thumbURL>
<smallURL width='140' height='112'>http://web.mit.edu/newsoffice/images/article_images/w140/20100625164520-0.jpg</smallURL>
<fullURL width='368' height='294'>http://web.mit.edu/newsoffice/images/article_images/20100625164520-0.jpg</fullURL>
<imageCaption><![CDATA[In today’s Internet, data traveling through optical fibers as beams of light have to be converted to electrical signals for processing. By dispensing with that conversion, a new network design could increase Internet speeds 100-fold.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='180' height='180'>http://web.mit.edu/newsoffice/images/article_images/20100625160644-2.jpg</fullURL>
<imageCaption><![CDATA[Vincent Chan, the Joan and Irwin Jacobs Professor of Electrical Engineering and Computer Science]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[The heart of the Internet is a network of high-capacity optical fibers that spans continents. But while optical signals transmit information much more efficiently than electrical signals, they’re harder to control. The routers that direct traffic on the Internet typically convert optical signals to electrical ones for processing, then convert them back for transmission, a process that consumes time and energy. <br /><br />In recent years, however, a group of MIT researchers led by Vincent Chan, the Joan and Irwin Jacobs Professor of Electrical Engineering and Computer Science, has demonstrated a new way of organizing optical networks that, in most cases, would eliminate this inefficient conversion process. As a result, it could make the Internet 100 or even 1,000 times faster while actually reducing the amount of energy it consumes.<br /><br />One of the reasons that optical data transmission is so efficient is that different wavelengths of light loaded with different information can travel over the same fiber. But problems arise when optical signals coming from different directions reach a router at the same time. Converting them to electrical signals allows the router to store them in memory until it can get to them. The wait may be a matter of milliseconds, but there’s no cost-effective way to hold an optical signal still for even that short a time.<br /><br />Chan’s approach, called “<a href="http://www.mit.edu/~medard/papersnew/WOBS%20Final.pdf" target="_blank">flow switching</a>,” solves this problem in a different way. Between locations that exchange large volumes of data — say, Los Angeles and New York City — flow switching would establish a dedicated path across the network. For certain wavelengths of light, routers along that path would accept signals coming in from only one direction and send them off in only one direction. Since there’s no possibility of signals arriving from multiple directions, there’s never a need to store them in memory.<br /><br /><strong>Reaction time</strong><br /><br />To some extent, something like this already happens in today’s Internet. A large Web company like Facebook or Google, for instance, might maintain huge banks of Web servers at a few different locations in the United States. The servers might exchange so much data that the company will simply lease a particular wavelength of light from one of the telecommunications companies that maintains the country’s fiber-optic networks. Across a designated pathway, no other Internet traffic can use that wavelength.<br /><br />In this case, however, the allotment of bandwidth between the two endpoints is fixed. If for some reason the company’s servers aren’t exchanging much data, the bandwidth of the dedicated wavelength is being wasted. If the servers are exchanging a lot of data, they might exceed the capacity of the link.<br /><br />In a flow-switching network, the allotment of bandwidth would change constantly. As traffic between New York and Los Angeles increased, new, dedicated wavelengths would be recruited to handle it; as the traffic tailed off, the wavelengths would be relinquished. Chan and his colleagues have developed network management protocols that can perform these reallocations in a matter of seconds.<br /><br />In a series of papers published over a span of 20 years — the latest of which will be presented at the OptoElectronics and Communications Conference in Japan next month — they’ve also performed mathematical analyses of flow-switched networks’ capacity and reported the results of extensive computer simulations. They’ve even tried out their ideas on a small experimental optical network that runs along the Eastern Seaboard. <br /><br />Their conclusion is that flow switching can easily increase the data rates of optical networks 100-fold and possibly 1,000-fold, with further improvements of the network management scheme. Their recent work has focused on the power savings that flow switching offers: In most applications of information technology, power can be traded for speed and vice versa, but the researchers are trying to quantify that relationship. Among other things, they’ve shown that even with a 100-fold increase in data rates, flow switching could still reduce the Internet’s power consumption.<br /><br /><strong>Growing appetite</strong><br /><br />Ori Gerstel, a principal engineer at Cisco Systems, the largest manufacturer of network routing equipment, says that several other techniques for increasing the data rate of optical networks, with names like burst switching and optical packet switching, have been proposed, but that flow switching is “much more practical.” The chief obstacle to its adoption, he says, isn’t technical but economic. Implementing Chan’s scheme would mean replacing existing Internet routers with new ones that don’t have to convert optical signals to electrical signals. But, Gerstel says, it’s not clear that there’s currently enough demand for a faster Internet to warrant that expense. “Flow switching works fairly well for fairly large demand — if you have users who need a lot of bandwidth and want low delay through the network,” Gerstel says. “But most customers are not in that niche today.”<br /><br />But Chan points to the explosion of the popularity of both Internet video and high-definition television in recent years. If those two trends converge — if people begin hungering for high-definition video feeds directly to their computers — flow switching may make financial sense. Chan points at the 30-inch computer monitor atop his desk in MIT’s Research Lab of Electronics. “High resolution at 120 frames per second,” he says: “That’s a lot of data.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MITEI-led study offers comprehensive look at the future of natural gas]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/gas-report-0625.html</link>
<story_id>15474</story_id>
<featured>0</featured>
<description><![CDATA[Researchers find significant potential for gas to displace coal, reducing greenhouse-gas emissions as a transition to a low-carbon future.]]></description>
<postDate>Fri, 25 Jun 2010 16:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100624145525-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100624145525-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100624145525-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[Natural gas will play a leading role in reducing greenhouse-gas emissions over the next several decades, largely by replacing older, inefficient coal plants with highly efficient combined-cycle gas generation. That’s the conclusion reached by a comprehensive study of the future of natural gas conducted by an MIT study group comprised of 30 MIT faculty members, researchers, and graduate students. The findings, <a href="http://web.mit.edu/mitei/research/studies/naturalgas.html" target="_blank">summarized in an 83-page report</a>, were presented to lawmakers and senior administration officials this week in Washington.<br /><br />The two-year study, managed by the MIT Energy Initiative (MITEI), examined the scale of U.S. natural gas reserves and the potential of this fuel to reduce greenhouse-gas emissions. Based on the work of the multidisciplinary team, with advice from a board of 16 leaders from industry, government and environmental groups, the report examines the future of natural gas through 2050 from the perspectives of technology, economics, politics, national security and the environment. <br /><br />The report includes a set of specific proposals for legislative and regulatory policies, as well as recommendations for actions that the energy industry can pursue on its own, to maximize the fuel’s impact on mitigating greenhouse gas. The study also examined ways to control the environmental impacts that could result from a significant expansion in the production and use of natural gas — especially in electric power production.<br /><br />“Much has been said about natural gas as a bridge to a low-carbon future, with little underlying analysis to back up this contention.  The analysis in this study provides the confirmation — natural gas truly is a bridge to a low-carbon future,” said MITEI Director Ernest J. Moniz in introducing the report. <br /><br /><a href="http://web.mit.edu/mitei/research/studies/naturalgas.html" target="_blank">Read the full report</a><br /><br /><a href="http://web.mit.edu/press/2010/natural-gas">Read a press release about the report</a><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[An easier way to synthesize new drug candidates]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/drug-candidates-0625.html</link>
<story_id>15472</story_id>
<featured>0</featured>
<description><![CDATA[MIT chemists’ answer to long-standing problem could have a big impact on pharmaceutical business.]]></description>
<postDate>Fri, 25 Jun 2010 04:00:04 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100624103054-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100624103054-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100624103054-0.jpg</fullURL>
<imageCaption><![CDATA[In this model, trifluoromethyl is attached to a ring of six carbons (known as a benzene ring). The gray balls represent carbon atoms, and the blue balls represent fluorine. (Hydrogen atoms attached to the benzene ring are not shown.)]]></imageCaption>
</image>
<body><![CDATA[Some drugs may be more effective the longer they last inside the body. To prevent such drugs from being broken down too rapidly, pharmaceutical manufacturers often attach a fluorine-containing structure called a trifluoromethyl group. However, the processes now used require harsh reaction conditions or only work in a small number of cases, limiting their usefulness for synthesizing new drug candidates for testing.<br /><br />Now, MIT chemists have designed a new way to attach a trifluoromethyl group to certain compounds, which they believe could allow pharmaceutical companies to create and test new drugs much faster and potentially reduce the cost of drug discovery.<br /><br />The new synthesis, reported in the June 25 issue of <em>Science</em>, should have an immediate impact, says David MacMillan, a Princeton University chemistry professor who was not involved in the research. <br /><br />“Overnight, people are going to start using this chemistry,” says MacMillan. “Every single person in the pharmaceutical industry who makes molecules that incorporate fluorine to test as drugs has needed this reaction for a very long time.”<br /><br />MIT Chemistry Professor Stephen Buchwald, who led the research team, says achieving the synthesis has been a long-standing challenge for chemists. “Some people said it couldn’t be done, so that’s a good reason to try,” says Buchwald, the Camille Dreyfus Professor of Chemistry at MIT.<br /><br />Eun Jin Cho, a postdoctoral associate in Buchwald’s lab, is the lead author of the paper. Other authors are graduate student Todd Senecal, postdoctoral associates Tom Kinzel and Yong Zhang, and former postdoctoral associate Donald Watson, now an assistant professor of chemistry at the University of Delaware. <br /><br /><strong>Fluorination</strong><br /><br />The trifluoromethyl group (abbreviated CF<sub>3</sub>) is a component of several commonly used drugs, including the antidepressant Prozac, arthritis medication Celebrex and Januvia, used to treat diabetes symptoms. <br /><br />When foreign compounds such as drugs enter the body, they get sent to the liver, where they are broken down and shipped on to the kidneys for excretion. However, CF<sub>3</sub> groups are hard for the body to break down because they contain three fluorine atoms. “Fluorine is not really a component of things we eat, so the body does not know what to do with it,” says Kinzel.<br /><br />CF<sub>3</sub> groups are also a common component of agricultural chemicals such as pesticides. To add a CF<sub>3</sub> group to organic (carbon-containing) molecules, chemists often use hydrogen fluoride under conditions that might produce undesired reactions among the many structural components found in complex molecules like pharmaceuticals or agrochemicals.<br /><br />With the new reaction, the CF<sub>3</sub> group can be added at a much later stage of the overall drug synthesis. The reaction can also be used with a broad range of starting materials, giving drug developers much more flexibility in designing new compounds. <br /><br />Work on the new synthesis was funded by the National Institute of General Medical Sciences, one of the National Institutes of Health.<br /><br /><strong>A catalytic process</strong><br /><br />Chemists have been trying to find a widely applicable catalytic method to attach CF<sub>3</sub> to aryl compounds (compounds containing one or more six-carbon rings) for a couple of decades. Some have achieved different parts of the reaction, but none successfully put all the pieces together to arrive at a method that is applicable for a wide range of different aryl compounds. The major challenge has been finding a suitable catalyst (a molecule that speeds up a reaction) to transfer the CF<sub>3</sub> entity from another source to the carbon ring.<br /><br />CF<sub>3</sub><sup>–</sup> (trifluoromethyl negative ion) tends to be unstable when detached from other molecules, so the catalyst must act quickly to transfer the CF<sub>3</sub> group before it decomposes. The MIT team chose to use a catalyst built from palladium, a silvery-white metal commonly used in catalytic converters. The MIT team is not the first to try palladium catalysis for this reaction, but the key to their success was the use of a ligand (a molecule that binds to the metal to stabilize it and hasten the reaction) called BrettPhos, which they had previously developed for other purposes.<br /><br />Coming up with a useful reaction required much testing of different combinations of palladium, ligand, CF<sub>3</sub> source, temperature and other factors. “Everything had to match up,” says Senecal. <br /><br />During the reaction, a CF<sub>3</sub> group is transferred from a silicon carrier to the palladium, displacing a chlorine atom. Subsequently, the aryl-CF<sub>3</sub> unit is released and the catalytic cycle begins anew. The researchers tried the synthesis with a variety of aryl compounds and achieved yields ranging from 70 to 94 percent of the trifluoromethylated products. <br /><br />In its current state, the process is too expensive for manufacturing use. For drug discovery, however, it may lower overall costs because it streamlines the entire synthesis process. “For discovery chemistry, the price of the metal is much less important,” says Kinzel.<br /><br />All of the reaction components are commercially available, so pharmaceutical and other companies will immediately be able to use this method. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The magic touch]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/haptic-0625.html</link>
<story_id>15473</story_id>
<featured>0</featured>
<description><![CDATA[An MIT researcher examines how the objects we handle affect the decisions we make.]]></description>
<postDate>Fri, 25 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100624142444-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100624142444-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100624142444-0.jpg</fullURL>
</image>
<body><![CDATA[Your success in your next job interview may not have much to do with the contents of your resume. Instead, it may depend on what’s under your resume. Namely: Have the people interviewing you put your CV on a heavy clipboard, or a light one?<br /><br />That is one finding made by Joshua Ackerman, an assistant professor of marketing at the MIT Sloan School of Management, whose research indicates that haptic impressions — our sense of touch — may strongly influence our thoughts.<br /><br />“Our understanding of the world and our social environment is not just a product of our minds,” says Ackerman. “It’s a product of our bodies as well.”<br /><br />In a new paper, “Incidental Haptic Sensations Influence Social Judgments and Decisions,” <a href="http://www.sciencemag.org/cgi/content/abstract/328/5986/1712" target="_blank">published this week</a> in the journal <em>Science</em>, Ackerman and his co-authors, Christopher Nocera of Harvard and John Bargh of Yale, describe the results of six studies showing a variety of ways that tactile sensations can affect decision-making. From workplace judgments to financial decisions, they write, “haptically acquired information exerts a rather broad influence over cognition, in ways of which we are probably often unaware.” <br /><br />Consider the clipboard test. Ackerman, Nocera and Bargh asked 54 people to scrutinize a job candidate by looking at a resume placed on either light or heavy boards. The people using heavy clipboards viewed the candidate as possessing a “more serious interest” in the job opening than those holding the resume on lighter clipboards.<br /><br />“First impressions are liable to be influenced by one’s tactile environment,” the authors write. Adds Ackerman: “It’s a surprising result because it’s so simple.” <br /><br />Alternately, the authors write, how much you are willing to pay for a car may depend on the kind of chair you sit in while thinking about the matter. They based this conclusion on the results of a trial in which 86 people were asked to make two hypothetical offers to a car dealer, starting at a price of $16,500, with the second offer made on the assumption that the first one had been rejected. The second offers from participants who sat in soft, cushioned chairs were 39 percent higher than those made by participants sitting on hard chairs. In this case, the “hardness [of the chair] produces perceptions of strictness, rigidity, and stability, reducing change from one’s initial decisions,” write Ackerman, Nocera and Bargh. <br /><strong><br />Early-childhood education</strong><br /><br />One reason this occurs, the authors suggest, is that “sensorimotor experiences” — those occurring in infancy, when a child is first exploring the world — “form a scaffold for the development of conceptual knowledge.” Touching objects in everyday life, they theorize, may not only produce physical sensations, but may also generate familiar ideas.<br /><br />“As people develop and explore the world through touch, they use these physical actions to develop an idea of the world,” says Ackerman. When people later use their sense of touch to form judgments in everyday life, he adds, “They are taking the easiest route to obtaining information, by drawing on the ideas they already have developed.” <br /><br />Other researchers say the suite of experiments described in the paper is impressive. “While each study is fairly persuasive on its own, taken together they form a clear picture of the importance of touch on cognition,” says Lawrence Williams, a trained psychologist who is a professor of marketing at the University of Colorado. Given the notion that haptic learning begins early, Williams adds, one promising area of research involves future experiments on the responses of infants to various touch sensations. “Would pre-verbal children show these effects if you could design studies at their level?” asks Williams.<br /><br />The authors believe their insights could have practical applications for many people in the business world, including job seekers, marketers, pollsters and more. Understanding how the tactile environment influences perceptions could be relevant in “almost any situation where you’re trying to present information about yourself, or where there is a person attempting to influence others,” claims Ackerman. <br /><br />Ackerman’s future research includes more work on haptic influences; at the moment he is studying how different experiences of touch influence our coordination and other physical activities. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT and the Skolkovo Foundation of Russia reach agreement]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/ia-skolkovo-0624.html</link>
<story_id>15475</story_id>
<featured>0</featured>
<description><![CDATA[Opportunities for educational and research collaboration to be explored.]]></description>
<postDate>Thu, 24 Jun 2010 18:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100624132333-1.png</thumbURL>
<smallURL width='140' height='138'>http://web.mit.edu/newsoffice/images/article_images/w140/20100624132333-1.jpg</smallURL>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100624132333-1.jpg</fullURL>
</image>
<body><![CDATA[Today, the Massachusetts Institute of Technology (MIT) and the Skolkovo Foundation, on behalf of the Russian Federation, announce an agreement to evaluate options for collaboration in education and research in Russia. MIT and the Skolkovo Foundation are signing a framework agreement for this purpose in Washington, D.C. during a visit by Russian President Dmitry Medvedev to the United States.<br /><br />As envisioned by the agreement, MIT and the Skolkovo Foundation will work together to assess opportunities for MIT to conduct educational and research activities in Russia in collaboration with its leading universities and research institutes, which will be selected during the assessment process. Such activities might include the establishment of a network of joint research laboratories bringing together researchers from MIT and Russia, and/or the creation of a new, separate academic institution in the Moscow suburb of Skolkovo. MIT will also evaluate a participation in the creation of an innovation city at Skolkovo as one of the project’s partners. Current founding partners include the Russian Academy of Sciences, Rusnano Corporation, Russian Venture Company, Bauman Technical University, and others. <br /><br />An important result expected from the collaboration of MIT and Skolkovo Foundation is to promote MIT’s culture of innovation and entrepreneurship among Russian scientists and students. The Skolkovo project will create paths to bring technologies to industry, launch high-tech start-ups, and facilitate the growth of the regional innovation ecosystems. MIT will also evaluate ways for including researchers and students from collaborating Russian institutions in MIT’s innovation competitions and events. <br /><br />MIT and the Skolkovo Foundation expect to conclude their evaluation by the end of 2010. If they make a final decision to proceed with one or more of these concepts or others, they expect to negotiate and sign a definitive agreement during the first quarter of 2011. <br /><br />Today's agreement is a result of an historic effort by the President of Russia, Dmitry Medvedev, to modernize the national economy and promote technological innovation. Discussions started in January 2010 during a two-day visit to MIT by senior Russian government officials, led by First Vice Prime Minister Igor Shuvalov, Deputy Chief of Presidential Staff Vladislav Surkov, and others. Mr. Surkov now chairs a task force established by President Medvedev to create a nationwide umbrella for technology development, commercialization and education in Russia within the Skolkovo project. <br /><br />Victor Vekselberg, co-chairman of Skolkovo Foundation, said, “Never before has Russia been so open to international advice and cooperation. We are tired of the brain drain that our country has suffered in recent years, and we want to reverse this process at once. We are committed to engaging the best individual and institutional partners in each area of Skolkovo project. Our team has already been joined by the famous Craig Barrett, and also by Nobel Prize laureates Zhores Alferov and Roger Kornberg. Now we are starting to engage with MIT, a world leader in research and education, as well as in technology commercialization.”<br /><br />Rafael Reif, MIT’s Provost, commented, “MIT’s mission is to advance knowledge and educate students in science, technology and other areas of scholarship and includes a commitment to work with others to bring new knowledge to bear on the world’s great challenges. We are interested in exploring whether opportunities exist for MIT, in collaboration with scientists and engineers in Russia, to conduct educational and research activities that are consistent with MIT’s mission and may contribute meaningfully to the Russian government’s strategic initiative and the Skolkovo Project.”<br /><a href="http://i-gorod.ru" target="_blank"><br />The Skolkovo Foundation</a> is a nonprofit organization in Russia charged by the Russian President with creating a new science and technology city in the Moscow suburb of Skolkovo. This city comprises a university, research institutions, centers of collective usage, business incubator, technology transfer and commercialization office, corporate offices and R&amp;D centers, as well as residential space and social infrastructure. Skolkovo city is governed by a special law, which gives its residents special economic conditions for running their businesses.<br /><br />Founded in 1861, the Massachusetts Institute of Technology, a co-educational privately endowed research university, is dedicated to advancing knowledge and educating students in science, technology and other areas of scholarship to serve the nation and world. The Institute has more than 1,000 faculty and 10,000 undergraduate and graduate students.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[In a first, astronomers detect strong winds on an exoplanet]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/windy-exoplanet-0624.html</link>
<story_id>15467</story_id>
<featured>0</featured>
<description><![CDATA[Researchers see tempestuous atmosphere on a planet located 150 light years from Earth.]]></description>
<postDate>Thu, 24 Jun 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100624142621-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100624142621-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100624142621-0.jpg</fullURL>
<imageCredits><![CDATA[Image: ESO/L. Calçada]]></imageCredits>
<imageCaption><![CDATA[This artist’s impression shows the Jupiter-like transiting exoplanet around its host star.]]></imageCaption>
</image>
<body><![CDATA[Since the first exoplanet — a planet outside our solar system — was discovered in 1995, more than 460 others have been found. While astronomers have been able to measure the size, orbital characteristics, and even some of the molecules that make up the atmospheres of some exoplanets, many mysteries about their formation and evolution remain. <br /><br />A team of astronomers, including a researcher from MIT’s Kavli Institute for Astrophysics and Space Research, has become the first to measure wind in the atmosphere of an exoplanet. By detecting heavy winds on HD209458b, a huge exoplanet located 150 light years away that is slightly more than half the mass of Jupiter, the researchers could then measure the movement of the planet as it orbited its host star — also another first for exoplanetary research.<br /><br />The work, which is detailed in a paper <a href="http://www.nature.com/nature/journal/v465/n7301/full/nature09111.html" target="_blank">published June 24</a> in <em>Nature</em>, will guide future research on exoplanets, since understanding the properties of a planet’s atmosphere is a critical first step for characterizing how that planet formed and evolved. <br /><br />Measuring the planet’s orbital movement is also important because the velocity of that movement can be used with Newton’s law of universal gravitation to get a more precise estimate of the mass of both the planet and its parent star. Before now, astronomers had to rely on complex mathematical models, as well as the changes in light that occurred when an exoplanet’s host star wobbled in response to the exoplanet’s gravitational pull, to determine the exoplanet’s mass. Thanks to a new technique that the researchers used to study HD209458b, astronomers should now be able to refine their estimates of the mass of some exoplanets and their stars. <br /><br />One way that astronomers can learn a lot about an exoplanet is by observing it as it passes in front of its host star as seen from Earth. By measuring the light obscured by an exoplanet during this event, which is known as a transit, astronomers can learn details about the planet, such as its size and what kinds of molecules exist in its atmosphere. Of the 463 exoplanets discovered to date, more than 80 are known to be transiting planets. (HD209458b, identified in 1999, was the first transiting exoplanet discovered.) <br /><br />Researchers detected the heavy winds in HD209458b’s atmosphere by studying carbon monoxide. According to co-author and Kavli postdoc Simon Albrecht, who collaborated with researchers from Leiden University and the Netherlands Institute for Space Research (SRON), the results are “among the many small steps the astronomy community is taking toward being able to, at some point, measure atmospheric conditions on exoplanets that are twins to our Earth.” <br /><br /><strong>Doable from the ground</strong><br /><br />What makes the work, partly funded by the Netherlands Organisation for Scientific Research, “potentially groundbreaking” is the ground-based technique that was used to detect the winds and orbital movement of HD209458b, according to Adam Showman, a planetary scientist at the University of Arizona. “Just the fact this is even doable from the ground is spectacular,” he said.<br /><br />Instead of using a space-based instrument like NASA’s Spitzer Space Telescope to study the faraway planet, the researchers used a ground-based, high-resolution spectrograph at the European Southern Observatory in Chile that can detect subtle changes in the wavelength of light when a planet transits its star. As HD209458b transited last August, its parent star left what lead author Ignas Snellen from the Leiden Observatory in the Netherlands described as “a fingerprint” of light that filtered through the planet’s atmosphere. The researchers then used the spectrograph to analyze that imprint of light to detect carbon monoxide molecules in the atmosphere. “It seems that H209458b is actually as carbon-rich as Jupiter and Saturn, and this could indicate that it was formed in the same way,” Snellen said.<br /><br />The researchers then spent several months analyzing spectrographic measurements of the movement of the carbon monoxide thanks to the Doppler shift, a phenomenon that creates subtle color changes in wavelengths of light when something moves. When an object moves toward us, it looks slightly bluer, and when it moves away, it looks slightly redder. The spectrograph revealed color shifts in the light absorbed by the exoplanet, which indicated that something was moving the gas. That something, the researchers believe, is heavy wind that is blowing carbon monoxide in the planet’s atmosphere up to 10,000 kilometers per hour (the fastest winds ever detected on another planet in our solar system were blowing at up to 2,000 kilometers per hour on Neptune, according to previous research). By tracking the movement of the carbon monoxide, the astronomers could then measure the movement of the planet as it orbited its host star. <br /><br />While the results are notable, future research must address what might be causing the heavy winds, said Showman. Right now, the spectrograph simply does not have enough spectral resolution to distinguish that level of detail. <br /><br />As the team continues to refine the ground-based technique used in this research, Albrecht said that he and his colleagues must do “a better job” of analyzing exoplanetary atmospheres for molecules that have fainter spectral signals than carbon monoxide, such as water. Their next step is to measure the atmospheres of exoplanets that are located slightly farther away from their host stars to see how this distance affects detectable concentrations of carbon monoxide and other molecules.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Thin films show surprising reactivity]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/fuel-cell-strips-0624.html</link>
<story_id>15468</story_id>
<featured>0</featured>
<description><![CDATA[MIT findings of high oxygen activity in thin-film materials might someday lead to greatly increased power production from future fuel cells]]></description>
<postDate>Thu, 24 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100623141251-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100623141251-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100623141251-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Eva Mutoro]]></imageCredits>
<imageCaption><![CDATA[A material called strontium-substituted lanthanum cobalt perovskite, or LSC, whose crystal structure is shown here.]]></imageCaption>
</image>
<body><![CDATA[A surprising MIT laboratory finding about the behavior of a thin sheet of material — less than a thousandth of the thickness of a human hair — could lead to improved ways of studying the behavior of electrodes and perhaps ultimately to improvements in the rate of power production from one type of fuel cell, according to a report published this week.<br /><br />In many cases, thin layers of a material — which may be just a few molecules in thickness — exhibit properties different from solid blocks of the same material. But even though this is a known phenomenon, the nature of the difference the MIT team found in the behavior of thin films of a mineral called perovskite — in this case, deposited as a thin layer on the surface of a crystal of zirconia — “was very much unexpected,” says Yang Shao-Horn, associate professor of mechanical engineering and materials science and engineering at MIT, who led the research. The work was done in collaboration with Hans Christen and Michael Biegalski at Oak Ridge National Laboratory.<br /><br />In fuel cells, a fuel such as hydrogen or methanol reacts in the presence of a catalyst, releasing its energy chemically rather than being burned. As a result, they can produce electricity from fuel without releasing greenhouse gases or other pollutants, and so are considered a promising alternative approach for generating electricity. And unlike batteries, which need to be recharged in a time-consuming process, a fuel cell can be refueled quickly.<br /><br />The main barrier to achieving greater efficiency in fuel cells, which are considered a promising way of supplying electricity for future transportation or stationary power systems, is the slow rate of oxygen reactions at the cathode, one of the two electrical terminals in the device. In present fuel cells, the rate of oxygen reduction (that is, oxygen atoms combining with hydrogen) is the limiting factor in the power output of the device. Many teams are pursuing ways of improving the efficiency and reducing the costs of the two major kinds of fuel cells: solid-oxide fuel cells (SOFCs) and proton-exchange membrane fuel cells (PEMFCs). This work addresses potential improvements in the cathode in SOFCs, which could find application in large-scale systems such as electric power plants. The new research suggests that this activity can be increased by up to a hundredfold by using thin films of certain perovskite compounds.<br /><br />Previous research had found the opposite, that thin films of some perovskite materials were a hundred times less reactive than the bulk material, Shao-Horn says. The new results are published online in the German journal <em>Angewandte Chemie</em>; the lead authors are former student Gerardo la O’ and postdoctoral researcher Sung-Jin Ahn. The work was supported by the NSF, the U.S. Department of Energy, Oak Ridge National Laboratory and the King Abdullah University of Science and Technology.<br /><br />By creating the kind of high-purity thin films of material used in this study — in this case, as thin as 20 nanometers, or billionths of a meter — it is possible to study the details of how the surface of the material reacts in much greater detail than has been possible in research with bulk materials. This research shows that unique thin-film characteristics can enhance catalytic activity. <br /><br />“To our knowledge, this is the first time these thin films have been shown to exhibit” the increased activity, Shao-Horn says. The team is continuing research to verify their hypothesis about the reasons for the increased activity, and to explore a family of materials that may exhibit similar properties. “We are working on determining why” the activity level is so high, Shao-Horn says, suggesting that the increased reactivity of the material may result from a stretching of the surface. This may change the content of oxygen vacancies or the electronic structure of the material, possibilities that are being examined in Shao-Horn’s group.<br /><br />While many fuel cells use electrodes made from precious metals such as platinum, the electrodes in this experiment are made from relatively abundant materials such as cobalt, lanthanum and strontium, Shao-Horn says, so they should be relatively inexpensive to produce. In addition, this material works at much lower temperatures than existing SOFC electrodes, which could be an advantage because “at lower temperatures, material degradation can be much reduced,” she says. Whereas current cells work at temperatures of 800 degrees Celsius or higher, the new approach might lead to materials that could work at 500 degrees Celsius, as was the case in these tests.<br /><br />This work is just the first step, however. Shao-Horn stresses that this is the beginning of a new fundamental research area, and could lead to exploration of a whole family of possible compounds in search of one with an optimal combination of high catalytic activity and high stability. This highly reactive material could find a home in places other than fuel cells: for instance, in high-temperature sensors and in membranes used to separate oxygen from nitrogen and other gases, she says.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[In The World: Easy on the eyes]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/itw-eyes.html</link>
<story_id>15465</story_id>
<featured>0</featured>
<description><![CDATA[Simple, low-cost device that affixes to a cell phone could provide quick eye tests throughout the developing world.]]></description>
<postDate>Wed, 23 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100624110403-0.png</thumbURL>
<smallURL width='140' height='163'>http://web.mit.edu/newsoffice/images/article_images/w140/20100624110403-0.jpg</smallURL>
<fullURL width='368' height='428'>http://web.mit.edu/newsoffice/images/article_images/20100624110403-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Andy Ryan]]></imageCredits>
<imageCaption><![CDATA[Postdoctoral Associate Ankit Mohan demonstrates NETRA (Near-Eye Tool for Refractive Assessment). The user places the device to the eye and, using phone's keypad, aligns patterns in the viewfinder. These alignments provide the data needed to determine a prescription.
]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100622161015-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Tiago Allen]]></imageCredits>
<imageCaption><![CDATA[NETRA combines inexpensive optical elements with interactive software components to create a new,
portable, and low-cost optometry system. ]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[There are two standard systems for determining a prescription for eyeglasses to correct refractive errors such as nearsightedness, farsightedness and astigmatism. One is to have the patient look through a large device called a phoropter, fitted with dozens of different lenses that can be swung into place in front of each eye in various combinations, while the patient tries to read a standard eye chart on the wall ahead. The other uses a more expensive system called an aberrometer that shines a laser into the eye and uses an array of tiny lenses to measure its characteristics, with no interaction from the patient.<br /><br />Now, a team at MIT's Media Lab has come up with a much quicker, simpler and cheaper way to get the same information — a method that is especially suitable for remote, developing-world locations that lack these expensive systems. Two billion people have refractive errors, and according to the World Health Organization, uncorrected refractive errors are the world's second-highest cause of blindness, affecting some 2 percent of the world's population; all these people are potential beneficiaries of the new system. The team is preparing to conduct clinical trials, but preliminary testing with about 20 people, and objective tests using camera lenses, have shown that it can achieve results comparable to the standard aberrometer test.<br /><br />In its simplest form, the test can be carried out using a small, plastic device clipped onto the front of a cell phone's screen. The patient looks into a small lens, and presses the phone's arrow keys until sets of parallel green and red lines just overlap. This is repeated eight times, with the lines at different angles, for each eye. The whole process takes less than two minutes, at which point software loaded onto the phone provides the prescription data. The device is described in a paper by MIT Media Lab Associate Professor Ramesh Raskar, Visiting Professor Manuel Oliveira, and Media Lab student Vitor Pamplona (lead author of the paper) and postdoctoral research associate Ankit Mohan, that will be presented in late July at the annual computer-graphics conference SIGGRAPH. <br /><br /> 





<br /> <strong>NETRA overview </strong><br /><em>Video: Paula Aguilera/Jonathan Williams/MIT Media Lab </em><br /><br /> "Our device has the potential to make routine refractive eye exams simpler and cheaper, and, therefore, more accessible to millions of people in developing countries," Oliveira says.<br /><br />"People have tried all kinds of things, some very clever," as possible replacements for the heavy and expensive conventional eye-testing systems, says Mohan. "The key thing that differentiates ours is that it doesn't require any moving parts." The technology takes advantage of the huge improvements over the last few years in the resolution of digital displays and their widespread proliferation on cell phones, even in some of the world's poorest countries — there are now some 4 billion cell phones in the world. Apart from the software to run on the phone, all that's needed is the snap-on plastic device, which Mohan says can be produced at a cost of about $1 to $2 today but could cost only a few cents in large quantities.<br /><br />The device uses an optical system derived from one some team members developed last year as a way of producing tiny barcodes (called Bokode) that could provide a large amount of information. Raskar explains that he had demonstrated that barcode device to many people, but when he showed it to his wife she had trouble seeing its patterns. He quickly realized that others he had shown it to had been wearing their glasses or contact lenses, but his wife had been looking into it directly and it had revealed the imperfections in her vision. "I said, 'Wow, maybe you don't need such an expensive device'" as those presently being used to test people's vision, Raskar recalls.<br /><strong><br />Focusing at different depths</strong><br /><br />The prototype system Raskar and his group developed as a result of that insight has an array of tiny lenses and a grid of pinholes that, combined with the software on the phone, "forces the user to focus at different depths" so the eye's focusing ability can be measured. Essentially, Raskar explains, the test works by transforming any blurriness produced by aberrations in the eye into an array of separate lines or dots instead of a fuzzy blob, which makes it easier for the user to identify the discrepancy clearly. Rather than estimating which of two views looks sharper, as in conventional eye tests, the user adjusts the display to make the separate lines or dots come together and overlap, which corresponds to bringing the view into sharp focus. The underlying principle is similar to that used by new "adaptive optics" systems that have recently allowed ground-based telescopes to exceed the performance of the Hubble Space Telescope; these sometimes use the same kind of Shack-Hartmann sensors used in eye testing aberrometers.<br /><br />The team will be field-testing the device in the Boston area this summer and will later test it in developing countries. The team already has applied for a patent on the system, named <a href="http://cameraculture.media.mit.edu/netra" target="_blank">NETRA</a> (Near-Eye Tool for Refractive Assessment), and team members won a prize this year in MIT's annual IDEAS competition — a contest for inventions and business ideas that have a potential to make a significant impact in the developing world — and were semi-finalists in the 2010 student-run MIT $100K Business Plan Competition.<br /><br /> Chika Ekeji, a student at the MIT Sloan School of Management who joined the team to help with commercialization of the system, says the group plans to launch production of the device as a for-profit company called PerfectSight, initially targeting parts of Africa and Asia. Ultimately, they also hope to produce a more advanced version that can incorporate its own higher-resolution display and be able to detect other conditions such as cataracts, which could be sold in the developed world as well. IT consultant Margaret McKenna is also assisting the team.<br /><br />Mohan says the way new technology has made such a simple, inexpensive and portable eye-testing device possible is analogous to what has happened in the field of photography. A century ago, he says, to have a picture taken you would go to a special studio that had large, expensive equipment, and then you would wait days to see the resulting photograph. "Today, it's something everyone has in their pocket," he says. <br /><br />By using high-resolution LCD displays with this system, it is potentially not only much faster than today's standard methods, but also "potentially more accurate," Raskar says, although that hasn't yet been demonstrated. <br /><br />"I've tried it myself, and found it to be very impressive," says Ken Perlin, professor of computer science at NYU's Media Research Lab, who is not connected with the project. "I think they found a real sweet spot where the right software can allow a consumer device to perform an accurate measurement of optical acuity that formerly required extremely expensive equipment." And ophthalmologist Francis D'Ambrosio of Massachusetts-based D'Ambrosio Eye Care says that once it is proven though clinical trials, "I feel that this type of technology could go a long way to helping people who otherwise wouldn't have access to eye doctors and refractive eye exams."<br /><br /><em><a href="http://web.mit.edu/newsoffice/topic/in-the-world.html">In The World</a> is a column that explores the ways members of the MIT community are developing technology — from the appropriately simple to the cutting edge — to help meet the needs of communities around the planet, especially those in the developing world. If you have suggestions for future columns, please e-mail <a href="mailto:newsoffice@mit.edu">newsoffice@mit.edu</a>.<br /></em><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Toward the Semantic Web]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/semantic-web-0622.html</link>
<story_id>15459</story_id>
<featured>0</featured>
<description><![CDATA[A new standard from the World Wide Web Consortium brings the Web a step closer to realizing the vision of its inventor, Tim Berners-Lee.]]></description>
<postDate>Tue, 22 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100621214252-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100621214252-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100621214252-1.jpg</fullURL>
</image>
<body><![CDATA[When the World Wide Web went live in 1991, it consisted of static pages of text connected to each other by hyperlinks, and that's pretty much what it remained for years. But from the outset, the Web's inventor, Tim Berners-Lee, had envisioned a much more sophisticated Web, a so-called Semantic Web, which wouldn't just store data but would actually know what it meant. Now an MIT professor, Berners-Lee also directs the <a href="http://www.w3.org/" target="_blank">World Wide Web Consortium (W3C)</a>, a standards body whose industrial participants include everybody from Adobe to Yahoo, and which maintains an office at MIT's Computer Science and Artificial Intelligence Lab. The W3C has just published a new standard that should help bring the Semantic Web that much closer to fruition. <br /><br />If the current Web is like a giant text file — which you can search for instances of particular words — the Semantic Web would be like a database, where every item of information is categorized, and new queries can combine categories in any imaginable way. You could, for instance, search the Web for a restaurant within a mile of a railway station in a town with a theater that offers vegetarian lasagna and at least one lamb dish. And if you wanted the restaurant’s menu, you could pull up just the menu — not page after page of review sites that happened to use the word "menu."<br /><br />But while an ordinary database has categories selected in advance by a programmer, the Semantic Web is "a database where each person controls their own data," says Sandro Hawke, systems architect at the World Wide Web Consortium (W3C). "You have your own parts of the database, so you can put whatever data out there that you want."<br /><br />A giant networked database where people control their own data has obvious advantages: huge numbers of people can contribute to it, and they can ensure that their contributions aren't categorized or recorded incorrectly. But it also has an obvious disadvantage: There's no guarantee that people will organize and label their data in a uniform way.<br /><br />To take a simple example, suppose that two nearby medical clinics put their staff lists online. Semantic Web technologies would allow the clinics to categorize the information in the lists. But suppose that one clinic chose to label the surnames of its doctors "surname," and the other clinic chose the label "last name." A Web search that listed local doctors by "surname" might not pick up those labeled "last name," and vice versa.<br /><br />In fact, an existing Semantic Web standard, the Web Ontology Language, solves this problem. The language gives programmers a way to specify that, for instance, "last name," "surname," and maybe "family name" or just "last" indicate the same types of data.<br /><br /><strong>The case for rules</strong><br /><br />But what if a third clinic, while still adopting Semantic Web technology, chooses to dump first names, last names, and middle initials into a single category, labeled "name"? A direct mapping of category to category will no longer work. Instead, unifying the data on different sites requires a rule, such as, Put everything up to the first space character in "first name," anything after the last space character in "last name," and anything else in "middle."<br /><br />The newly released Semantic Web standard is called the Rule Interchange Format, or RIF, and it gives Web programmers a way to write rules for translating between data on different sites. But that's not the only purpose rules serve on the Web. For instance, Hawke points out, an online Web retailer might offer customers free shipping if their total purchases exceed some threshold in a given time period; but the retailer's Web servers might store no data about its customers other than individual invoices. The code for sifting through the invoices and determining whether to offer the discount is another example of a rule. "Part of the standards game is to have these very different use cases around the same table and then get one standard that can be used in all these different pieces of software," Hawke says.<br /><br />If the RIF standard becomes widely adopted, it's likely to go unnoticed by most Internet users. The Web is already replete with pages that aggregate data from other sites: A personalized Google home page, for instance, might include headlines from several different news sources, weather reports from yet another site, and stock prices from still another. When such content aggregators are already popular online destinations, it can be hard to convey exactly what the advantage of a Semantic Web would be. But as Hawke puts it, "You can always build something to aggregate data you already know about"; what the Semantic Web offers is a way to aggregate data you <em>don't</em> already know about. A small site that lists weekend events in a particular neighborhood, for instance, could retrieve data from sources that didn't even exist when it was built, as long as they categorized their data according to Semantic Web standards.<br /><br />Although it has been nearly 20 years since Berners-Lee launched the first website, if his original idea finally comes to fruition, "it'll happen so quickly that no one will know," Hawke says. "They'll just notice the Internet doing more cool things."<br /><br /> 










<br /> <strong>What is the Semantic Web?</strong><br /> <em>Video: Melanie Gonick</em><br /> Sandro Hawke, systems architect for the World Wide Web Consortium, discusses the new standards that will bring the Semantic Web a step closer to reality. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Enhancing the power of batteries]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/batteries-nanotubes-0621.html</link>
<story_id>15456</story_id>
<featured>0</featured>
<description><![CDATA[MIT team finds that using carbon nanotubes in a lithium battery can dramatically improve its energy capacity.]]></description>
<postDate>Sun, 20 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100623150801-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100623150801-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100623150801-0.jpg</fullURL>
<imageCaption><![CDATA[From left, students Betar Gallant and Seung Woo Lee and professors Yang Shao-Horn and Paula Hammond, in one of the labs where the use of carbon nanotubes in lithium batteries was researched.]]></imageCaption>
</image>
<body><![CDATA[Batteries might gain a boost in power capacity as a result of a new finding from researchers at MIT. They found that using carbon nanotubes for one of the battery’s electrodes produced a significant increase — up to tenfold — in the amount of power it could deliver from a given weight of material, compared to a conventional lithium-ion battery. Such electrodes might find applications in small portable devices, and with further research might also lead to improved batteries for larger, more power-hungry applications.<br /><br />To produce the powerful new electrode material, the team used a layer-by-layer fabrication method, in which a base material is alternately dipped in solutions containing carbon nanotubes that have been treated with simple organic compounds that give them either a positive or negative net charge. When these layers are alternated on a surface, they bond tightly together because of the complementary charges, making a stable and durable film.<br /><br />The findings, by a team led by Associate Professor of Mechanical Engineering and Materials Science and Engineering Yang Shao-Horn, in collaboration with Bayer Chair Professor of Chemical Engineering Paula Hammond, are reported in a paper published June 20 in the journal Nature Nanotechnology. The lead authors are chemical engineering student Seung Woo Lee PhD ’10 and postdoctoral researcher Naoaki Yabuuchi.<br /><br />Batteries, such as the lithium-ion batteries widely used in portable electronics, are made up of three basic components: two electrodes (called the anode, or negative electrode, and the cathode, or positive electrode) separated by an electrolyte, an electrically conductive material through which charged particles, or ions, can move easily. When these batteries are in use, positively charged lithium ions travel across the electrolyte to the cathode, producing an electric current; when they are recharged, an external current causes these ions to move the opposite way, so they become embedded in the spaces in the porous material of the anode.<br /><br />In the new battery electrode, carbon nanotubes — a form of pure carbon in which sheets of carbon atoms are rolled up into tiny tubes — “self-assemble” into a tightly bound structure that is porous at the nanometer scale (billionths of a meter). In addition, the carbon nanotubes have many oxygen groups on their surfaces, which can store a large number of lithium ions; this enables carbon nanotubes for the first time to serve as the positive electrode in lithium batteries, instead of just the negative electrode.<br /><br />This “electrostatic self-assembly” process is important, Hammond explains, because ordinarily carbon nanotubes on a surface tend to clump together in bundles, leaving fewer exposed surfaces to undergo reactions. By incorporating organic molecules on the nanotubes, they assemble in a way that “has a high degree of porosity while having a great number of nanotubes present,” she says.  <br /><br /><strong>Powerful and stable</strong><br /><br />Lithium batteries with the new material demonstrate some of the advantages of both capacitors, which can produce very high power outputs in short bursts, and lithium batteries, which can provide lower power steadily for long periods, Lee says. The energy output for a given weight of this new electrode material was shown to be five times greater than for conventional capacitors, and the total power delivery rate was 10 times that of lithium-ion batteries, the team says. This performance can be attributed to good conduction of ions and electrons in the electrode, and efficient lithium storage on the surface of the nanotubes.<br /><br />In addition to their high power output, the carbon-nanotube electrodes showed very good stability over time. After 1,000 cycles of charging and discharging a test battery, there was no detectable change in the material’s performance.<br /><br />The electrodes the team produced had thicknesses up to a few microns, and the improvements in energy delivery only were seen at high-power output levels. In future work, the team aims to produce thicker electrodes and extend the improved performance to low-power outputs as well, they say. In its present form, the material might have applications for small, portable electronic devices, says Shao-Horn, but if the reported high-power capability were demonstrated in a much thicker form — with thicknesses of hundreds of microns rather than just a few — it might eventually be suitable for other applications such as hybrid cars. <br /><br />While the electrode material was produced by alternately dipping a substrate into two different solutions — a relatively time-consuming process — Hammond suggests that the process could be modified by instead spraying the alternate layers onto a moving ribbon of material, a technique now being developed in her lab. This could eventually open the possibility of a continuous manufacturing process that could be scaled up to high volumes for commercial production, and could also be used to produce thicker electrodes with a greater power capacity. “There isn’t a real limit” on the potential thickness, Hammond says. “The only limit is the time it takes to make the layers,” and the spraying technique can be up to 100 times faster than dipping, she says.<br /><br />Lee says that while carbon nanotubes have been produced in limited quantities so far, a number of companies are currently gearing up for mass production of the material, which could help to make it viable for large-scale battery manufacturing.<br /><br />Yury Gogotsi, professor of materials science at Drexel University, says, “This is an important achievement, because there is a need for energy storage in a thin-film format for powering portable electronic devices and for flexible, wearable electronics. Bridging the performance gap between batteries and electrochemical capacitors is an important task, and the MIT group has made an important step in this direction.”<br /><br />Some uncertainties remain, however. “The electrochemical performance data presented in the article may only be valid for relatively thin films with no packaging,” Gogotsi says, pointing out that the measured results were for just the individual electrode, and results might be different for a whole battery with its multiple parts and outer container. “The question remains whether the proposed approach will work for much thicker conventional electrodes, used in devices that are used in hybrid and electric cars, wind power generators, etc.” But, he adds, if it does turn out that this new system works for such thicker electrodes, “the significance of this work will increase dramatically.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT releases updated iPhone application]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/iphone-update.html</link>
<story_id>15453</story_id>
<featured>0</featured>
<description><![CDATA[Version 2.0 features events calendar and many other features and improvements.]]></description>
<postDate>Fri, 18 Jun 2010 13:34:51 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100618095342-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100618095342-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100618095342-1.jpg</fullURL>
<imageCaption><![CDATA[Version 2.0 of the MIT iPhone app features access to the Institute's events calendar, among other improvements.]]></imageCaption>
</image>
<body><![CDATA[MIT this month released an update to its popular iPhone application that features access to the Institute’s events calendar and many other features.<br /><br />Among the many improvements in version 2.0 of the app are a search function for news; bookmarking of the campus map; sharing of news and events via Twitter; and quicker access to shuttle schedules. <br /><br />Since MIT unveiled the free app in January, more than 10,000 users have downloaded it. To get the app, please visit <a href="http://itunes.apple.com/us/app/mit-mobile/id353590319" target="_blank">the App Store on Apple’s iTunes</a>.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Haiti’s plight]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/haiti-women-06182010.html</link>
<story_id>15448</story_id>
<featured>0</featured>
<description><![CDATA[MIT anthropologist Erica James examines the psychological damage inflicted on the island nation’s inhabitants.]]></description>
<postDate>Fri, 18 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100616110001-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100616110001-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100616110001-1.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of UC Press]]></imageCredits>
<imageCaption><![CDATA[The cover of Erica James's book, “Democratic Insecurities”]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='200' height='200'>http://web.mit.edu/newsoffice/images/article_images/20100616110321-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jon Sachs]]></imageCredits>
<imageCaption><![CDATA[Erica James]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[The destructive earthquake that hit Haiti in January was only the most recent of the Caribbean nation’s troubles. Years of war, political terror, and dictatorship had left the country in deep disrepair. <br /><br />Failed states such as Haiti do not just engender internal strife and violence, however, but can also lead to damaged psyches among their inhabitants, a problem MIT anthropologist Erica James explores in a new book, “Democratic Insecurities: Violence, Trauma, and Intervention,” published by the University of California Press. The psychological toll of political instability, she contends, has never been effectively addressed on a large scale by the outside countries and non-governmental organizations trying to help Haiti. <br /><br />Indeed, the “paradox” of well-meaning aid, James writes, has been that “many of the efforts to rehabilitate the nation and its citizens, and to promote democracy and economic stability, inadvertently reinforced the practices of predation, corruption and repression that they were intended to repair.” Humanitarian efforts did not end social divisions, but often recreated splits among citizens based on the recent past; dueling organizations of victims of political violence, for instance, squabbled over the legitimacy of members’ claims.<br /><br />That is a strong claim, but James’ conclusions come from extensive field studies she conducted while also working in Haiti between 1995 and 2000, providing physical therapy to women who had suffered from personal or political acts of violence, including rape. At the time, Haiti was trying to re-establish a viable democracy after decades of dictatorship and, in the early 1990s, a violent coup. “Democratic Insecurities” details the cases of many women James encountered who had spent years living in a condition of severe “ensekerite,” a local term meaning perpetual insecurity. One woman James worked with, Caroline, was repeatedly beaten throughout her life, then later gang raped, an event that caused her husband to leave her and forced Caroline to put her children in the care of a brother; she was perpetually “anxious, depressed and even suicidal,” and had trouble finding work.<br /><br />Such Haitians needed assistance. But as James sees it, aid programs inadvertently created a “political economy of trauma,” in which citizens competed to assume the role of victims, creating new conflicts and resentments. One rape victim James treated, named Liliane, reintroduced herself to James years later, and gave a public account of having subsequently having been beaten and robbed by a group of men; as James discovered, the second story was false. <br /><br />These fictions were hardly shocking in Haiti, the most impoverished state of the Western Hemisphere, where annual per-capita income was still just $1,340 per person just before the earthquake, according to the International Monetary Fund. Foreign financial assistance was a desired resource for citizens. But aid programs did not provide a long-term platform for growth in the country. One prominent program, the Human Rights Fund, distributed an average of $15 per week for a family of four. In another case, relatives of Haitians who died in the sinking of a ferry received no more than $125 in compensation.<br /><br />To be sure, James did witness Haitians who rebuilt their lives thanks to outside aid. Marie, a woman who was gang-raped in 1994, had returned to steady employment by 2000. “Her receipt of assistance from several agencies and agents in the aid apparatus was in no small part integral to her triumph,” James writes in the book.<br /><br />“Some people received a significant amount of assistance and got back on their feet,” says James. “But there was not a systematic network of institutions that could create sustainable change.” <br /><br /><strong>Getting aid right</strong><br /><br />Aid groups often need to show tangible signs of progress, such as the quick distribution of aid, which means they can rarely take the time to understand a country’s social needs from the inside. <br /><br />“There is typically a disconnect between what people are experiencing on the ground and what is perceived at the administrative level,” James says. “There needs to be much more involvement by high-level decision-makers with people in local communities.” <br /><br />Colleagues applaud James’ ability to link the personal and political in her study. “She’s right at the cutting edge of anthropological studies into conditions of extreme insecurity and political violence, where there is inadequate protection from government and people are vulnerable in failed states,” says Arthur Kleinman, a professor of anthropology at Harvard. “Psychological and psychiatric studies are useful, but she’s also speaking to the broader domain of social policy. We’ve got to address social sources of insecurity and rebuild the fabric of society.” <br /><br />Specifically, James says, rebuilding Haiti today still requires directing aid dollars to the clinics and organizations on the ground specializing in psychological health. This is a largely missing element, she believes, from the “Action Plan” for reconstruction that the Government of Haiti released at a United Nations conference in March generating support for Haiti’s development. “There is no mention of mental health being a priority right now,” James says. “But there are many people with chronic mental-health issues.” As James acknowledges, it is too soon to pass judgment on post-earthquake relief efforts. She would, however, like aid groups to account for the full range of problems Haitians face, and believes there are “hidden costs” to not focusing more on the psychological needs of the population. <br /><br />“I don’t see her saying the commitment to aid shouldn’t be given,” adds Kleinman. “But her work should feed into a rethinking of these issues.”<br /><br />Since the earthquake, James has only been able to speak with just one person she encountered in Haiti in the 1990s. “For most of the people I knew, I’ve had no contact whatsoever,” she says. In the meantime, James is writing a second book, examining how faith-based charities help the Haitian immigrant community integrate itself into American society. And as much as the earthquake has changed the face of Haiti, getting aid right remains vital. “In some ways the world I described in the book no longer exists, but in other ways the issue surrounding aid practices are as important as ever,” James says. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Coordinated stargazing]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/kuiper-06172010.html</link>
<story_id>15446</story_id>
<featured>0</featured>
<description><![CDATA[MIT astronomer leads the first team to study a Kuiper Belt object during a stellar occultation.]]></description>
<postDate>Thu, 17 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100616092602-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100616092602-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100616092602-1.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA]]></imageCredits>
<imageCaption><![CDATA[An artist's rendering of a Kuiper Belt object.]]></imageCaption>
</image>
<body><![CDATA[Far beyond the orbit of Neptune in a region of the outer solar system known as the Kuiper Belt float thousands of icy, moon-sized bodies called Kuiper Belt objects (KBOs). Astronomers think they are the remnants of the bodies that slammed together to form the planets more than 4 billion years ago. Unlike Earth, which has been continually eroded by wind and water since it was formed, KBOs haven’t changed much over time and may hold clues about the early solar system and planet formation. <br /><br />Until now, astronomers have used telescopes to find KBOs and obtain their spectra to determine what types of ices are on their surface. They have also used thermal-imaging techniques to get a rough idea of the size of KBOs, but other details have been difficult to glean. While astronomers think there are about 70,000 KBOs that are larger than 100 kilometers in diameter, the objects’ relatively small size and location make it hard to study them in detail. One method that has been has been proposed for studying KBOs is to observe one as it passes briefly in front of a bright star; such events, known as stellar occultations, have yielded useful information about other planets in the solar system. By monitoring the changes in starlight that occur during an occultation, astronomers can determine the object’s size and temperature, whether it has any companion objects and if it has an atmosphere. <br /><br />The trick is to know enough about the orbit of a KBO to be able to predict its path and observe it as it passes in front of a star. This was done successfully for the first time last October when a team of 18 astronomy groups led by James Elliot, a professor of planetary astronomy in MIT’s Department of Earth, Atmospheric and Planetary Sciences, observed an occultation by an object named “KBO 55636.” <br /><br />As Elliot and his colleagues report in a paper published June 17 in <em>Nature</em>, the occultation provided enough data to determine the KBO’s size and albedo, or how strongly it reflects light. The surface of 55636 turns out to be as reflective as snow and ice, which surprised the researchers because ancient objects in space usually have weathered, dull surfaces. The high albedo suggests that the KBO’s surface is made of reflective water-ice particles, and that would support a theory about how the KBO formed. Many researchers believe there was a collision that occurred one billion years ago between a dwarf planet in the Kuiper Belt known as Haumea and another object that caused Haumea’s icy mantle to break into a dozen or so smaller bodies, including 55636. <br /><br />More importantly, the research demonstrates that astronomers can predict occultations accurately enough to contribute to a new NASA mission known as the Stratospheric Observatory For Infrared Astronomy (SOFIA) that completed its first in-flight observations in May. A Boeing 747SP aircraft that has a large telescope mounted in its rear fuselage, SOFIA can record infrared measurements of celestial objects that are not possible from the ground. Elliot hopes his research will help guide future flights of SOFIA to observe stellar occultations in detail. <br /><br /><strong>Betting on an occultation</strong><br /><br />Elliot, who has been studying 55636’s orbit for five years, thought it would most likely pass in front of an unnamed star on Oct. 9, 2009. But the KBO’s small size made it difficult to predict exactly where the object would travel, and so, to be on the safe side, he and his colleagues assembled a network of 18 observation stations along a 5,900-kilometer stretch of the Earth’s surface that corresponded to the KBO’s predicted shadow path. Such a strategy “covered our uncertainty about where the path would go, both to the north and to the south,” Elliot explains. “It was our way of hedging our bets.” <br /><br />While some of the stations couldn’t observe because of weather, and others simply didn’t detect the occultation, two stations in Hawaii captured data on the changes in starlight that occurred during the roughly 10-second occultation. After measuring the exact amount of time that the star was blocked from view, as well as the velocity with which the shadow of 55636 moved across Earth, the researchers calculated that the KBO has a radius of about 143 kilometers. Knowing this, they could then calculate the object’s albedo.<br /><br />The highly reflective surface of 55636 is perplexing because the surfaces of celestial bodies in the outer solar system are supposed to darken over time as a result of dust accumulation and exposure to solar radiation. John Stansberry, an astronomer at the University of Arizona, says that if Elliot’s “solid piece of work” can be confirmed in follow-up research, then the results show that 55636 is “an extremely unique” KBO because similarly sized KBOs are thought to have significantly smaller albedos. “The result suggests that it would be worthwhile to try to measure the albedos of other Haumea family members to see if they are also very high,” says Stansberry. <br /><br />Although other highly reflective bodies in the solar system, such as the dwarf planet Pluto and Saturn's moon Enceladus, have their surfaces continuously renewed with fresh ice from the condensation of atmospheric gases or by volcanic activity that spews water instead of lava, 55636 is too small for these mechanisms to be at work, says Elliot. He has no plans to investigate the cause of the high albedo but will continue to collect data about the orbits and positions of the largest KBOs in order to predict future occultations with enough accuracy that he doesn’t have to rely on a vast network of observers. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Researchers seek to put the squeeze on cancer]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/angiogenesis-06152010.html</link>
<story_id>15438</story_id>
<featured>0</featured>
<description><![CDATA[Cell contractions may be key to initiating new blood-vessel growth near tumors.]]></description>
<postDate>Tue, 15 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100611145614-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100611145614-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100611145614-1.jpg</fullURL>
<imageCaption><![CDATA[MIT and Tufts researchers have shown that mechanical forces from cells that surround small blood vessels may control the growth of new vessels.]]></imageCaption>
</image>
<body><![CDATA[Cancer researchers have been studying angiogenesis — the growth of new blood vessels — since the early 1970s, when Judah Folkman first theorized that tumors could be destroyed by cutting off their blood supply. <br /><br /> For most of that time, scientists have focused on the biochemical signals that promote angiogenesis, in hopes of finding drugs that can starve tumors by blocking their ability to release the proteins that promote vessel growth. More recently, a few scientists have taken a new approach: studying how contractions in nearby cells can stimulate angiogenesis. <br /><br /> Krystyn Van Vliet, associate professor of materials science and engineering at MIT, and researchers at Tufts University recently showed that cells called pericytes, which surround small blood vessels, generate contractions that could serve as a mechanical signal to initiate angiogenesis. “Up to now, people have assumed that the role of pericytes was biochemical in nature,” says Van Vliet. <br /><br /> Pinpointing the role of those mechanical signals could help researchers develop drugs that either promote angiogenesis to enhance wound healing or suppress the harmful angiogenesis that leads to tumor growth or vision loss (in age-related macular degeneration or diabetes-induced retinal damage), says Ira Herman, a professor of physiology at Tufts University School of Medicine and expert in pericyte cell biology who collaborated with Van Vliet on this study. <br /><br /><strong> The search for new drugs</strong> <br /><br /> Angiogenesis is vital for all body tissues: Without access to functioning blood vessels and their life-sustaining stream of oxygen and nutrients, cells can’t survive. Cancer cells, with their revved-up metabolism, especially need to stimulate blood-vessel growth because they require more fuel than ordinary cells. <br /><br /> Folkman’s theory on angiogenesis and tumors spawned an entire research field — known as anti-angiogenesis therapy — as biologists scrambled to figure out how cancer cells turn on angiogenesis, and, more importantly from a treatment standpoint, how to shut it off. <br /><br /> In the past 40 years, scientists have pinpointed more than a dozen proteins, known as growth factors, that appear to be involved in angiogenesis. They have also identified drugs, including endostatin and angiostatin, that have shown some ability to slow tumor growth by interfering with those factors. Those drugs are now being tested in clinical trials. The FDA recently approved an antibody drug known as bevacizumab for use in some metastatic cancers. That drug, marketed as Avastin, targets vascular endothelial growth factor, a protein that acts on endothelial cells (cells that form the lining of blood vessels). <br /><br /> Those represent the first generation of angiogenesis drugs, but the field holds promise for many more once scientists learn more about angiogenesis and the different cells involved, says Marsha Moses, director of the Vascular Biology Program at Children’s Hospital in Boston, who believes pericytes are a very promising target.  <br /><br /> Van Vliet and Herman’s research is an important step towards better understanding pericytes’ role in angiogenesis, Moses says. “They have not necessarily shown the direct effect on angiogenesis, but they’ve laid the groundwork for a series of studies” that should reveal whether pericytes will become a viable drug target, she says. <br /><br /><strong> Nanoscale measurements </strong><br /><br /> Herman, a cell biologist, has been studying pericytes since the early 1980s. In 2007, he first reported that pericyte contractions might trigger angiogenesis, but he didn’t have a good way to measure the forces they generated, or to visualize how they would affect surrounding cells. <br /><br /> He recruited Van Vliet, an expert in atomic force microscopy who studied angiogenesis as a postdoctoral associate in Moses’ lab at Children’s Hospital, to explore this issue. In graduate school, Van Vliet had worked on measuring the mechanical behavior of metals at the nanoscale and studying how that behavior influences macroscopic behavior.  <br /><br /> “I wanted to see if I could apply some of these nanoscale mechanical tools to biological problems,” such as how cells interact with materials, and how those chemical and mechanical interactions at the cell-material interface can affect cell functions such as angiogenesis, says Van Vliet. <br /><br /> In a paper published in April in the <em>Journal of Physics: Condensed Matter</em>, Van Vliet, Herman and MIT students Sunyoung Lee, Adam Zeiger and John Maloney and Tufts research associate Maciej Kotecki reported measuring and imaging the force of pericyte contractions using an atomic force microscope — the first time that had ever been done. <a href="http://ocw.mit.edu/courses/materials-science-and-engineering/3-052-nanomechanics-of-materials-and-biomaterials-spring-2007/" target="_blank">Atomic force microscopy</a> generates very high-resolution images (about 5-nanometer resolution) by “feeling” the surface of a sample with a tiny probe tip. <br /><br /> The researchers believe that pericyte contractions exert influence on blood-vessel growth by either directly tugging on endothelial cells or altering the stiffness of the membrane underlying the endothelial cells. <br /><br /> In current studies, Van Vliet and Herman are investigating how the pericyte forces affect endothelial cells’ growth rate, shape and chemical secretions. They hope to determine whether pericyte contractions can act as an “angiogenic switch,” turning on blood vessel growth. They are also trying to determine the mechanism by which the pericytes exert their forces, which appears to involve inducing a network of cytoskeletal proteins to tense up, pulling in the cell membrane. <br /><br /> The researchers have already identified a few drugs that can interfere with the pericyte contractions by disrupting the activity of those cytoskeletal proteins. Such drugs could potentially be useful for anti-angiogenesis therapy, but only if they could be delivered specifically to the target cells, says Herman. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[A new use for gold]]></title>
<author><![CDATA[Diana LaScala-Gruenewald, MIT News correspondent]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/sticky-nanoparticles-0611.html</link>
<story_id>15435</story_id>
<featured>0</featured>
<description><![CDATA[Engineers turn a drawback — the stickiness of gold nanoparticles — into an advantage.]]></description>
<postDate>Fri, 11 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100623150912-0.png</thumbURL>
<smallURL width='140' height='116'>http://web.mit.edu/newsoffice/images/article_images/w140/20100623150912-0.jpg</smallURL>
<fullURL width='368' height='307'>http://web.mit.edu/newsoffice/images/article_images/20100623150912-0.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy Kimberly Hamad-Schifferli]]></imageCredits>
<imageCaption><![CDATA[An image of gold nanoparticles.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100610152648-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Christopher Huang]]></imageCredits>
<imageCaption><![CDATA[Associate Professor Kimberly Hamad-Schifferli of the Departments of Biological Engineering and Mechanical Engineering]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Gold nanoparticles — tiny spheres of gold just a few billionths of a meter in diameter — have become useful tools in modern medicine. They’ve been incorporated into miniature drug-delivery systems to control blood clotting, and they’re also the main components of a device, now in clinical trials, that is designed to burn away malignant tumors. <br /><br />However, one property of these particles stands in the way of many nanotechnological developments: They‘re sticky. Gold nanoparticles can be engineered to attract specific biomolecules, but they also stick to many other unintended particles — often making them inefficient at their designated task.  <br /><br />MIT researchers have found a way to turn this drawback into an advantage. In a paper <a href="http://pubs.acs.org/doi/full/10.1021/nn100362m?prevSearch=hamad-schifferli&amp;searchHistoryKey=" target="_blank">recently published</a> in <em>American Chemical Society Nano</em>, Associate Professor Kimberly Hamad-Schifferli of the Departments of Biological Engineering and Mechanical Engineering and postdoc Sunho Park PhD ’09 of the Department of Mechanical Engineering reported that they could exploit nanoparticles’ stickiness to double the amount of protein produced during in vitro translation — an important tool that biologists use to safely produce a large quantity of protein for study outside of a living cell.  <br /><br />During translation, groups of biomolecules come together to produce proteins from molecular templates called mRNA. In vitro translation harnesses these same biological components in a test tube (as opposed to in vivo translation, which occurs in live cells), and a man-made mRNA can be added to guarantee the production of a desired protein. For example, if a researcher wanted to study a protein that a cell would not naturally produce, or a mutated protein that would be harmful to the cell in vivo, he might use in vitro translation to create large quantities of that protein for observation and testing. But there’s a downside to in vitro translation: It is not as efficient as it could be. “You might get some protein one day, and none for the next two,” explains Hamad-Schifferli.<br /><br />With funding from the Institute of Biomedical Imaging and Bioengineering, Hamad-Schifferli and her co-workers initially set out to design a system that would prevent translation. This process, known as translation inhibition, can stop the production of harmful proteins or help a researcher determine protein function by observing cell behavior when the protein has been removed. To accomplish this, Hamad-Schifferli attached DNA to gold nanoparticles, expecting that the large nanoparticle-DNA (NP-DNA) aggregates would block translation. <br /><br />She was discouraged, however, to find that the NP-DNA did not decrease protein production as expected. In fact, she had some unsettling data suggesting that instead of inhibiting translation, the NP-DNA were boosting it. “That’s when we put on our engineering caps,” recalls Hamad-Schifferli.  <br /><br />It turns out that the sticky nanoparticles bring the biomolecules needed for translation into close proximity, which helps speed the translation process. Additionally, the DNA part of the NP-DNA complex is designed to bind to a specific mRNA molecule, which will be translated into a specific protein. The binding must be tight enough to hold the mRNA in place for translation, but loose enough that the mRNA can also attach to the other molecules necessary for the process. Because the designed DNA molecule has a specific mRNA partner, that mRNA in a solution of many similar molecules can be enhanced without having to be isolated. <br /><br />In addition to enhancing in vitro translation, Hamad-Schifferli’s NP-DNA complexes may have other applications. According to Ming Zheng, a research chemist with the National Institute of Standards and Technology, they could be combined with carbon nanotubes — tiny, hollow cylinders that are incredibly strong for their size. They may ultimately be the cornerstone of transport systems that ferry drugs into cells or between cells. The stickiness of the NP-DNA might enhance the speed and accuracy of such a drug-delivery system.   <br /><br />Although Hamad-Schifferli is confident that her discovery will make in vitro translation more reliable and efficient, she is not done. She hopes to tinker with her system to further enhance protein production in vitro, and see if the system can be applied to enhance translation in live cells. To help reach these goals, she must design and conduct experiments to determine which molecules are involved in the enhancement process, and how they interact. “The upside is that we’ve been lucky,” Hamad-Schifferli says, reflecting on her discovery. “The downside is that it will be difficult to tease out exactly how the system works.”  <br /> <br />]]></body>
</item>
<item>
<title><![CDATA[Multitasking is no problem for these brain cells]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/multitask-neuro-0610.html</link>
<story_id>15433</story_id>
<featured>0</featured>
<description><![CDATA[Scientists find that neurons in the brain’s planning center can handle more than one kind of job.]]></description>
<postDate>Thu, 10 Jun 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100609190048-1.png</thumbURL>
<smallURL width='140' height='103'>http://web.mit.edu/newsoffice/images/article_images/w140/20100609190048-1.jpg</smallURL>
<fullURL width='368' height='273'>http://web.mit.edu/newsoffice/images/article_images/20100609190048-1.jpg</fullURL>
<imageCaption><![CDATA[Previous studies showed that when monkeys are trained to categorize animals by distinguishing cats from dogs, some neurons in the prefrontal cortex become tuned to the concept of “cat” while others respond to the idea of “dog.” This new study trained the monkeys to perform two different categorization tasks — distinguishing cats from dogs and sedans from sports cars. ]]></imageCaption>
</image>
<body><![CDATA[In humans and other primates, the prefrontal cortex is the seat of high-level functions such as learning, decision making, and planning. Neuroscientists have long wondered whether neurons in that part of the brain are specialized for one type of task or if they are “generalists” — that is, able to participate in many functions. A new study from MIT’s Picower Institute for Learning and Memory comes down in favor of the generalist theory. <br /><br />MIT professor Earl Miller and others in his lab showed that when they trained monkeys to perform two different categorization tasks, about half of the neurons involved could switch between the two. The findings, reported in the June 10 issue of the journal <em>Neuron</em>, suggest that neurons of the prefrontal cortex have a much greater ability to adapt to different cognitive demands than neurons in other parts of the brain. These results support ideas about the malleability of neurons — nervous-system cells that process and transmit information — that Miller first proposed a decade ago.<br /><br />Miller, the Picower Professor of Neuroscience in MIT’s Department of Brain and Cognitive Sciences, says he’s not surprised by the findings. “We have a lot of mental flexibility,” he says. “We can change our topic of conversation, we can change what we’re thinking about. Some part of the brain has to have that flexibility on a neural level.”<br /><strong><br />Listening to single neurons</strong><br /><br />Most neuroscientists who study brain activity in monkeys train the animals on only one task, so until now it had been impossible to reveal whether single neurons in the prefrontal cortex could be involved in more than one job.<br /><br />In previous studies, Miller has shown that when monkeys are trained to categorize animals by distinguishing cats from dogs, some neurons in the prefrontal cortex become tuned to the concept of “cat” while others respond to the idea of “dog.”<br /><br />This time, Miller, postdoctoral fellow Jason Cromer, and research scientist Jefferson Roy trained the monkeys to perform two different categorization tasks — distinguishing cats from dogs and sedans from sports cars. They recorded activity from about 500 neurons in the monkeys’ prefrontal cortex as the animals switched back and forth between the tasks. <br /><br />Although they found that some neurons were more attuned to car images and others to animal images, they also identified many neurons that were active during both tasks. In fact, these “multitasking” neurons were best at making correct identifications in both categories.   <br /><br />The findings suggest that neurons in the prefrontal cortex have a unique ability to adapt to different tasks, says Miller. In other parts of the brain, earlier research has shown, most neurons are highly specialized. Neurons in the visual cortex, for example, are programmed to respond to very specific inputs, such as a vertical line or a certain color. Some have even been shown to fire only in response to one particular face.<br /><br />“Our results suggest that the prefrontal cortex is different from the sensory cortex and the motor cortex. It’s highly plastic,” says Miller. “That’s important, because it means the human brain has the capacity to absorb a lot of information.”<br /><br />The Neuron study focused on two categorization tasks, but Miller hopes to run another study in which the monkeys learn a third task involving some other cognitive function. That could give another hint about how much information our brains can handle, says David Freedman, an assistant professor of neurobiology at the University of Chicago.<br /><br />“We’re very good at learning dozens, hundreds, or even thousands of categories,” he says. “You wonder if there is some limit, or would these neurons be as flexible as we are as observers?” <br /><br />Freedman says he would also be interested to see whether the same prefrontal-cortex neurons can multitask between activities that involve different kinds of sensory inputs — for example, a visual task and an auditory task.<br /><br /><strong>Overstimulation</strong><br /><br />Meanwhile, Miller has a study under way that he believes could demonstrate a biological basis for the impaired categorization ability often seen in people with autism. Autistic children often have a hard time understanding that two slightly different objects — for example, a red toothbrush and a blue toothbrush — both belong to the same category. <br /><br />Miller theorizes that an evolutionarily older part of the brain, known as the basal ganglia, gathers information about new objects, and the prefrontal cortex learns how to categorize them. “The basal ganglia learn the pieces of the puzzle, and the prefrontal cortex puts the pieces together,” he says.<br /><br />In his current study, Miller is monitoring brain activity in monkeys as they learn a categorization task. He expects to find a sharp peak in prefrontal-cortex activity at the moment when the monkeys learn that certain objects belong to the same category.<br /><br />Eventually, he hopes to show that in autism, the balance between those two brain regions is thrown off: there is either too much activity in the basal ganglia or not enough in the prefrontal cortex.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Suresh formally nominated to lead NSF]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/suresh-nominated.html</link>
<story_id>15429</story_id>
<featured>0</featured>
<description><![CDATA[President Obama sends MIT engineering dean’s nomination to the U.S. Senate]]></description>
<postDate>Wed, 09 Jun 2010 14:01:52 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100609100516-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100609100516-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100609100516-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Justin Knight]]></imageCredits>
<imageCaption><![CDATA[U.S. President Barack Obama has nominated Subra Suresh, dean of the MIT School of Engineering, to serve as the next director of the National Science Foundation.]]></imageCaption>
</image>
<body><![CDATA[President Barack Obama has formally nominated MIT Dean of Engineering Subra Suresh to be the next director of the National Science Foundation. <br /><br />Obama sent Suresh’s nomination to the U.S. Senate for confirmation on Tuesday, June 8. On June 3, Obama had announced his intention to nominate the dean to the position, which carries a six-year term.<a href="http://web.mit.edu/newsoffice/2010/suresh-nsf-06039987.html"> Read MIT News’ full coverage of that announcement</a><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Quark gluon plasma]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/exp-quark-gluon-0609.html</link>
<story_id>15427</story_id>
<featured>0</featured>
<description><![CDATA[By colliding particles, physicists hope to recreate the earliest moments of our universe, on a much smaller scale.]]></description>
<postDate>Wed, 09 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100608165022-1.png</thumbURL>
<smallURL width='140' height='112'>http://web.mit.edu/newsoffice/images/article_images/w140/20100608165022-1.jpg</smallURL>
<fullURL width='368' height='296'>http://web.mit.edu/newsoffice/images/article_images/20100608165022-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Brookhaven National Laboratory]]></imageCredits>
<imageCaption><![CDATA[A visualization of one of the first full-energy collisions between gold ions at Brookhaven Lab's Relativistic Heavy Ion Collider, as captured by the Solenoidal Tracker At RHIC (STAR) detector.]]></imageCaption>
</image>
<body><![CDATA[For a few millionths of a second after the Big Bang, the universe consisted of a hot soup of elementary particles called quarks and gluons. A few microseconds later, those particles began cooling to form protons and neutrons, the building blocks of matter.<br /><br />Over the past decade, physicists around the world have been trying to re-create that soup, known as quark-gluon plasma (QGP), by slamming together nuclei of atoms with enough energy to produce trillion-degree temperatures. <br /><br />“If you’re interested in the properties of the microseconds-old universe, the best way to study it is not by building a telescope, it’s by building an accelerator,” says <a href="http://web.mit.edu/physics/people/faculty/rajagopal_krishna.html" target="_blank">Krishna Rajagopal</a>, an MIT theoretical physicist who studies QGP.<br /><br />Quarks and gluons, though they make up protons and neutrons, behave very differently from those heavier particles. Their interactions are governed by a theory known as quantum chromodynamics, developed in part by MIT professors <a href="http://en.wikipedia.org/wiki/Jerome_Isaac_Friedman" target="_blank">Jerome Friedman</a> and <a href="http://web.mit.edu/physics/people/faculty/wilczek_frank.html" target="_blank">Frank Wilczek</a>, who both won Nobel prizes for their work. However, the actual behavior of quarks and gluons is difficult to study because they are confined within heavier particles. The only place in the universe where QGP exists is inside high-speed accelerators, for the briefest flashes of time.<br /><br />In 2005, scientists at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory reported creating QGP by smashing gold atoms together at nearly the speed of light. These collisions can produce temperatures up to 4 trillion degrees — 250,000 times warmer than the sun’s interior and hot enough to melt protons and neutrons into quarks and gluons. <br /><br />The resulting super-hot, super-dense blob of matter, about a trillionth of a centimeter across, could give scientists new insights into the properties of the very early universe. So far, they have already made the surprising discovery that QGP is a nearly frictionless liquid, not the gas that physicists had expected. <br /><br />By doing higher-energy collisions, scientists now hope to find out more about the properties of quark gluon plasma and whether it becomes gas-like at higher temperatures. They also want to delve further into the very surprising similarities that have been seen between QGP and ultracold gases (near absolute zero) that MIT’s Martin Zwierlein and others have created in the laboratory. Both substances are nearly frictionless, and theoretical physicists suspect that string theory may explain both phenomena, says Rajagopal.<br /><br />At the Large Hadron Collider in Geneva, MIT faculty Gunther Roland, Wit Busza and Boleslaw Wyslouch are among the physicists planning to double the temperature achieved at Brookhaven, offering a glimpse of an even-earlier stage of the universe’s formation.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Emeritus: Sound reasoning]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/emeritus-halle-0608.html</link>
<story_id>15423</story_id>
<featured>0</featured>
<description><![CDATA[MIT emeritus linguist Morris Halle sees his influence live on — and take some unexpected directions]]></description>
<postDate>Tue, 08 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100607132326-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100607132326-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100607132326-1.jpg</fullURL>
<imageCredits><![CDATA[Photos: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[MIT's professor of linguistics Donca Steriade (left) and institute professor emeritus Morris Halle (right), experts in phonology, discussing a basic question: Why do human languages sound the way they do?]]></imageCaption>
</image>
<body><![CDATA[<em>Editor’s note: This is the first in a </em><em><a href="http://web.mit.edu/newsoffice/topic/emeritus.html" target="_blank">series of  articles</a></em><em> linking the work of MIT’s emeritus faculty members with the current state of research in their given fields.</em><br /><br />In 1978, a PhD candidate in linguistics named Donca Steriade arrived at MIT not long after leaving communist Romania. Steriade recalls that, excited about her studies and fearful of failure, she thought, “If I don’t know what they expect me to know, they’re going to send me back to Romania.”<br /><br />Steriade took a class with Morris Halle and received a jolt. Halle, now 86 and an Institute Professor Emeritus at MIT, is one of the 20th century’s most influential academic linguists. He helped create modern phonology, the study of the production of sound in language. <br /><br />“Morris was very stern with all of us,” says Steriade. “His first reaction, when he was not completely satisfied with our work, was to ask us if we wanted to stay in linguistics. He would say: ‘Do you really want to be in this field?’ My first year was rather traumatic, because Morris asked me that multiple times.” When Steriade handed in her first paper to Halle, it came back with a blunt comment: “A garden of horrors.” <br /><br />Steriade had come to MIT to tackle a major problem in phonology: Determining the extent to which the broad laws that apply to sound patterns in all languages are due to the fact that all people share the same kind of vocal apparatus. Halle, for one, suspected that other factors played a role; Steriade wanted to see how much speech perception — our hearing — influenced the rules of spoken language.  <br /><br />In time, Steriade not only survived her first year of graduate school, but became an accomplished professor of linguistics at MIT. And now she even defends Halle’s prickly approach. “I think it was right to ask if we wanted this profession,” Steriade says. “It was difficult to find jobs, and you really had to be dedicated to the subject to seek this kind of life.” Moreover, students who did want to be in linguistics found themselves in a vibrant intellectual community.<br /><br />“It is impossible not to get excited in Morris’ classes,” the linguists Stephen Anderson (now of Yale) and Paul Kiparsky (now of Stanford) once wrote in a published essay about Halle, noting, “what Morris says in class … is often outrageous, but it sets off something productive in his listeners.” Halle, they added, was “largely responsible for the special quality of life that has characterized linguistics at MIT.” <br /><br />He is also something of an Institute grandfather. Two of Steriade’s own PhD students (whom she taught at UCLA), phonologists Adam Albright and Edward Flemming, are now MIT professors and have offices just down the hall in the Stata Center. <br /><br /><strong>‘Noam, where’s your other office?’ </strong><br /><br />Halle’s route to American academia was circuitous. He was born into a Latvian Jewish family in 1923. After Germany invaded Poland, Halle’s father, a businessman, moved the family to the United States in 1940. (Not all of Halle’s relatives left, however, and the Nazis soon decimated Latvia’s Jewish population.) Halle was drafted into the U.S. Army, served in France during World War II — “I didn’t have a glorious record; I just did what they told me” — then completed his graduate work at Harvard with the eminent linguist Roman Jakobson.<br /><br /><img src="http://web.mit.edu/newsoffice/images/stories/halle-steriade_3.jpg" border="0" align="left" />By 1951, Halle had also been hired by MIT. “I got a job at MIT mainly because I could teach languages,” he says. Linguistics at the Institute then consisted of language courses, not research, and Halle is fluent in English, German, French, Russian, Latvian and Hebrew.<br /><br />MIT became the world center of linguistics research after Halle suggested it could use a promising young research fellow at Harvard: Noam Chomsky. “In the summer of 1955, Chomsky, with whom I was friends, needed a job,” says Halle. “So I went to the department head and I said, ‘Why don’t we hire Chomsky?’ ” MIT inaugurated its PhD program in linguistics in 1960.<br /><br />In good MIT fashion, Halle and Chomsky had unglamorous offices next to each other. “When I came in 1978,” recalls Steriade, “the department was spread throughout Building 20, and Noam and Morris had offices that were the two most miserable holes in the whole place.” <br /><br />As Halle now jokes: “I would say to Noam, ‘Where’s your other office?’”<br /><br />In the 1950s, Chomsky upended linguistics by developing his theory of Universal Grammar, which holds that language is not simply acquired through social learning. Instead we have an innate faculty for language; we have to learn words and rules in any language, but all languages have underlying organizational commonalities. And while he focused on syntax — the principles governing the structure of language — Chomsky collaborated with Halle extensively to extend these ideas to phonology, culminating in their seminal 1968 book, The Sound Pattern of English. <br /><br />In the work, Halle and Chomsky laid out for the first time the elaborate series of rules in English that turn written words into vocal utterances. These laws are linked to syntax. Our voices fall when we say “blackboard,” but rise when we say “black board.” The first is a noun, the second an adjective and noun; this difference in syntax underlies the difference in sound.  <br /><br /><strong>From Romania to Cambridge</strong><br /><br />Meanwhile, Steriade was clearing her own unusual path to MIT. Her father, Mircea Steriade, was a renowned neuroscientist who left Romania during the Ceausescu regime. Steriade studied classics in Bucharest, then left, too. “I wanted to pursue the project that Morris and Noam had defined,” Steriade says, referring to the MIT-based effort to find the universal elements of language. Her doctoral thesis examined the shared properties of syllables across languages, finding that, despite appearances, even ancient Greek syllables have important commonalities with those of modern languages. <br /><br />Since then, Steriade has focused on modern Indo-European languages. Among other things, she used insights in speech perception to make new observations about a phenomenon called “neutralization,” in which distinct sounds in words become smoothed over to help speech flow. (One example in English is the way “writer” and “rider” sound alike.) Steriade has found that across languages, neutralization occurs where contrasts are hardest for listeners to detect. Thus the way we hear influences the evolution of the rules of speech. This is the kind of discovery Steriade hoped to make when she arrived at MIT in 1978: Spoken language is shaped by more than just our physical ability to speak. <br /><br />Steriade has also become a valued mentor herself. “She has a great ability to make phonological theory exciting,” says Flemming. It seems Steriade has never called a student’s work “a garden of horrors,” either. “I turned in plenty of papers to Donca, and I never got a comment like that,” allows Flemming. “She can be very charitable toward modest papers if they at least have interesting ideas.” <br /><br /><strong>Sub-optimal theory?</strong><br /><br />While Steriade proudly identifies herself as a student of Halle, today they take opposite sides of a prominent linguistics debate over a concept called Optimality Theory, which differs from Halle’s account about the mechanisms we use to turn words into sounds . Why do we pronounce the final “c” in “electric” with a “k” sound, but then give the same letter an “s” sound when we say the word “electricity”? Why does the “s” in “consign” sound like, well, “s,” while in “resign” the same letter now has a “z” sound? <br /><br />Halle believes that as we learn to speak a language from others, we acquire a kind of pronunciation checklist with an order of operations; this allows us to make correct alterations even as we learn new rules . “This is the most interesting puzzle in the sound structure of language,” says Halle. “The answer, I believe — and I taught this to Donca, whether she agrees or not — is that there are computational steps. So one step is: Change ‘s’ to ‘z’ [as a sound] if it comes between vowels. Another rule says: Change ‘k’ to ‘s’ [as a sound] before ‘ity,’ as in ‘electricity.’ A small computer program in your head does that.” <br /><br />In the early 1990s, some phonologists began arguing that we do not alter sounds through these steps, but instead use a set of “conflicting constraints” arranged in a hierarchy. “Not all of these conflicting preferences can be satisfied at once,” explains Steriade, “so you have to prioritize among them.” A preference that we keep the “k” sound in “electric” might be overruled by a preference for the “s” sound when possible. Optimality Theory’s advocates still seek universal components of language; however, they think learning a new language entails acquiring a new ranking of globally similar preferences.<img src="http://web.mit.edu/newsoffice/images/stories/halle-steriade_10.jpg" border="0" align="right" /><br /><br />Steriade diplomatically claims that in one way this debate “is a tiny difference that has been magnified in contemporary phonology.” We may indeed run though a sequence of computations while turning underlying words into sounds, she suggests, so in this regard, while optimality theorists “hoped they were going to eliminate the view Morris has, it’s become obvious that’s not possible.” <br /><br />On the other hand, Steriade believes there is still a “fundamental conceptual difference” between the views. While Halle describes words becoming sounds through a more arbitrary, ad-hoc series of conventions that evolve in a given language, Optimality Theory asserts that the conflicting preferences that apply to pronunciation are not arbitrary at all.<br /><br />In theory, Halle should be amenable to whatever the balance of evidence suggests. In 1974, Halle delivered an address to the Linguistic Society of America in which he declared, “the linguist must be prepared to lose as well as to win.” How does that idea strike him today? “I was younger then,” quips Halle, adding: “If you believe something, put your money where your mouth is, and 10 years later you will know if you were right or wrong. Nobody else will need to tell you when you’re whipped.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[How the brain recognizes objects]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/people-images-0607.html</link>
<story_id>15418</story_id>
<featured>0</featured>
<description><![CDATA[A new computational model sheds light on the workings of the human visual system and could help advance artificial-intelligence research, too.]]></description>
<postDate>Mon, 07 Jun 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100607125544-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100607125544-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100607125544-0.jpg</fullURL>
<imageCredits><![CDATA[Images courtesy of Sharat Chikkerur]]></imageCredits>
<imageCaption><![CDATA[A new computational model of how the primate brain recognizes objects creates a map of “interesting” features (right) for a given image. The model’s predictions of which parts of the image will attract a viewer’s attention (green clouds, left) accord well with experimental data (yellow and red dots).]]></imageCaption>
</image>
<body><![CDATA[Researchers at MIT’s McGovern Institute for Brain Research have developed a new mathematical model to describe how the human brain visually identifies objects. The model accurately predicts human performance on certain visual-perception tasks, which suggests that it’s a good indication of what actually happens in the brain, and it could also help improve computer object-recognition systems.<br /><br />The model was designed to reflect neurological evidence that in the primate brain, object identification — deciding what an object is — and object location — deciding where it is — are handled separately. “Although what and where are processed in two separate parts of the brain, they are integrated during perception to analyze the image,” says Sharat Chikkerur, lead author on a paper appearing this week in the journal <em>Vision Research</em>, which describes the work. “The model that we have tries to explain how this information is integrated.”<br /><br />The mechanism of integration, the researchers argue, is attention. According to their model, when the brain is confronted by a scene containing a number of different objects, it can’t keep track of all of them at once. So instead it creates a rough map of the scene that simply identifies some regions as being more visually interesting than others. If it’s then called upon to determine whether the scene contains an object of a particular type, it begins by searching — turning its attention toward — the regions of greatest interest.<br /><br />Chikkerur and Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences and at the Computer Science and Artificial Intelligence Laboratory, together with graduate student Cheston Tan and former postdoc Thomas Serre, implemented the model in software, then tested its predictions against data from experiments with human subjects. The subjects were asked first to simply regard a street scene depicted on a computer screen, then to count the cars in the scene, and then to count the pedestrians, while an eye-tracking system recorded their eye movements. The software predicted with great accuracy which regions of the image the subjects would attend to during each task.<br /><br /><strong>Don’t cross the streams</strong><br /><br />The software’s analysis of an image begins with the identification of interesting features — rudimentary shapes common to a wide variety of images. It then creates a map that depicts which features are found in which parts of the image. But thereafter, shape information and location information are processed separately, as they are in the brain.<br /><br />The software creates a list of all the interesting features in the feature map, and from that, it creates another list, of all the objects that contain those features. But it doesn’t record any information about where or how frequently the features occur.<br /><br />At the same time, it creates a spatial map of the image that indicates where interesting features are to be found, but not what sorts of features they are. It does, however, interpret the “interestingness” of the features probabilistically. If a feature occurs more than once, its interestingness is spread out across all the locations at which it occurs. If another feature occurs at only one location, its interestingness is concentrated at that one location.<br /><br />Mathematically, this is a natural consequence of separating information about objects’ identity and location and interpreting the results probabilistically. But it ends up predicting another aspect of human perception, a phenomenon called “pop out.” A human subject presented with an image of, say, one square and one star will attend to both objects about equally. But a human subject presented an image of one square and a dozen stars will tend to focus on the square.<br /><br /><strong>Act locally</strong><br /><br />Like a human asked to perform a visual-perception task, the software can adjust its object and location models on the fly. If the software is asked to identify only the objects at a particular location in the image, it will cross off its list of possible objects any that don’t contain the features found at that location.<br /><br />By the same token, if it’s asked to search the image for a particular kind of object, the interestingness of features not found in that object will go to zero, and the interestingness of features found in the object will increase proportionally. This is what allows the system to predict the eye movements of humans viewing a digital image, but it’s also the aspect of the system that could aid the design of computer object-recognition systems. A typical object-recognition system, when asked to search an image for multiple types of objects, will search through the entire image looking for features characteristic of the first object, then search through the entire image looking for features characteristic of the second object, and so on. A system like Poggio and Chikkerur’s, however, could limit successive searches to just those regions of the image that are likely to have features of interest.<br /><br />John Reynolds, an associate professor in the Systems Neurobiology Laboratory at the Salk Institute for Biological Studies, finds Poggio and Chikkerur’s model intriguing because of its convergence with work that he and others have been doing on the physiology of the brain. “It holds the potential for linking underlying biology to information processing in a way that’s new and exciting,” Reynolds says. He points out, for instance, that while some neurological diseases, such as Alzheimer’s disease and schizophrenia, have physiological characteristics that can be studied independently, their relationship to the diseases’ cognitive effects is not always clear. “We’d like to be able to link those to failures of behavior and perception,” Reynolds says, “and those are naturally expressed in terms of these questions that Tommy’s raising, which is what computations are being performed.”<br /><br />Reynolds speculates that, in the future, Poggio and Chikkerur’s model could be expanded so that, in the same way that it makes predictions about human eye movement, it could predict the cognitive deficits associated with disease. “It might suggest a way of reaching down into the biology and saying, ‘Look, this is the kind of mechanism that may be failing in those people,’” Reynolds says. “That could lead to treatments.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Noelle Selin on curbing mercury]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/3q-selin-mercury-0607.html</link>
<story_id>15417</story_id>
<featured>0</featured>
<description><![CDATA[As U.N. negotiations begin this week on a global mercury treaty, an MIT atmospheric scientist explains the challenges ahead.]]></description>
<postDate>Mon, 07 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100604140135-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100604140135-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100604140135-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Noelle Selin, an assistant professor of engineering systems in MIT’s Engineering Systems Division]]></imageCaption>
</image>
<body><![CDATA[<em>The first United Nations negotiating session for a global, legally binding mercury treaty begins today in Stockholm. Continuing through Friday, this is the first of five planned negotiating sessions that will address global controls on mercury, a toxin that causes neurological damage and impairs brain development in infants and children around the world. The sessions are expected to result in a global treaty to be signed in late 2013 that will address the emissions and use of mercury in products, wastes and international trade. Noelle Selin, an assistant professor of engineering systems in MIT’s Engineering Systems Division, with a joint appointment in atmospheric chemistry in the Department of Earth, Atmospheric and Planetary Sciences, studies the interactions between science and policy in international environmental negotiations. She sat down with MIT News to discuss the first negotiating session, and what she considers to be the biggest hurdles to signing a global treaty, which is “not a given” for the U.S.</em><br /><br /><strong>Q.</strong> What do you see as the biggest challenge in the effort to reduce mercury emissions worldwide?<br /><br /><strong>A. </strong>I see two major intersecting challenges: addressing the global spread of mercury emissions from coal-fired power plants in the context of the increasing demand for energy, and dealing with local impacts of mercury contamination.<br /><br />The single largest source of anthropogenic mercury emissions is power generation, particularly from coal-fired power plants. A growing, worldwide demand for energy is increasing the use of coal, and this trend will lead to more mercury emissions if not controlled. About half of current anthropogenic emissions come from Asia, mostly from China, which is dramatically increasing its use of coal. Much of the coal used in China is also relatively high in mercury content. Recent research shows that future emissions of mercury to the atmosphere significantly depend most on how energy-based industrial development proceeds in Asia. <br /><br />Dealing simultaneously with both local issues and long-range transport of mercury will also be a critical challenge for an international agreement. Mercury emitted in elemental form travels worldwide. At the same time, some other forms of emitted mercury deposit close to emission sources. Local impact also comes from the use of mercury in processes and products. Mercury is used extensively in artisanal gold mining in developing countries. Workers and local communities are exposed to some of the highest levels of mercury contamination in the world. Mercury also continues to be used in products, such as thermometers, thermostats, fluorescent light bulbs and a wide range of electronic equipment, including computer monitors and cell phones. Disposal of these products, particularly electronic waste (e-waste) in developing countries, can expose local populations to mercury. <br /><br /><strong>Q.</strong> Even if an international treaty is passed, how will it be implemented or enforced?<br /><br /><strong>A.</strong> In general, implementation and enforcement of international environmental agreements are difficult. Some countries simply do not have the intention or political will to meet their obligations. Furthermore, many developing countries lack the financial resources and technical capacity to effectively implement international environmental regulations. For this reason, some environmental agreements include mechanisms for capacity building, as well as the provision of financial assistance. However, this is often one of the most contentious topics of negotiation, and the availability of necessary resources for implementation are often limited as many developing countries argue that industrialized countries do not provide enough support for capacity building.<br /><br />Another implementation challenge will be coordinating an international mercury treaty with other environmental agreements that already partly cover mercury and other hazardous substances. The Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and Their Disposal controls the international trade and management of hazardous waste including waste containing mercury. The Rotterdam Convention on the Prior Informed Consent Procedure for Certain Hazardous Chemicals and Pesticides in International Trade sets out provisions for the import and export of hazardous chemicals, including mercury. Coordination with these two agreements will be important in addressing the entire life cycle of mercury, including mining and production, use, emission, disposal and cleanup. <br /><br /><strong>Q.</strong> How would an international treaty affect developed countries like the U.S. that already regulate mercury emissions? How do current laws in the U.S. regarding mercury emissions and use compare to other industrialized nations?<br /><br /><strong>A.</strong> The U.S. regulates mercury emissions from municipal-waste combustion and medical-waste incineration, but does not currently regulate mercury emissions from coal-fired power plants, which are the largest domestic mercury emission source. This is an area where U.S. regulations should be strengthened; the EPA is currently developing power-plant emissions standards for mercury.<br /><br />European countries also have stronger regulations than the U.S. on mercury in many products, including a large number of common electronic goods. Sweden, for example, has banned mercury in almost all products, but there are some exceptions, including the use of mercury in compact fluorescent light bulbs. In the U.S., many efforts to phase out mercury in products are voluntary, although some states have more stringent regulations. In fact, California has largely copied European Union regulation on mercury and other hazardous substances in electronics, going beyond federal requirements. <br /><br />For the U.S., any treaty ratification requires the advice and consent of the Senate. and must be approved by two-thirds of all senators. Over the past few decades, this has been an obstacle for U.S. participation in many multilateral environmental agreements. As a result, the U.S. has not ratified several important environmental treaties, including the Basel and Rotterdam conventions. Domestic politics is likely to be a continuing challenge for U.S. implementation of environmental regulations and international cooperation on mercury, and it is not a given that the U.S. would become a party to a mercury treaty.<br /><br />]]></body>
</item>
<item>
<title><![CDATA['You can’t play it safe and win’]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/recap-commence.html</link>
<story_id>15407</story_id>
<featured>0</featured>
<description><![CDATA[Speaker Ray Stata urges graduates to find solutions to humanity’s greatest problems — and to risk failure along the way — at MIT Commencement.]]></description>
<postDate>Fri, 04 Jun 2010 18:30:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100604141001-1.png</thumbURL>
<smallURL width='140' height='126'>http://web.mit.edu/newsoffice/images/article_images/w140/20100604141001-1.jpg</smallURL>
<fullURL width='368' height='331'>http://web.mit.edu/newsoffice/images/article_images/20100604141001-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Conor Walsh, who received a PhD in mechanical engineering, gets ready for the Commencement exercises in Killian Court on Friday, June 4.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100604141349-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[A graduate leaves the stage after receiving his diploma.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Don’t be afraid to take chances and even risk failure, Ray Stata urged MIT’s Class of 2010 at the Institute’s 144th Commencement Exercises on Friday, June 4. “People don’t learn by doing things right,” said Stata ’57, SM ’58, this year’s Commencement speaker. “They learn by making mistakes and then reflecting on what went wrong.”<br /><br />Under sunny skies in the spacious but jam-packed Killian Court, Stata, MIT President Susan Hockfield, and two student leaders all reminded the newly minted MIT degree-holders of the privileged education they have received, and of the importance of using their skills to solve global problems and to make the world a better place. But Stata challenged them to keep in mind that the sometimes-serendipitous path to success is not a straightforward one.<br /><br />Stata, the chairman and co-founder of Analog Devices Inc. and a life member emeritus of the MIT Corporation, described himself to the graduating class as a “<a href="http://web.mit.edu/newsoffice/2010/stata-remarks-commence.html" target="_self">fellow nerd</a>” and said that “as MIT graduates, we are all innovators and entrepreneurs at heart.” Drawing on decades of experience, Stata said that “entrepreneurs have to be optimistic and relentless in believing that they will discover or create the missing pieces, even when they are not sure how … You can’t play it safe and win.”<br /><br />Hao Ding, who received his bachelor of science in mechanical engineering and was also a commencement singer, found Stata’s speech inspiring: “His experience really relates to ours. He left this place with a very specific goal in life to succeed. He is a great example of how MIT students can go out and make a difference. His perspective of what an MIT degree means is different from anyone else’s in the outside world, and his speech gives us an idea of what that perspective will mean in the very near future.”<br /><br /><strong>‘Dare to be part of the solution’</strong><br /><br />Stata, whose 1997 gift to the Institute enabled the construction of the Frank Gehry-designed Ray and Maria Stata Center, reminisced about the origins of Analog Devices in 1965, which he co-founded with fellow MIT alumnus Matthew Lorber ’56, SM ’58. “You sometimes don’t know where the path you take will lead,” he told the graduates, describing how a chance meeting between him and Lorber in Harvard Square, years after their graduation, ultimately led to the creation of the company.<br /><br />Stata said that his advice applies not just to those who choose to start their own businesses. “The spirit of innovation and entrepreneurship applies not just to business, engineering and science, but to every aspect of work and life,” he said. “You are all well equipped to challenge the status quo and to bring about dramatic changes and improvements in whatever you choose to do.”<br /><br />Stata said he saw cause for optimism despite the many difficult problems facing the world: “When I reflect on the ingenuity of the human race and on the truly amazing things which have been accomplished just in my lifetime, I’m optimistic that your generation will not only find solutions to today’s challenges but you will also discover new opportunities for progress in a more integrated and inclusive world.<br /><br />“You have a special responsibility to help create a future where every person on the planet can have the hope and the prospects for a better life,” he said. “Dare to be part of the solution.”<br /><br />Stata’s message resonated with Nancy Day, whose daughter, Julia, received her SB in mechanical engineering. “I thought that Ray Stata was phenomenal,” said Day, who traveled from her home in Dallas to watch her daughter graduate. “He is a real-time, honest example of what they can achieve. I also thought it was particularly special the amount of gratitude that was expressed [during the ceremony]. The mention of the students’ families was important. The students seemed humble, proud and ready to give.”<br /><br /><strong>Aiming for more than mere sustainability</strong><br /><br />President Hockfield, in her <a href="http://web.mit.edu/newsoffice/2010/hockfield-charge-commence.html" target="_blank">charge to the graduating class</a>, commended the students’ altruism. That, coupled with the power of an MIT education, can create a powerful force for positive change. And such change, she said, is needed now more than ever. <br /><br />“Today,” she said, “we all look out at a world riddled with manifestly unsustainable systems, from the environment to the global economy; from healthcare to transportation; from water, to cities, to energy — ailing systems whose remedies will call on the core strengths of MIT.”<br /><br />Pointing out that the world’s increasing awareness of the need for sustainability serves as a kind of guardrail against a precipice, she said “with the particular strengths of your generation, with the ingenuity and practicality you learned at MIT, and with an appreciation of the distant ramifications of present action, I believe you have the power to set us a more ambitious goal, to move from ‘sustainability’ to a far-reaching kind of healing.”<br /><br />During this academic year, MIT awarded 1,116 bachelor’s degrees (including those awarded in September and February), 1,580 master’s degrees, 17 engineer’s degrees and 583 doctoral degrees. At the June 4 Commencement ceremony, 912 undergraduate students and 1443 graduate students received their diplomas.<br /><br />Alex Hamilton Chan, president of the Graduate Student Council, in a rousing voice congratulated what he called “the world’s best graduating class,” and urged them to add a third element to MIT’s traditional motto of mind and hands (mens et manus), stressing the importance of heart as well. “MIT has empowered you to be a force for good,” he said. “You are the world’s hope … Make your life one long gift to humanity.”<br /><br />Those sentiments were echoed by Jason Scott, president of the Class of 2010, who encouraged his fellow graduates to “be empathetic and lead through example.” Scott presented the class gift to Hockfield, who congratulated the class for having achieved a “near-miraculous” 73-percent participation rate in the gift fund — a record. The gift of $32,000 will be used to provide housing for students through the summer, to enable them to do volunteer work or pursue unpaid internships.<br /><br />Jacinda Shelly, who received both her SB and SM in electrical engineering and computer science, said that “walking up to the stage was the most exciting thing that has happened to me in the past five years.” And Meetu Kapur, a graduate of the Sloan Fellows Program, described the day as “everything I’ve looked forward to” and “absolutely like a dream.”<br /><br /><em>Additional reporting by Morgan Bettex</em><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Ray Stata's Commencement address]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/stata-remarks-commence.html</link>
<story_id>15404</story_id>
<featured>0</featured>
<description><![CDATA['The spirit of innovation and entrepreneurship applies not just to business, engineering and science, but to every aspect of work and life']]></description>
<postDate>Fri, 04 Jun 2010 14:00:03 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100604142700-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100604142700-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100604142700-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Raymond S. Stata ’57 delivers the address at the 144th Commencement on Friday, June 4.]]></imageCaption>
</image>
<body><![CDATA[<em>Below is the prepared text of the Commencement address by Raymond S. Stata ’57, chairman and cofounder of Analog Devices Inc., for MIT's <a href="http://web.mit.edu/newsoffice/2010/recap-commence.html">144th Commencement</a> held June 4, 2010.</em><br /><br />Good morning and congratulations. You should be brimming with pride this morning. You earned a degree from the world’s most prestigious university focused on science and engineering. You mastered MIT’s unique brand of learning and doing which will distinguish you throughout your career. Congratulations as well to your family and friends who supported you and encouraged you along the way.<br /><br />Your satisfaction and happiness in life depend on the choices you make and on the principles and values which guide them. So far your choices have served you extremely well. As a fellow nerd, I am honored to share with you some of my experiences and some of the principles and values that have worked well for me.<br /><br />As MIT graduates we are all innovators and entrepreneurs at heart. We search for opportunities to do things better, to make things happen and to change the world. <br /><br />In my case, I also had a strong desire to be in control of my destiny. So to satisfy this need I aspired to someday start my own company. With this end in mind, shortly after graduation I went to work for Hewlett Packard. Hewlett Packard was for me like a mini MBA where I learned the basics of business. But more importantly I learned from Hewlett Packard that commitment to the welfare of employees and to the development of their full potential are the cornerstones on which successful businesses are built. <br /><br />You sometimes don’t know where the path you take will lead. When I was about 27, one day in Harvard Square, I bumped into Matthew Lorber, an acquaintance from my student days. Matt was looking for a roomate; we ended up not only sharing an apartment, but also together three years later we founded Analog Devices. Of course, we had no idea that Analog Devices would become a multi-billion dollar company and a leader in an important segment of the semiconductor industry. We started by carving out a small niche and then step by step we extended our core competencies and built on our success. <br /><br />Analog’s business strategy focused on opportunities where we could achieve and sustain leadership. Our mantra was “Market Leadership through Technical Innovation.” Being number one not only produces superior business results, it also engenders a sense of pride and satisfaction and stimulates your competitive instincts to stay ahead. Don’t ever give up your ambition to be the best at whatever you do. It’s much more rewarding — and it’s also more fun.<br /><br />We quickly discovered that you can’t be an innovative company unless you have great innovators. So at Analog, rather than “sidetracking” our best engineers into management, we created a parallel ladder to encourage them to continue their technical careers. We not only provided compensation that was comparable to the management track, but we also gave engineers a voice in influencing business strategy, investment decisions, organization policies; I encouraged managers to treat engineers as full business partners. <br /><br />The highest rung on our technical ladder was reserved for our most distinguished engineers, who we called Analog Devices Fellows. Fellows at Analog are important people whose views are respected and valued. Don’t let organization hierarchy obscure the people who are most important to your success.<br /><br />Another important way to unlock innovation is to free people to do their best work. In building the organization I found that a lot of talented people were just like me. They wanted the freedom to decide what to do and how to do it. So I shaped a culture which gave employees broad latitude to make decisions. We didn’t have a lot of rules and controls. We depended more on developing people’s judgment. We aligned the goals of the company and the goals of employees, and we encouraged employees to think about the company’s success as a prerequisite to their own. Empowering people to make decisions and to take ownership is a powerful motivator. This was a key factor in our success.<br /><br /> In effect, we built our market position on a culture of innovation. But then the question becomes — how do you maintain your lead? Andy Grove, the former CEO of Intel, wrote a book in which he proclaimed, “only the paranoid survive.” Beware of S-Curve’s because everything has life cycles — technologies, products, markets and companies. To sustain innovation, you can’t abandon your appetite for risk.<br /><br />When we started Analog Devices, linear integrated circuits or so-called IC’s had not yet been invented. We manufactured operational amplifiers and converters by manually assembling discreet transistors and other components on printed circuit boards. <br /><br />Just two years after we opened our doors, the industry’s first integrated circuit operational amplifiers were introduced. Our hand-tweaked amplifiers consistently outperformed these new devices — but each year they got better, and they were an order of magnitude cheaper. I concluded that — to avoid becoming victims of our own S-curve — we had to take bold steps to learn how to design and manufacture IC’s, or our success would be short lived. <br /> <br />Everyone in the company disagreed. They said that integrated circuits would never meet our customer’s performance requirements, that we shouldn’t risk a rapidly growing, profitable business, that we didn’t have the financial resources to compete with the large semiconductor companies and besides no one in the company knew anything about semiconductor technology. So the board said no. It was too risky. <br /><br />For me the risk of inaction was even greater, so I made an offer the board couldn’t refuse. I offered to personally fund a startup company to design and manufacture IC’s which Analog Devices would sell. If the venture succeeded, ADI would have an option to acquire the company with no gain for my investment. <br /><br />If it failed I would personally suffer the loss. People thought I was foolish, but it worked. Three years later, Analog Devices acquired the startup and got serious about semiconductor technology.<br /><br />We figured out ways to manufacture IC’s that achieved performance even better than our hand assembled products, at a fraction of the cost. This breakthrough innovation established ADI as the industry leader in high performance linear integrated circuits, a position we still hold today.<br /><br /> The point is, entrepreneurs have to be optimistic and relentless in believing they will discover or create the missing pieces even when they are not sure how. Courage and the capacity to take risk are fundamental to entrepreneurship. If you are not stepping outside your comfort zone to take calculated risks, chances are you will not be exploiting your full potential. You can’t play it safe and win. <br /><br />Don’t be afraid to fail or make mistakes — odds are you will many times. Failure is not always bad. In fact people don’t learn by doing things right. They learn by making mistakes and then reflecting on what went wrong. <br /><br />My wife Maria says that I am frequently wrong but never in doubt. Well, I’m not too sure about the frequently part but I have surely made my share of mistakes. Perhaps my biggest failure was Analog Devices Enterprises, a venture investment fund I setup within the company. <br /><br />Nothing came of it, and we lost a lot of money. My mistake was to allow our investments to stray too far from Analog’s core competencies. In the delicate balance between focus and diversification, I stepped over the line — but we certainly learned from it.<br /><br />But if success for an entrepreneur depends on cultivating perpetual innovation, it also depends on how you work with others, how much you demand of yourself — and how much you believe in what you’re doing.<br /><br />For most of you, your accomplishments will be magnified by productive collaborations, and by your ability to lead teams and organizations. To those ends, I’ve found you can be most effective by building trustful relationships. You earn trust through honesty, adhering to the facts; integrity living by principles; sincerity, meaning what you say; reliability, meeting your commitments; and competence, knowing what you are doing.<br /><br />As high achievers in the real world you will soon discover as I did that you can’t substitute working smarter for working harder. To excel at the top of your game you have to do both. So be prepared to continue to drink from the fire hose as a way of life.<br /><br />The most important advice I can give you is this: if ever you find that you are not passionate and excited about what you are doing, then start searching for an opportunity where you will be. Nothing is more important and gratifying than getting up in the morning with an eagerness to get to work and accomplish something important. Set high standards for what you expect from your work, be courageous in stepping into the unknown and think big about what you can accomplish.<br /><br />If history is any guide, as MIT graduates 40 percent of you will have played an important role in an early stage startup venture by the time you reach age forty-five. A recent Sloan School study reported that cumulatively, 122,000 living MIT graduates have founded more than 25,000 companies which are still in business today and which collectively generate $2 trillion in revenues and 3.3 million jobs. And this does not include the companies that were acquired. <br /><br />And the fact is that the spirit of innovation and entrepreneurship applies not just to business, engineering and science, but to every aspect of work and life. You are all well equipped to challenge the status quo and to bring about dramatic changes and improvements in whatever you choose to do. And you will also find opportunities as volunteers to apply your knowledge and skills to solve important societal problems. <br /><br />From my involvement in K through 12 education, I’ve found that public institutions are the ones most in need of intervention from iconoclasts like us to engineer desperately needed change and improvements.<br /> <br />MIT has played a very important role in my life. As a “hay-seed” from a small farm community, I was the first in my family to attend college. Without MIT’s need blind admissions I would not have had access to the Institute’s unique education experience. As soon as we could afford it, Maria and I made a gift to not only repay MIT for the significant investment it made in me, but also to repay a debt of gratitude for what MIT had enabled me to do in my career.<br /><br />Beyond what I gained personally, as I became more involved I came to better understand MIT’s enormous impact on the world — an impact that I’m convinced will become even more important in this complicated century. With its unmatched depth and breadth of research, and its culture of interdisciplinary collaboration, MIT is uniquely positioned to tackle the world’s biggest problems — energy, climate change, poverty and the diagnosis and care of disease. Maria and I believe so strongly in that mission — and in the people of MIT — that we have actively looked for ways to help MIT do what it does best.<br /><br />For example, as a member of the Electrical Engineering and Computer Science Visiting Committee, I learned there was an urgent need to better integrate these disciplines by relocating the Computer Science side of the department from across the tracks in Tech Square to campus. So Maria and I offered to make a lead gift to catalyze this important decision. Initially the new facility was conceived as just another box on Vassar Street. But once started, the imagination of the faculty and Institute leaders was unleashed and the project took on a bolder vision than just a new home for computer science. <br /> <br />It quickly became an inspiring space for students and faculty from across the campus to play, hang out, eat, exercise and even dance — a space to facilitate human interaction and to germinate new ideas. The Center stands today as a constant reminder that MIT is about breaking with tradition and exploring new frontiers.<br /><br />There are many, many examples, large and small, where MIT graduates got involved, saw a need and became part of the solution.<br /><br />I urge you to stay involved with MIT, to search for ways where you can make a difference and when your time comes, to give back. Like its peer universities, MIT very much depends on the work, wisdom and wealth of its graduates to sustain its greatness. Equally important, as the products of MIT’s education, it is through your accomplishments that MIT’s contributions to society are amplified.<br /><br />You are entering a troubled world that is both in crisis and in transition; In crisis due to the lapse of judgment and moral values of business and political leaders which triggered a deep and far reaching financial meltdown; in transition due to China, Russia and India ending decades of isolation and releasing 3 billion people into the free market economy. Coping with the stresses on financial and natural resources from an aging and growing global population with increasing expectations for a better life, presents seemingly insurmountable challenges. But when I reflect on the ingenuity of the human race and on the truly amazing things which have been accomplished just in my lifetime, I’m optimistic that your generation will not only find solutions to today’s challenges but you will also discover new opportunities for progress in a more integrated and inclusive world.<br /><br />Remember, as MIT graduates you are the best the world has to offer. With technology playing an ever-increasing role, you have a special responsibility to help create a future where every person on the planet can have the hope and the prospects for a better life. My charge to you this morning is don’t play it safe — because the risk of inaction is too great. Dare to be part of the solution.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: MIT’s 144th Commencement exercises]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/slideshow-commence.html</link>
<story_id>15406</story_id>
<featured>0</featured>
<description><![CDATA[Participants enjoyed warm, sunny weather — and words of wisdom.]]></description>
<postDate>Fri, 04 Jun 2010 14:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100604140636-24.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100604140636-24.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604140636-24.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Diplomas sit ready to be given out to the Class of 2010.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604120351-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[The ceremonial mace, which is carried into the Commencement exercises by the MIT Alumni Association president.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100604124019-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[The diplomas are delivered to Killian Court prior to the ceremony.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604122001-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Raymond S. Stata ’57, the Commencement speaker, prepares for the exercises.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604122001-4.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[A graduating student reads a copy of The Tech, the MIT student newspaper, while waiting to line up prior to the Commencement exercises.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604122220-5.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Institute chaplain Robert Randolph dons his regalia prior to the Commencement ceremonies.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604122221-6.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604122221-7.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[The Commencement procession begins makes its way toward Killian Court along Massachusetts Avenue.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604122439-8.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[MIT President Susan Hockfield and other academic and Institute leaders meet prior to the ceremony.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='264'>http://web.mit.edu/newsoffice/images/article_images/20100604122439-9.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Soon to be graduates have their picture taken in front of MIT's Building 7.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604123055-10.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Family, friends and more await their graduates in Killian Court.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604123056-11.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604123056-12.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604123503-13.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='433'>http://web.mit.edu/newsoffice/images/article_images/20100604123504-14.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604123504-15.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604125919-16.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[MIT Alumni Association President Kenneth Wang, president of U.S. Summit Company, leads the Institute delegation into the Commencement ceremony.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604125919-17.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Beginning at their 50th reunion, MIT alumni don a "Red Coat" and those celebrating this milestone are honored each year at commencement. Here, a member of the Class of 1960 watches the Commencement proceedings.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604125920-18.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100604130025-18.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='272'>http://web.mit.edu/newsoffice/images/article_images/20100604133312-20.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Commencement speaker Raymond S. Stata ’57 listens during the ceremony.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604133312-21.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[MIT President Susan Hockfield presents a diploma to one of the graduates.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='256'>http://web.mit.edu/newsoffice/images/article_images/20100604133711-22.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Two graduates walk off stage after receiving their diplomas.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604140636-23.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[The procession makes its way through the graduate seating area toward the main stage in Killian Court.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604150020-25.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604150020-26.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604150021-27.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604162631-28.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100604162632-29.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
</otherImages>
<body><![CDATA[<br /><br />Warm, sunny weather greeted members of the Class of 2010, family, friends and other guests who participated in MIT's <a href="http://web.mit.edu/newsoffice/2010/recap-commence.html">144th Commencement exercises</a>, held on Friday, June 4 in Killian Court.<br /> <br /><a href="http://web.mit.edu/newsoffice/2010/stata-remarks-commence.html">In his Commencement address</a>, Raymond S. Stata ’57 told students that as MIT graduates, they were “the best the world has to offer” — and he challenged them to build a better world.<br /><br />“My charge to you this morning is don’t play it safe — because the risk of inaction is too great,” Stata, chairman and cofounder of Analog Devices Inc., told the more than 2,350 newly minted MIT degree holders in the audience. “Dare to be part of the solution.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[President Hockfield's charge to the graduates]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/hockfield-charge-commence.html</link>
<story_id>15405</story_id>
<featured>0</featured>
<description><![CDATA[She urges students to not simply advance the cause of 'sustainability,' but to push even further and do 'a great deal of good']]></description>
<postDate>Fri, 04 Jun 2010 14:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100604142645-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100604142645-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100604142645-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[MIT President Susan Hockfield delivers her charge to the graduates at the Institute's 144th Commencement on Friday, June 4.]]></imageCaption>
</image>
<body><![CDATA[<em>Below is the prepared text of the charge to the graduates by MIT President Susan Hockfield for the Institute's  <a href="http://web.mit.edu/newsoffice/2010/recap-commence.html">144th Commencement</a> held June 4, 2010.</em><br /><br />Today’s graduates of MIT: This day is, truly, for you. Here, in the stately embrace of Killian Court, we gather to celebrate your success. You have distinguished yourselves in courses of study that stand among the most demanding in the world. For all that you have accomplished, we congratulate you. <br /><br />In the midst of celebrating your achievements, our joy would be incomplete if we did not recognize two groups of people who helped bring you to this moment: First, your families and friends, many of whom join us today, with justifiable pride and with joy. We welcome them, knowing full well that none of you would be here, clothed in solemn academic regalia, without the constant confidence of family members and friends who embraced your dreams and lighted your paths. This is their day, too. Graduates, I invite you to rise, and to join me in thanking your families and friends. The second group to thank includes your many teachers and mentors here at MIT. Our remarkable faculty have devoted their lives to exploring and explaining the unknown. And they welcomed you to join them in the race to the frontiers of human understanding. Let’s take a moment to thank the women and men who shared their discoveries, ignited your enthusiasm with their own, and taught you the infinitely useful discipline of mind and hand.<br /><br />Speaking for the faculty, one of the great pleasures of MIT’s academic community is that we are all teachers, and we are all students, all the time. And so today, though it is technically my job to offer a charge to you, our graduates, I want to start by explaining how much my generation can learn from yours. I will skip past the things that we will probably never learn, like the proper way to “unfriend” someone or how to talk about using Twitter, with a straight face, and move on to a few qualities that seem to shine out in everything you do, and that the world needs now more than ever.<br /><br />My generation endured, and sometimes incited, struggles that threatened to tear this nation apart. Those struggles, while accelerating change in many dimensions, often produced more noise than effect, and they left cracks in some of the pillars of community, especially in the idea of responsibility to the larger community beyond the self. When Bill Gates came to campus this spring, he encouraged you, as he said, to “make sure that our brightest minds are working on [humanity’s] most important problems.” But with so many of you already devoting your creativity, time and passion to tackling the world’s most pressing challenges, he spoke here not merely to an audience inclined to follow his advice, but to those already leading the way.<br /><br />Yet your generation wears its commitment to the greater good quite lightly. You use your skills to help repair a broken world, however, you see nothing remarkable about it; you simply expect it of each other, and of yourselves. Over the past decade, the number of students who volunteer through MIT’s Public Service Center has grown somewhat, but the real difference lies in the depth and ambition of their engagement, which has blossomed from interest in volunteering in the neighborhood now and again, to a deep culture of service that has inspired members of the Class of 2010 to launch a free summer camp for the children of local cancer patients, to bring battery-enhanced electricity to remote villages in Tanzania, and to design wheelchairs for people in developing nations around the world. At the same time, MBA students in MIT Sloan’s wildly popular Global Lab program, or G-lab, have used their newfound business skills to magnify the power of fledgling enterprises, like developing a scalable business model for food carts that deliver nutritious meals to children in Indonesia’s poorest neighborhoods. And you have also put your shoulders to the wheel that accelerates economic growth, by launching the kind of innovation-based businesses that drive our nation’s economy and the world’s. This year’s MIT Sloan graduates alone are rolling out 35 start-up companies, as we speak, seven of them built on new technologies invented at MIT.<br /><br />You have surely inherited one thing from my generation: a copious stockpile of jargon. Jargon that attempts to capture and control some of the untamed conditions of our world by affixing a name to them: <em>Globalization. Diversity. Work-Life Balance.</em> You, quite properly, treat our jargon as quaintly obsolete: You swap the conflicted notion of globalization for the bright conviction that you can work anywhere and you should. You don’t fret about diversity, you simply choose the people with whom you live and work based on interests and talents that transcend yesterday’s 20th century boundaries. And why would you let your life and your work get out of balance anyway? Just launch a company that values both as much as you do. <br /><br />You have also transformed one jargon-heavy platitude into the great challenge of your generation. Today, we all look out on a world riddled with manifestly unsustainable systems, from the environment to the global economy; from healthcare to transportation; from water, to cities, to energy — ailing systems whose remedies will call on the core strengths of MIT. It is that call on MIT’s intellectual resources that brought President Obama to our campus in October, to highlight the critical need to develop clean energy technologies, at great speed, and on a prodigious scale. He urged you to defy the easy complacency of pessimism, reminded you that we are “heirs to a legacy of innovation,” and challenged you to help invent our clean energy future.<br /><br />I am extremely proud that you are answering that call and expanding its challenge by insisting on and inventing ambitiously sustainable systems: <br /><br />Some of you are inventing sustainable practices through engineering and entrepreneurship: The students who won this year’s $100K Competition proposed a start-up that will bring the world a nanoengineered cement, stronger than any existing version and promising to cut the torrent of CO2 generated during standard concrete production in half. Some of you are pursuing sustainability by rethinking the systems that society depends on, like the Aeronautics and Astronautics students who worked with Professor Mark Drela and others to envision a new plane that consumes 70% less fuel, or the doctoral candidate who analyzed how to balance the rising demand for air travel with improving air quality and climate impacts, and helped shape new international rules governing commercial aviation.<br /><br />Some of you create sustainable solutions by applying new technology to old problems, like providing basic, affordable healthcare for everyone. In this year’s IDEAS competition, one winning student team invented PerfectSight, an extremely affordable system for diagnosing nearsightedness and farsightedness – using a cell phone. And some of you are pursuing sustainability through policy change: Last week in Congress, Senator Jeff Bingaman introduced an important new energy bill that aims to deliver dramatic gains in energy efficiency from our complex energy supply chains, a supremely MIT idea that he first learned about through our graduate student-led Energy Club.<br /><br />Whatever field you choose, I hope and I fully expect that you will advance the cause of “sustainability.” But I anticipate that you will push us even further, beyond the cliché, because simple sustainability is not enough; it is necessary but not sufficient. By itself, sustainability resembles the medical principle, “First, do no harm,” a guardrail to protect us from a precipice. But with the particular strengths of your generation, the ingenuity and practicality you learned at MIT, and an appreciation of the distant ramifications of present action, I believe you have the power to set us a more ambitious goal, to move from “sustainability” to a far-reaching kind of healing, from “doing no harm” to doing a great deal of good.<br /><br />Graduates of MIT: Today is your day, and now is your moment to take all you have learned at MIT — the power of analysis; the capacity for good old-fashioned hard work; the fearless creativity; and the commitment to restoring an unsustainable world — and put them to work around the globe. In person and on-line, through the Alumni Association and through your friends, I encourage you to stay connected to MIT for the rest of your lives. For all that you have created, discovered, invented, explored and mastered at MIT — Congratulations MIT graduates of 2010.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Corporation elects new members, chair]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/new-corporation-members.html</link>
<story_id>15409</story_id>
<featured>0</featured>
<description><![CDATA[John Reed is formally elected chair; 8 term members and 3 life members are also appointed]]></description>
<postDate>Fri, 04 Jun 2010 13:30:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100603163654-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100603163654-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100603163654-1.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100603163654-2.jpg</fullURL>
</image>
</otherImages>
<body><![CDATA[The MIT Corporation — the Institute's board of trustees — elected eight term members and three life members at its quarterly meeting on Friday, June 4, prior to the Commencement exercises. All memberships are effective July 1.<br /><br />The election results were announced by Corporation Chair Dana G. Mead PhD ’67, who chaired his final meeting prior to stepping down. He is succeeded by John Reed ’61, SM ’65, who was formally elected to the role of chair at the meeting. Mead was also elected as a life member on Friday.<br /><br /> Life members serve without a specific term until they turn 75 years old, and term members serve five-year terms; both have voting rights in the Corporation. Alumni/ae nominees and representatives of recent graduating classes also serve five-year terms.<br /><br />At age 75, life members become life members emeritus; while they no longer have a vote, they continue to play an active role in Institute affairs.<br /><br />Along with Mead, the other two new life members are James H. Simons ’58 and John A. Thain ’77. Simons is president of Euclidean Capital and board chair of Renaissance Technologies LLC, a highly quantitative investment firm, from which he retired in 2009. Thain is chairman and CEO of CIT Group Inc.<br /><br />It was also announced at the meeting that K. Anne Street ’69, SM ’72 has been named the 2010-2011 president of the Association of Alumni and Alumnae of MIT. As such, she becomes an ex officio member of the Corporation. She succeeds Kenneth Wang ’71, who returns to the Corporation for a five-year term that will conclude in 2015.<br /><br />As of July 1, the Corporation will consist of 76 distinguished leaders in education, science, engineering and industry; of those, 25 are life members and 8 are ex officio. An additional 33 individuals are life members emeritus.<br /><br /> The elected term members are:<br /> 
<table border="0" cellpadding="10">
<tbody style="vertical-align:top">
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/bodman.jpg" border="0" width="90" height="120" /></td>
<td><strong>Samuel W. Bodman III</strong><em><br /> Former United States Secretary of   Energy</em><br /> Term: Five years<br /> Education: SB, Cornell University   (1961); ScD, MIT (1965)<strong> </strong></td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/ho.jpg" border="0" width="90" height="120" /></td>
<td><strong>David D. Ho</strong><br /><em>Founding Scientific Director and Chief  Executive Officer Aaron Diamond AIDS Research Center</em><br />Term: Five  years (second five-year term)<br />Education: BS, California Institute of  Technology (1974); MD, Harvard Medical School (1978)<br /> Current MIT  activities: Corporation Visiting Committees (Biology, Whitaker College)</td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/johnson.jpg" border="0" width="90" height="120" /></td>
<td><strong>Sarah Stewart Johnson</strong><br /><em>White House Fellow</em><br />Term: Five  years (recent classes nominee)<br />Education: BA, Washington University  (2001); BA, MSc, University of Oxford (2003, 2005); PhD, MIT (2008)</td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/jones.jpg" border="0" width="90" height="120" /></td>
<td><strong>Anita K. Jones</strong><br /><em>University Professor Emerita, University of  Virginia</em><br />Term: Five years (second five-year term) <br />Education:  BA, Rice University (1964); MA, University of Texas at Austin (1968);  PhD, Carnegie Mellon University (1973)<br />Current MIT activities:  Corporation Executive Committee; Corporation Visiting Committees  (Engineering Systems Division, Sponsored Research); Advisory Board  Member, MIT Lincoln Laboratory</td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/tang.jpg" border="0" width="90" height="120" /></td>
<td><strong>Martin Y. Tang</strong><br /><em>Director, MTDD, Ltd.</em><br />Term: Five years  (Alumni Association nominee; second five-year term)<br />Education: SB,  Cornell University (1970); SM, MIT (1972)<br />Current MIT activities:  Corporation Visiting Committees (Dean for Undergraduate Education, Sloan  School of Management)</td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/van-lee.jpg" border="0" width="90" height="120" /></td>
<td><strong>Reginald Van Lee</strong><br /><em>Executive Vice President, Booz Allen  Hamilton, Inc.</em><br />Term: Five years (Alumni Association nominee)<br />Education:  SB, MIT (1979); SM, MIT (1980); MBA, Harvard Business School (1984)<br />Current  MIT activities: Chair, Black Alumni of MIT; member, MIT Council for the  Arts</td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/wang.jpg" border="0" width="90" height="120" /></td>
<td><strong>Kenneth Wang</strong><br /><em>President, U.S. Summit Company</em><br />Term:  Five years (Alumni Association nominee; second five-year term)<br />Education:  SB, MIT (1971); MBA, Harvard Business School (1976).<br />Current MIT  activities: Corporation Visiting Committees (Architecture, Dean for  Student Life, Humanities); recent Alumni Association president <br /></td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/white.jpg" border="0" width="90" height="120" /></td>
<td><strong>Chiquita V. White</strong><br /><em>Associate Director, Research and Product  Development, Global Personal Power, Procter &amp; Gamble</em><br />Term:  Five years (second five-year term)<br />Education: SB, MIT (1985); SM,  University of Pennsylvania (1987)<br />Current MIT activities: Life Member  and the immediate past president of the Black Alumni of MIT;  Corporation Executive Committee; Corporation Visiting Committees  (Chemical Engineering; Earth, Atmospheric and Planetary Sciences) <br /></td>
</tr>
<tr>
<td><img src="http://web.mit.edu/newsoffice/images/stories/street.jpg" border="0" width="90" height="120" /></td>
<td><strong>K. Anne Street</strong><br /><em>President and CEO, Riverside Consulting  Group, Inc.</em><br />Term: Ex officio for one year, as president of the  Association of Alumni and Alumnae of MIT<br />Education: SB, MIT (1969);  SM, MIT (1972)<br />Current MIT activities: Corporation Visiting  Committees (Materials Science and Engineering, Nuclear Science and  Engineering, Corporation Development Committee); member, MIT Council for  the Arts <br /></td>
</tr>
</tbody>
</table>
<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: Doctoral hooding]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/slideshow-hooding.html</link>
<story_id>15408</story_id>
<featured>0</featured>
<description><![CDATA[PhD, ScD recipients honored at annual ceremony]]></description>
<postDate>Thu, 03 Jun 2010 16:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100603135755-15.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100603135755-15.jpg</smallURL>
<fullURL width='368' height='246'>http://web.mit.edu/newsoffice/images/article_images/20100603135755-15.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603133130-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603133131-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603133131-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603133156-4.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='246'>http://web.mit.edu/newsoffice/images/article_images/20100603133156-5.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603133157-6.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603133317-7.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603134054-8.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603134054-9.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='258'>http://web.mit.edu/newsoffice/images/article_images/20100603134617-10.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603135029-11.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603135029-12.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603135240-13.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100603135240-14.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
<image>
<fullURL width='368' height='254'>http://web.mit.edu/newsoffice/images/article_images/20100603135755-16.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
</image>
</otherImages>
<body><![CDATA[<br /><br />At MIT, advanced degree candidates receive their doctoral hoods in a separate ceremony held the day before <a href="http://web.mit.edu/commencement/2010/" target="_blank">Commencement</a>.<br /><br />This year's ceremony for newly minted PhDs and ScDs took place from 11 a.m. to 1 p.m. on June 3 in Rockwell Cage. This slideshow — <a href="http://amps-web.amps.ms.mit.edu/public/comm2010/ondemand/hooding/" target="_blank">and an archived webcast</a> — highlight the hooding ceremony.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Operating in orbit]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2009/satcher-event-0603.html</link>
<story_id>15402</story_id>
<featured>0</featured>
<description><![CDATA[Astronaut and alumnus Bobby Satcher recounts his experience as the first orthopedic surgeon in space]]></description>
<postDate>Thu, 03 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100602153616-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100602153616-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100602153616-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: NASA]]></imageCredits>
<imageCaption><![CDATA[Astronaut and MIT alumnus Robert L. Satcher Jr. '86, PhD '93, STS-129 mission specialist, participates in a spacewalk. ]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100603090747-1.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of David Barron]]></imageCredits>
<imageCaption><![CDATA[Robert L. Satcher Jr.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[As an orthopedic oncologist who studies bone that has been damaged by cancer, Robert “Bobby” Satcher ’86, PhD ’93, HST MD ’94 is also interested in the effects of microgravity on the human body. He got the chance to experience those effects firsthand when he became the first orthopedic surgeon to venture into space in November 2009.<br /><br />Satcher was one of six astronauts who spent 11 days aboard the recently retired shuttle Atlantis as part of NASA’s STS-129 Space Shuttle mission to deliver 14 tons of spare parts to the International Space Station (ISS). During the mission, he completed two spacewalks to attach hardware to the exterior of the ISS that will help keep the research facility running until 2015. <br /><br />“It’s definitely the most physically demanding thing we do as astronauts,” Satcher, speaking at a talk at MIT on Tuesday hosted by the Harvard-MIT Division of Health Sciences and Technology (HST), said about his spacewalk experience. “It’s really kind of a sensory overload.”<br /><br />If anyone could handle the physical and mental stress of that task, it would be Satcher, according to Joseph R. Madsen, an associate professor of neurosurgery at Harvard Medical School and president of the HST Alumni Association. “Dr. Satcher demonstrates the core idea of the HST program — combining a deep understanding of human biology with a deep understanding of engineering and science to accomplish things that can hardly be imagined,” he said.<br /><strong><br />Space surgery</strong><br /><br />Satcher has been on leave as an assistant professor of orthopedic surgery at the Northwestern University Feinberg School of Medicine since 2004, when he was selected to be an astronaut. Prior to the launch of STS-129, Satcher underwent 18 months of rigorous mission training that included land- and water-survival classes and learning about the technical aspects of the space shuttle and orbital mechanics.<br /><br />Despite that intensive astronaut training, it was actually Satcher’s medical background that proved to be the most valuable during the mission. Satcher said that his surgical experience helped prepare him for using highly sophisticated instruments during the two spacewalks he completed that totaled more than 12 hours. During the first spacewalk, he helped attach a spare antenna to the ISS and perform maintenance work on a robotic arm, and during his second spacewalk, he helped install a new high-pressure oxygen tank.<br /><br />Satcher, who holds a doctorate in chemical engineering from MIT, said he was most intrigued with the scientific aspects of the mission, including conducting short-term experiments and delivering equipment for ongoing experiments on the ISS. <br /><br />He was particularly interested in the orthopedic experiments, such as measuring how the astronauts’ heights increased in space as a result of the absence of gravity, or microgravity, and delivering mice that over-express a gene related to osteogenesis, or the process of bone formation, to the ISS. The mice will be brought back during a later mission to see how their skeletons develop in space and whether they experience bone loss. <br /><br />As an orthopedic surgeon, Satcher is also interested in the negative side effects of microgravity. Although scientists have known for decades that microgravity causes bone to lose essential minerals, muscles to atrophy and the cardiovascular system to weaken, they are still experimenting with different countermeasures to pinpoint the best way to keep astronauts healthy.  <br /><br />Satcher was eager to test various resistive exercises, including doing squats or running attached to a treadmill, that he believes are “very effective” countermeasures to offset the effects of microgravity. “We know exercise works,” he said, adding that it still took several days for his muscles and sense of balance to readjust when he returned to Earth.<br /><br />Because the space shuttle program is scheduled to end later this year, Satcher said that he would have to fly aboard a Russian spacecraft if he wanted to return to space before 2020. Even so, he remained positive about NASA’s future under President Obama’s new space policy, which would boost NASA’s budget by $6 billion over the next five years. “NASA has done a tremendous amount with very little,” he said, adding that American taxpayers pay more each year for Halloween activities ($25) than they do to send people into space ($20). <br /><br />]]></body>
</item>
<item>
<title><![CDATA[More is less]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/hidden-variables-0602.html</link>
<story_id>15394</story_id>
<featured>0</featured>
<description><![CDATA[Complex computer models can involve thousands of variables. But paradoxically, adding more variables can sometimes make them easier to work with.]]></description>
<postDate>Wed, 02 Jun 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100601160925-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100601160925-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100601160925-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
<imageCaption><![CDATA[Explaining the relationships between observable data (blue dots) can involve complicated mathematics that correlates each data point with each of the others (blue lines). But a “hidden variable” that describes general properties of all the data points (green dot) can make the mathematics much simpler (green lines).]]></imageCaption>
</image>
<body><![CDATA[The architect Mies van der Rohe is famous for promoting the slogan “less is more.” But if Venkat Chandrasekaran, a graduate student in the Department of Electrical Engineering and Computer Science, had a slogan for his own work, it might be “more is less.”<br /><br />Science, engineering and other quantitative disciplines are largely concerned with uncovering the mathematical relationships between data points — such as energies of molecules, measurements of temperature or gene activity, or stock prices. In most cases, adding more data points just makes the math more complicated. But sometimes it makes it simpler. And for many types of calculations, if there are additional data points that will make them simpler, Chandrasekaran’s techniques will find them.<br /><br />To see how adding data points can mean simpler calculations, suppose that you’re trying to understand the relationships between a bunch of stocks in the same industry sector — say, Apple, Gateway, Dell, Hewlett-Packard and other computer manufacturers. On the one hand, an increase in Apple’s share price could mean a decrease in, say, Dell’s, because Apple and Dell compete for a limited pool of computer buyers’ dollars; on the other hand, if large institutional investors are bullish about computer stocks in general, an increase in Apple’s stock could indicate an increase in Dell’s as well. <br /><br />It might be possible to build a complicated mathematical model that, on the basis of considerations like the companies’ price-to-earnings ratios, trade volumes and revenues determines whether an increase in Apple’s share price will cause an increase or decrease in Dell’s — and Gateway’s, and Hewlett-Packard’s, and so on. But it might also turn out that a single extra variable — say, the average price of all the companies’ stock — provides a good indication of general trends in the sector. Since the new variable accounts for institutional investors’ enthusiasm or skittishness, the relationships between the individual stocks no longer have to. The overall calculation becomes much simpler.<br /><br /><strong>Irrelevant referent</strong><br /><br />In this case, Chandrasekaran’s techniques would tell you only that adding another variable — the average stock price — simplifies the overall calculation. They wouldn’t tell you why. And indeed, the extra variable could turn out to be something more complicated than an average. It might factor in the price-to-earnings ratios of some companies, the revenues of others, the share prices of still others, and so on. A savvy analyst might be able to deduce that this new, more complex variable represents the trading strategies of a bunch of large hedge funds that concentrate on the computer industry. But then again, it could be that no one has any idea what the new variable refers to.<br /><br />“There’s this temptation that I even had initially, that you can sort of discover hidden variables,” says Chandrasekaran. “And that’s true: You can discover hidden variables. But it’s not going to be easy to attribute meaning to these hidden variables.” For most purposes, however, that may not matter. “From the mathematical point of view, just putting these things in helps you simplify,” Chandrasekaran says. If the added variable helps you predict Dell’s share price from Apple’s, does it really matter what it refers to — or whether it refers to anything at all?<br /><br /><strong>Getting to the bottom</strong><br /><br />At the most recent Symposium on System Identification, hosted by the International Federation of Automatic Control, Chandrasekaran and MIT Professors of Electrical Engineering Alan Willsky and Pablo Parrilo described their approach to finding hidden variables that simplify calculations. <br /><br />Generally, computer science is concerned with questions of computational complexity: Given a particular algorithm, you want to know whether a computer can execute it quickly, slowly or never. So computer science provides some standard methods for calculating the complexity of mathematical models.<br /><br />If you have an equation that describes the complexity of a mathematical model, you want to find its minimum values: where the complexity is lowest, the model is simplest, and thus easiest to work with. If you imagine the graph of the equation as a complex surface with lots of peaks and troughs, you want to find the bottom of the deepest trough.<br /><br />But that in itself can be a prohibitively complex process. Computer scientists have developed a host of methods for analyzing such equations and finding solutions that are <em>probably</em> near the bottom of a trough in a particular region of the graph. For certain types of problems, however, the techniques developed by Chandrasekaran and his colleagues are mathematically guaranteed to find the bottom of the graph’s lowest trough.<br /><br />According to Ben Recht, an assistant professor in the University of Wisconsin’s computer sciences department, “There are a lot of people who would be surprised if you told them that you could solve this particular hidden-variable problem using [Chandrasekaran’s] methods.” He adds, however, that “it’s not a general-purpose tool, even for these hidden-variable problems.” Chandrasekaran agrees. In fact, he prefers to describe his methods as “tricks” rather than “techniques,” because it might require some mathematical insight to determine how to apply them in any particular case.<br /><br />Still, Recht says, “he’s shown that in a relatively large set of cases, you can actually use this. And it’s a first step to explore the space of what sorts of problems can be solved using this technology.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Knightian uncertainty]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/explained-knightian-0602.html</link>
<story_id>15391</story_id>
<featured>0</featured>
<description><![CDATA[The economic crisis has revived an old philosophical idea about risk and uncertainty. But what is it, exactly?]]></description>
<postDate>Wed, 02 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100601171218-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100601171218-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100601171218-0.jpg</fullURL>
</image>
<body><![CDATA[The global economic crisis of the last two years has stemmed, in part, from the inability of financial institutions to effectively judge the riskiness of their investments. For this reason, the crisis has cast new attention on an idea about risk from decades past: “Knightian uncertainty.” <br /><br />Frank Knight was an idiosyncratic economist who formalized a distinction between risk and uncertainty in his 1921 book, <em>Risk, Uncertainty, and Profit</em>. As Knight saw it, an ever-changing world brings new opportunities for businesses to make profits, but also means we have imperfect knowledge of future events. Therefore, according to Knight, risk applies to situations where we do not know the outcome of a given situation, but can accurately measure the odds. Uncertainty, on the other hand, applies to situations where we cannot know all the information we need in order to set accurate odds in the first place. <br /><br />“There is a fundamental distinction between the reward for taking a known risk and that for assuming a risk whose value itself is not known,” Knight wrote. A known risk is “easily converted into an effective certainty,” while “true uncertainty,” as Knight called it, is “not susceptible to measurement.” An airline might forecast that the risk of an accident involving one of its planes is exactly one per 20 million takeoffs. But the economic outlook for airlines 30 years from now involves so many unknown factors as to be incalculable.<br /><br />Some economists have argued that this distinction is overblown. In the real business world, this objection goes, all events are so complex that forecasting is always a matter of grappling with “true uncertainty,” not risk; past data used to forecast risk may not reflect current conditions, anyway. In this view, “risk” would be best applied to a highly controlled environment, like a pure game of chance in a casino, and “uncertainty” would apply to nearly everything else. <br /><br />Even so, Knight’s distinction about risk and uncertainty may still help us analyze the recent behavior of, say, financial firms and other investors. Investment banks that in recent years regarded their own apparently precise risk assessments as trustworthy may have thought they were operating in conditions of Knightian risk, where they could judge the odds of future outcomes. Once the banks recognized those assessments were inadequate, however, they understood that they were operating in conditions of Knightian uncertainty — and may have held back from making trades or providing capital, further slowing the economy as a result. <br /><br />Ricardo Caballero, chair of MIT’s Department of Economics and the Ford International Professor of Economics, Macroeconomics, and International Finance, is among those who have recently invoked Knightian uncertainty to explain the behavior of investors in times of financial panic. As Caballero stated in a lecture at the International Monetary Fund’s research conference last November: When investors realize that their assumptions about risk are no longer valid and that conditions of Knightian uncertainty apply, markets can witness “destructive flights to quality” in which participants rid their portfolios of everything but the safest of investments, such as U.S. Treasury bonds. <br /><br />One solution offered by Caballero to stem these moments of panic is government-issued investment insurance for large financial institutions. In this sense, the existence of Knightian uncertainty is not just a quasi-philosophical dispute; the subjective perception of Knightian uncertainty among businesses is a pressing practical problem.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Sylvia T. Ceyer named head of Department of Chemistry]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/ceyer-chemistry-head.html</link>
<story_id>15395</story_id>
<featured>0</featured>
<description><![CDATA[The specialist in surface science will succeed Timothy Swager, effective July 1]]></description>
<postDate>Tue, 01 Jun 2010 21:04:11 EDT </postDate>
<otherImages>
<image>
<fullURL width='180' height='239'>http://web.mit.edu/newsoffice/images/article_images/20100601170849-1.jpg</fullURL>
<imageCaption><![CDATA[Sylvia T. Ceyer]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Sylvia T. Ceyer, the J. C. Sheehan Professor of Chemistry, will begin her new role as head of the Department of Chemistry on July 1, 2010. Ceyer will succeed current department head, Timothy Swager. Professor John Essigmann will join her as associate department head. <br /><br />“Professor Ceyer is distinguished for her research in surface science, is recognized at MIT for her outstanding classroom teaching and has an exceptional record of service, both at MIT and on the national scene,” said Marc Kastner, dean of the School of Science. “I look forward to working closely with her in her new position as head of chemistry.”<br /><br />Ceyer will succeed current department head, <a href="http://www.mit.edu/~chemistry/faculty/swager.html" target="_blank">Timothy Swager.</a> “Tim has been an energetic and creative department head, and I am grateful to him for serving the chemistry department so well for the past five years,” said Dean Kastner.<br /><br />Professor Ceyer’s current research explores the dynamics of reactions between molecules and solid surfaces, specializing in the use of ultra-high vacuum molecular beam scattering techniques.  She has uncovered the physical origins of the lack of surface reactivity under ultra-high vacuum conditions and then used that knowledge to effect high-pressure heterogeneous catalytic reactions in an ultrahigh vacuum environment where microscopic reaction steps can be discerned. Her research also focuses on the conversion of natural gas to usable fuels and understanding the fundamental mechanisms for plasma etching of semiconductors.<br /><br />“It is my goal to further the Department of Chemistry’s commitment to outstanding chemical research and education as set by a long line of distinguished department heads and faculty,” said Ceyer. “Working with John Essigmann as associate head offers a unique opportunity to build a strong alliance between science and engineering,” she added. <br /><br />Ceyer’s achievements in teaching, research, and service have earned her numerous awards, including the Gibbs Medal, Hope College Distinguished Alumni Award, the first W.M. Keck Foundation Professorship in Energy, Nobel Laureate Signature Award for Graduate Education, Baker Award for Undergraduate Teaching, School of Science Teaching Prize, Arthur Smith Award, and a MacVicar Teaching Fellowship. Ceyer is chair of the Physical and Mathematical Sciences Class of the National Academy of Sciences and former chair of its chemistry section, and a fellow of the American Academy of Arts and Sciences. She is a member of the Basic Energy Sciences Advisory Committee for the Department of Energy and served as associate editor of Physical Review Letters. Before joining the MIT Department of Chemistry in 1981, Professor Ceyer received her PhD from the University of California at Berkeley and worked at the National Bureau of Standards as a post-doctoral fellow. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: John Marshall on the Gulf of Mexico oil spill]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/3q-marshall-oil-0601.html</link>
<story_id>15378</story_id>
<featured>0</featured>
<description><![CDATA[An MIT oceanographer discusses why ‘we have never had a spill like this’ — and what that means for cleanup efforts.]]></description>
<postDate>Tue, 01 Jun 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100527120830-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100527120830-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100527120830-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: NASA]]></imageCredits>
<imageCaption><![CDATA[This image, taken by NASA's Terra satellite, shows the growing oil slick in the Gulf of Mexico reflected by the sun.]]></imageCaption>
</image>
<body><![CDATA[<em>More than a month after the tragic events that set off the largest oil spill in U.S. history, scientists and BP officials continue to disagree over the amount of oil that has escaped into the Gulf of Mexico. Unlike other oil spills that have occurred relatively close to the surface, this spill is located about a mile down, which has made it difficult to predict the effects. John Marshall, an oceanographer in MIT’s Department of Earth, Atmospheric and Planetary Sciences, spoke to MIT News about the deep-sea catastrophe and the long, difficult cleanup ahead. <br /><br /></em>Q. What is it about this oil spill that is going to significantly complicate cleanup efforts?<br /><br />A. The challenge for the cleanup operation depends on two factors: how much oil has escaped from the broken well - a figure that has been in much dispute - and to what degree it has spread around the ocean. The big problem for the cleanup is that in this spill, unlike the surface releases from broken oil tankers, such as the Amoco Cadiz in 1978 and the Exxon Valdez in 1989, the oil is coming up from the bottom, some 5,000 feet down. As the oil makes its way to the surface, the whole depth of the ocean is being contaminated. We have never had a spill like this before. Oil bubbling up from the seabed is folding in water from its surroundings, much like how smoke coming out of a chimney mixes in with surrounding air. The good thing is that the mixing process hugely dilutes the rising oil plume, breaking it into lots of very fine particles. The bad thing is that large volumes of water -many, many times larger than the millions of gallons of crude that have escaped - become contaminated with the oil and the dispersant chemicals being added at the source. Think of an Alka Seltzer tablet in a cup of water creating small bubbles that rise up to the surface. In the oil leak, the bubbles are tiny pockets of warm oil being mixed with gases like methane and other substances that undergo chemical reactions as they rise.<br /><br />But unlike the Seltzer bubble analogy, it appears that a significant quantity of oil is not making it directly up to the surface. There are reports of two large subsurface clouds, one at 1,500 feet and one at 3,000 feet. It's not clear where these clouds of microscopic oil particles are going or how to clean them up. It's likely that nothing can be done about this subsurface oil, and that we will have to rely on nature's solution - bacteria that eat oil - to deal with the problem.<br /><br />The oil that does make it to the surface is then influenced by winds blowing over the surface of the Gulf. Over time, the oil spreads horizontally by currents powered by wind and tides, including the Gulf of Mexico's powerful stream known as the "loop current," and is eventually drawn into the general circulation of the ocean. Some of it will be carried away to contaminate other parts the Gulf, and some of it will be washed up on local shores.<br /><br />Q. There have been reports that the oil may already have seeped into the loop current. What would this mean for the rest of the Gulf of Mexico and for the Atlantic seaboard?<br /><br />A. Mixing and spreading of the oil into the wider ocean will make it much more difficult to corral the significant quantities of oil released. Our cleaning and capturing efforts will end up processing just a tiny fraction of it. The oil slick at the surface can be tracked by satellite and is already being drawn into complex swirling patterns as it is carried along by currents. Ocean circulation models are proving useful in figuring out where the oil is going. We can predict the likely general path of the slick as it is drawn into the loop current and out of the Gulf in to the Atlantic. However, just as there is a fundamental limit to the predictability of weather, the detailed positions and timings of these oil swirls are unpredictable. Some coastlines will avoid the disaster, and others may be hit hard. Another big problem is that we have few observations of where the subsurface clouds of oil are spreading and when and if they will make landfall.<br /><br />Q. Some scientists are saying that a lot of the damage that has occurred so far as a result of this spill has occurred in the water column. What is the water column, and what are the implications of damage to the water column?<br /><br />A. Yes, it appears that lots of microscopic particles of oil are hanging out at mid-depth in the water column, which is the column of water that extends from the sea surface to the sea floor. In normal circumstances, the water column arranges itself so that light, generally warmer fluid is on top, and heavy, colder fluid below. <br /><br />As the plume of oil, water and gas rises through the water column, it loses buoyancy, or the ability to float, because of the mixing in of colder surrounding fluid. It appears that this is actually causing the rising oil clouds to be temporarily arrested at particular levels in the water column, from which they then spread out horizontally.<br /><br />Biologists are concerned about the effect these stalled clouds could have on life in the water column, not just at mid-depth and the surface, but also on bottom-dwelling organisms that ultimately depend for their food on dead organic matter that sinks down from above. Some scientists argue that the surfactant, or soapy wetting agent, being added at the bottom source and at the surface of the ocean to help disperse the oil, could ultimately cause more damage to the health of the water column than the oil itself. The health of the water column might only be restored after many years, perhaps a decade or even longer.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Giving proteins a new glow]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/cell-labeling-0601.html</link>
<story_id>15386</story_id>
<featured>0</featured>
<description><![CDATA[MIT chemists have designed a way to fluorescently label proteins that could shed light on protein functions never before seen.]]></description>
<postDate>Tue, 01 Jun 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100528131147-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100528131147-1.jpg</smallURL>
<fullURL width='368' height='369'>http://web.mit.edu/newsoffice/images/article_images/20100528131147-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Katharine White and Tao Uttamapinant]]></imageCredits>
<imageCaption><![CDATA[MIT researchers have designed a fluorescent probe that can be targeted to different locations within a cell. Here, the probe is labeling only proteins in the cell membrane.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100528131147-2.png</fullURL>
<imageCredits><![CDATA[Image: Katharine White and Tao Uttamapinant]]></imageCredits>
<imageCaption><![CDATA[The probe labels proteins in the cell nucleus.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='370'>http://web.mit.edu/newsoffice/images/article_images/20100528131147-3.png</fullURL>
<imageCredits><![CDATA[Image: Katharine White and Tao Uttamapinant]]></imageCredits>
<imageCaption><![CDATA[The probe labels proteins in the cytosol.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Since the 1990s, a green fluorescent protein known simply as GFP has revolutionized cell biology. Originally found in a Pacific Northwest jellyfish, GFP allows scientists to visualize proteins inside of cells and track them as they go about their business. Two years ago, biologists who discovered and developed the protein as a laboratory tool won a Nobel Prize for their work.<br /><br />However, using GFP as a fluorescent probe has one major drawback — the protein is so bulky that it can interfere with the proteins it’s labeling, preventing them from doing their normal tasks or reaching their intended destinations. <br /><br />“For a long time, people have been trying to find better ways to label proteins,” says Katharine White, an MIT graduate student in the lab of Alice Ting, associate professor of chemistry. <br /><br />Ting, White and their colleagues have now come up with a new way to overcome the disadvantages of GFP, by tagging proteins with a much smaller probe. Their probe allows proteins to carry out their normal functions, offering scientists the chance to glimpse never-before-seen activity.<br /><br />The researchers describe the new technique, dubbed PRIME (PRobe Incorporation Mediated by Enzymes), in the <em>Proceedings of the National Academy of Sciences</em> this week. <br /><br /><strong>Tracking proteins</strong><br /><br />First isolated from a jellyfish in 1962, GFP allows scientists to track otherwise invisible proteins as they move about the cell, orchestrating processes such as cell division and metabolism. To achieve this, scientists tack the gene for GFP onto the gene for the protein they want to study. After the engineered gene is introduced into cells, it will produce proteins that glow fluorescent green. <br /><br />However, GFP’s large size (238 amino acids) can interfere with some proteins, such as actin, a molecule that helps give cells their structure and is involved in cell division, motility and communication with other cells.<br /><br />“People use fluorescent proteins to study actin all the time, but fusion to the fluorescent proteins has detrimental effects on actin’s function and trafficking,” says Tao Uttamapinant, co-lead author of the PNAS paper with White and former MIT postdoctoral associate Hemanta Baruah. <br /><br />To overcome the drawbacks of GFP, Ting and her students used a blue fluorescent probe that is much smaller than GFP. Unlike GFP, the new probe is not joined to the target protein as it’s produced inside the cell. Instead, the probe is attached later on by a new enzyme that the researchers also designed.<br /><br />For this to work, the researchers must add the gene for the new enzyme, known as a fluorophore ligase, to each cell at the same time that they add the gene for the protein of interest. They also add a short tag (13 amino acids) to the target protein, and this tag allows the enzyme to recognize the protein. When the blue fluorescent probe (7-hydroxycoumarin) is added to the cell, the enzyme attaches it to the short tag on the target protein.<br /><br />With this method, proteins such as actin can move freely throughout the cell and cross into the nucleus even when tagged with the fluorescent probe. <br /><br />The researchers also demonstrated that they can label proteins in specific parts of the cell, such as the nucleus, cell membrane or cytosol (the interior of the cell), by tagging the enzyme with genetic sequences that direct it to specific locations. That way, the enzyme attaches the fluorescent probe only to proteins in those locations.<br /><br />That ability to target proteins in a specific part of the cell is the most impressive aspect of the new method, says Jun Yin, assistant professor of chemistry at the University of Chicago. “If you study a protein, the first thing you want to know is where it’s located and what its trafficking pattern is,” says Yin, who was not involved in this research.<br /><br />Yin, who also studies protein labeling, believes the new MIT technique is general enough that it could be modified to detect other changes inside a cell. “It has potential not only for fluorophores, but also other probes to respond to the microenvironment, such as pH changes or changes in ion concentrations,” which would shed further light on cell functions, he says. <br /><br />The MIT team is now working on engineering enzymes that will work with other types of probes. Ting has also filed for a patent on the fluorescent probe technique and plans to commercialize the technology so other labs can use it.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Obama intends to nominate Suresh as next NSF director]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/suresh-nsf-06039987.html</link>
<story_id>15410</story_id>
<featured>0</featured>
<description><![CDATA[The MIT engineering dean would lead the independent federal agency that supports science and engineering research.]]></description>
<postDate>Mon, 31 May 2010 21:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100604103822-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100604103822-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100604103822-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Justin Knight]]></imageCredits>
<imageCaption><![CDATA[U.S. President Barack Obama intends to nominate Subra Suresh, dean of the MIT School of Engineering, to serve as the next director of the National Science Foundation.]]></imageCaption>
</image>
<body><![CDATA[U.S. President Barack Obama <a href="http://www.whitehouse.gov/the-press-office/president-obama-announces-more-key-administration-posts-1" target="_blank">announced Thursday</a> that he intends to nominate Subra Suresh, dean of the MIT School of Engineering, to serve as the next director of the National Science Foundation. If confirmed by the U.S. Senate, Suresh, the Vannevar Bush Professor of Engineering at MIT, would be appointed to a six-year term as director.<br /><br />“Through his invigorating leadership, Dean Suresh has led MIT's School of Engineering while pursuing his own remarkable research portfolio at the intersection of the life sciences and engineering,” said MIT Provost L. Rafael Reif. “In keeping with MIT's long tradition of national service, he will bring this same breadth of knowledge and vision to the National Science Foundation.”<br /><br />The White House made its announcement about the president’s intention to nominate Suresh in a statement that also included news of the president’s intention to nominate Maura Connelly to be U.S. ambassador to Lebanon and Daniel B. Smith to be U.S. ambassador to Greece. “I am proud that such experienced and committed individuals have agreed to take on these important roles in my administration. I look forward to working with them in the coming months and years,” Obama said. <br /><br />The NSF is a federal government agency that supports fundamental research and education in all the non-medical fields of science and engineering. With an annual budget of nearly $7 billion, the NSF funds approximately 20 percent of all federally supported basic research conducted by U.S. colleges and universities.<br /> <br />Trained as a mechanical engineer, Suresh has made dramatic contributions to a range of fields in engineering and science. He has expanded his research interests to encompass materials, nanotechnology and the life sciences, and has most recently done extensive work on the red blood cell and its nanobiomechanical properties as they influence a variety of diseases. Suresh has made significant advances and created a range of new experimental methodologies to unravel the inner workings of such diseases as malaria.<br /><br />Suresh is the author of more than 220 research articles in international journals, coeditor of five books, and coinventor on more than 12 U.S. and international patents. More than 100 students, postdoctoral associates, and research scientists have trained in his research group, and many now occupy prominent positions in academia, industry and governments around the world. He is author or coauthor of several books, including <em>Fatigue of Materials and Thin Film Materials</em> — widely used in materials science and engineering. <br /><br />Suresh has held joint faculty appointments in four MIT departments, and has served as dean of the School of Engineering since July 2007. During his tenure, the school has seen unprecedented growth in the diversity of its faculty. Approximately 45 new faculty members have joined the school since he became dean, and in 2009, for the first time in its history, the school hired more new women faculty than men. <br /><br />“Subra is an outstanding engineering scientist,” said Marc Kastner, dean of MIT’s School of Science. “He has a very broad perspective on why science is important for its own sake — as well as for its applications.”<br /><br />Suresh received his bachelor of technology degree from the Indian Institute of Technology, Madras, in 1977, his MS from Iowa State University in 1979 and his ScD from MIT in 1981. Following postdoctoral research from 1981 to 1983 at the University of California at Berkeley and the Lawrence Berkeley National Laboratory, he joined Brown University as an assistant professor of engineering in December 1983; he was promoted to full professor in July 1989. He joined MIT in 1993 as the R. P. Simmons Professor of Materials Science and Engineering.<br /><br />Suresh is the recipient of the 2007 European Materials Medal, the highest honor conferred by the Federation of European Materials Societies, and the 2006 Acta Materialia Gold Medal. In 2006, <em>Technology Review</em> magazine selected Suresh's work on nanobiomechanics as one of the top 10 emerging technologies that “will have a significant impact on business, medicine or culture.” He has been elected to the U.S. National Academy of Engineering, the American Academy of Arts and Sciences, the Indian National Academy of Engineering, and the German National Academy of Sciences, the Indian Academy of Sciences and the Spanish Royal Academy of Sciences.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT explores plans for enlivening Kendall Square]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/kendall-square.html</link>
<story_id>15380</story_id>
<featured>0</featured>
<description><![CDATA[Discussions begin within MIT and Cambridge communities.]]></description>
<postDate>Fri, 28 May 2010 13:01:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100528151242-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100528151242-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100528151242-0.jpg</fullURL>
</image>
<body><![CDATA[MIT has initiated preliminary conversations within the Institute community, as well as with Cambridge officials and representatives of Kendall Square and the surrounding neighborhoods, to discuss early concepts for invigorating the Kendall Square area. <br /><br />While no specific project has yet been proposed, MIT is considering ideas for its properties near the MBTA station that would create a more vibrant environment consistent with Kendall Square’s position as an innovation cluster. The project is being led by the MIT Investment Management Company (MITIMCo), a division of the Massachusetts Institute of Technology that manages the investment of the Institute’s endowment and all investment real estate.<br /><br />Kendall Square is currently home to several academic and research buildings affiliated with MIT, as well as over 150 biotechnology and information technology firms, including Microsoft and Google, the Cambridge Innovation Center, and a growing number of clean-energy and technology startup companies. While the concentration of corporate, research and academic activities in Kendall Square began to take shape over the last 10 to 15 years, efforts to complement the success of those endeavors with a more dynamic social community have been less successful to date. <br /><br />“Kendall Square is home to a kind of creative intensity that you don’t encounter many other places on Earth: It has an entrepreneurial culture and an incredibly inspiring focus on society’s important problems,” MIT President Susan Hockfield said in an address to the Kendall Square Association in February. “If we want Kendall Square to grow and thrive over the long term, we need to make sure that the most creative entrepreneurs and most talented inventors and scientists find Kendall Square so magnetic, so appealing that they can’t think seriously about ‘other options.’” <br /><br />MIT has been a leading participant in efforts by the Kendall Square Association (KSA), which represents more than 100 local organizations, to explore ways to maximize the potential of the Square. MIT and the KSA have the shared goal of creating retail, amenities, cultural opportunities, and public spaces that foster collaboration and dynamic synergies — all in the heart of a world-class business and academic hub that is fueling the future of innovation.<br /><br />“Past initiatives to improve the streetscape and add amenities to Kendall Square have been disparate and fairly limited in scope,” said Steve Marsh, managing director for real estate for MITIMCo. “These piecemeal efforts haven’t had the scale and concentration of retail activity necessary to meet the needs of the entrepreneurs, established companies, academicians, students, neighbors and business leaders who have come to value and rely on Kendall Square’s creative energy.”<br /><br />MITIMCo is currently considering a variety of approaches for a transit-oriented, mixed-use redevelopment of parcels it owns in Kendall Square, with a focus on supporting existing businesses and maximizing benefits for the MIT community and nearby neighborhoods. Over the next several months, it will develop a more formal proposal for potential uses, including ground-floor retail, venues for the public to gather, corporate, academic, and research space and housing. <br /><br />As part of this process, MITIMCo will continue communicating within the Institute community and with tenants in Kendall Square, neighbors, as well as business leaders and city officials, to provide a timeline of activities and next steps.<br /><br />“Our objective is to fuel innovation in Cambridge by creating increased vitality in Kendall Square through the animation of the streetscape. We want to introduce expanded retail and entertainment uses and provide continued opportunities for forward-thinking companies to grow and flourish. We also hope these efforts will help attract new life-sciences, technology, energy and other entrepreneurial ventures to Kendall Square, and that a revitalized Kendall Square will create jobs and increase the tax base. Naturally, as we explore possibilities for redevelopment we will need to ensure that MIT’s academic needs are met going forward,” Marsh said.<br /><br />“Toward those goals,” he continued, “we will be eager to receive input from both the Cambridge and MIT communities about what people would be most excited to see in a revitalized Kendall Square, and particularly, creative ideas for the retail and public gathering spaces.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[‘Tattoo’ may help diabetics track their blood sugar]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/glucose-tattoo-0528.html</link>
<story_id>15382</story_id>
<featured>0</featured>
<description><![CDATA[Chemical engineers are working on carbon nanotubes that could be injected under the skin to reveal blood glucose levels.]]></description>
<postDate>Fri, 28 May 2010 04:00:03 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100527150658-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100527150658-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100527150658-0.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='185' height='260'>http://web.mit.edu/newsoffice/images/article_images/20100527150853-2.jpg</fullURL>
<imageCaption><![CDATA[Michael Strano, the Charles and Hilda Roddey Associate Professor of Chemical Engineering ]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[People with type I diabetes must prick their fingers several times a day to test their blood sugar level. Though the pain is minor, the chore interferes with daily life.<br /><br />“They never really escape it,” says Paul Barone, a postdoctoral researcher in MIT’s Department of Chemical Engineering. Barone and professor Michael Strano are working on a new type of blood glucose monitor that could not only eliminate the need for finger pricks but also offer more accurate readings.<br /><br />“Diabetes is an enormous problem, global in scope, and despite decades of engineering advances, our ability to accurately measure glucose in the human body still remains quite primitive,” says Strano, the Charles and Hilda Roddey Associate Professor of Chemical Engineering. “It is a life-and-death issue for a growing number of people.”<br /><br />Strano and Barone’s sensing system consists of a “tattoo” of nanoparticles designed to detect glucose, injected below the skin. A device similar to a wristwatch would be worn over the tattoo, displaying the patient’s glucose levels. <br /><br /><strong>Continuous glucose detection</strong><br /><br />A <a href="http://content.nejm.org/cgi/content/full/NEJMoa0805017" target="_blank">2008 study</a> in the <em>New England Journal of Medicine</em> showed that continuous monitoring helped adult type I diabetes patients who were at least 25 years old better control their blood glucose levels. However, existing wearable devices are not as accurate as the finger-prick test and have to be recalibrated once or twice a day — a process that still involves pricking the finger. <br /><br />“The most problematic consequences of diabetes result from relatively short excursions of a person’s blood sugar outside of the normal physiological range, following meals, for example,” says Strano. “If we can detect and prevent these excursions, we can go a long way toward reducing the devastating impact of this disease.”<br /><br />Most existing continuous glucose sensors work via an injection of an enzyme called glucose oxidase, which breaks down glucose. An electrode placed on the skin interacts with a by-product of that reaction, hydrogen peroxide, allowing glucose levels to be indirectly measured. However, none of those sensors have been approved for use longer than seven days at a time.<br /><br />Bruce Buckingham, a professor of pediatric endocrinology at the Stanford School of Medicine and an author of the <em>NEJM</em> study, says glucose monitoring is definitely headed toward wearable sensors. However, he expects it will be a few years before any are approved for use without backup monitoring with a finger prick test. “As time goes on, the devices to do this should become smaller, easier to wear, and more accurate,” says Buckingham, who is not involved in the MIT project.<br /><br /><strong>Taking advantage of nanotubes</strong><br /><br />The technology behind the MIT sensor, described <a href="http://pubs.acs.org/doi/full/10.1021/nn901025x?prevSearch=%255Bauthor%253A%2Bstrano%255D&amp;searchHistoryKey=" target="_blank">in a December 2009 issue</a> of <em>ACS Nano</em>, is fundamentally different from existing sensors, says Strano. The sensor is based on carbon nanotubes wrapped in a polymer that is sensitive to glucose concentrations. When this sensor encounters glucose, the nanotubes fluoresce, which can be detected by shining near-infrared light on them. Measuring the amount of fluorescence reveals the concentration of glucose. <br /><br />The researchers plan to create an “ink” of these nanoparticles suspended in a saline solution that could be injected under the skin like a tattoo. The “tattoo” would last for a specified length of time, probably six months, before needing to be refreshed. <br /><br />To get glucose readings, the patient would wear a monitor that shines near-infrared light on the tattoo and detects the resulting fluorescence. One advantage of this type of sensor is that, unlike some fluorescent molecules, carbon nanotubes aren’t destroyed by light exposure. “You can shine the light as long as you want, and the intensity won’t change,” says Barone. Because of this, the sensor can give continuous readings.<br /><br />Development of the nanoparticles and the wearable monitor is being funded by MIT’s Deshpande Center for Technological Innovation.<br /><br />Barone and Strano are now working to improve the accuracy of their sensor. Any glucose monitor must pass a test known as the Clarke Error Grid, the gold standard for glucose-sensor accuracy. The test, which compares sensor results to results from a lab-based glucose meter, needs to be very stringent, since mistakes in glucose detection can be fatal. <br /><br />They are still years away from human trials, says Barone, but they may soon start trials in animals. Those tests will be key to determining the value of this approach, says Buckingham. “You don’t know how good it will be until you put it in someone and see how strong the signal is,” he says.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[‘Efficiency Forward']]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/nstar-conference-0528.html</link>
<story_id>15376</story_id>
<featured>0</featured>
<description><![CDATA[Ambitious collaboration between MIT and NSTAR aims to cut campus electricity use by 15 percent over 3 years.]]></description>
<postDate>Fri, 28 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100527173054-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100527173054-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100527173054-0.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100526162253-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Melanie Gonick]]></imageCredits>
<imageCaption><![CDATA[From left to right, Associate Professor of Electrical Engineering Vladimir Bulovic, MIT Executive Vice President and Treasurer Theresa M. Stone, and Joseph R. Nolan Jr., NSTAR Senior Vice President of Customer and Corporate Relations at Wednesday's press conference. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100526162253-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Melanie Gonick]]></imageCredits>
<imageCaption><![CDATA[Jason Jay, a doctoral student in the MIT Sloan School of Management and who has been a member of the MIT Campus Energy Task Force for three years, speaks at Wednesday's press conference.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In what could serve as a model for achieving large-scale energy-efficiency improvements, MIT and Boston-based utility NSTAR on Wednesday announced an ambitious collaboration that aims to slash the Institute’s electricity use by 15 percent over the next three years. <br /><br />The program, “MIT Efficiency Forward,” is expected to save MIT $50 million in energy costs over the next decade by incorporating a portfolio of energy-saving projects and approaches, including sustainable new construction, major equipment retrofits, behavior-change programs, electric and gas incentives and experimentation with new technologies. While the goal is for MIT to save at least 34 million kilowatt-hours — equivalent to the yearly amount of electricity consumed by more than 4,500 Massachusetts homes — by 2013, NSTAR officials said they expect the Institute will exceed that mark.<br /><br />“We are hoping this is really the first step, not only for MIT, but for other institutions and corporations throughout Massachusetts,” said Joseph R. Nolan, Jr., NSTAR senior vice president of customer and corporate relations. In addition to contributing more than $1.5 million to the approximately $13 million program, NSTAR will provide energy-efficiency tools and expertise to assist MIT.<br /><br />MIT Efficiency Forward marks the largest efficiency project in NSTAR’s history and is meant to advance the Green Community Act that Massachusetts Gov. Deval Patrick signed in July 2008. Under the law, utility companies must offer significantly more rebates and other incentives for customers to upgrade lighting, air conditioning and industrial equipment to more efficient systems whenever those incentives cost less than the cost to generate the electricity to power the less-efficient equipment.<br /><br />Susan Coakley of Northeast Energy Efficiency Partnerships, a nonprofit that facilitates partnerships to advance energy efficiency in the Northeast, said MIT Efficiency Forward’s three-year goal to reduce consumption by 15 percent was both “aggressive” and “achievable.” She said the goal was in line with that of other major research institutions and universities such as Princeton University, which is working to reduce overall utility usage on campus by at least 25 percent over the next 10 years. <br /><br /><strong>Living laboratory</strong><br /><br />MIT Efficiency Forward builds on work developed and coordinated by the Campus Energy Task Force, which was launched by the MIT Energy Initiative (MITEI) in 2006 to reflect the Institute’s commitment to efficiency, innovation, cutting carbon emissions and energy usage and to serve as a model for other universities. By treating MIT’s campus as a living laboratory for research, educational and learning opportunities, the task force has helped MIT “walk the talk” on energy and sustainability. <br /><br />With more than 200 Institute faculty currently working on energy-related research, energy is part of “the fabric of MIT research and education,” said Vladimir Bulovic, an associate professor in the Department of Electrical Engineering and Computer Science. The NSTAR collaboration will take that a step further by providing new teaching opportunities for students.<br /><br />Just as students play an integral role in the task force, so, too, will they be heavily involved in MIT Efficiency Forward. A student advisory group will help devise outreach and awareness strategies, research project opportunities and approaches to measure, monitor and verify energy savings. “One particular option will be to assign teams of freshman and sophomores in analyzing energy conservation and efficiency options, recommending how they can be framed most effectively and monitoring the outcomes as part of project-based curriculum development,” said Donald Lessard, a professor in the MIT Sloan School of Management and a co-chair, with Bulovic, of MITEI’s Energy Education Task Force. MIT Efficiency Forward will also develop opportunities for students to collaborate with faculty and staff on research topics that can support the program’s goals through the Undergraduate Research Opportunities Program (UROP).<br /> <br />MIT Executive Vice President and Treasurer Theresa M. Stone said measuring and verifying the energy savings will be a hallmark of the program. She said the Institute would develop new approaches to measure energy reduction and would share them with other institutions, organizations and regulatory authorities. <br /><br />The Campus Energy Task Force has invested about $3 million in energy-efficiency measures since 2006, and Stone said that the NSTAR collaboration will advise the Institute on how to maximize such efforts by reinvesting the accrued savings in other campus energy-efficiency projects.<br /><br /><strong>How the goal will be met</strong><br /><br />About half of the expected 34-million kwh reduction will result from changes in lighting made across half of the MIT campus, some of which have already occurred in the Stata Center and in the Stratton Student Center. According to Peter Cooper, manager of sustainability engineering and utility planning at the Department of Facilities, these changes include shutting off lights when people don’t need them, dimming lights in certain spaces and changing lighting fixtures to ones that are more effective at reflecting light. Cooper also hopes to implement LED (light-emitting diode) bulbs, which last longer and use less energy than incandescent bulbs, in some locations.<br /><br />About a third of the electricity savings will result from changes to various heating, ventilation and air conditioning (HVAC) systems, such as reducing the speed of the ventilation fans at night. Upgrades to both the lighting and HVAC systems are currently underway at Hayden Memorial Library.<br /><br />The remaining savings will result from behavior-change programs, such as student-driven energy-efficiency campaigns, and sustainable construction techniques being implemented in new buildings, such as those that will house the Koch Institute for Integrative Cancer Research and the MIT Sloan School of Management, Cooper said. Both of those buildings are slated to open within the next year.<br /><br />In addition to MIT Efficiency Forward, the Institute is seeking to reduce greenhouse gas emissions through combined heat and power generation, comprehensive waste minimization and recycling, renewable-power systems, alternative-fuel campus vehicles and extensive commuter-benefit programs.<br /><br /><em>Additional reporting by David L. Chandler</em><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Jim Walsh on the Korean standoff]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/3q-walsh.html</link>
<story_id>15373</story_id>
<featured>0</featured>
<description><![CDATA[An MIT international security expert discusses the escalating tensions between North Korea and South Korea, and the possibility of repairing relations.]]></description>
<postDate>Wed, 26 May 2010 18:17:51 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100526142628-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100526142628-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100526142628-1.jpg</fullURL>
</image>
<body><![CDATA[<em>The always-tense relations between North Korea and South Korea hit a low this week. The South Koreans and their president, Lee Myung-bak, have increasingly come to the conclusion that the sinking of one of their warships in March, which killed 46 sailors, was caused by a North Korean torpedo. Lee has promised to reduce trade ties between the countries while North Korea has threatened that any military retaliation will lead to “all-out war.” Making the situation more problematic, the United States and other countries still hope to dismantle North Korea’s nuclear arsenal. MIT News asked Jim Walsh, a research associate with the Security Studies Program, to comment on the situation. An expert on nuclear proliferation, Walsh has in the past conducted talks with officials in North Korea about security matters and closely follows events in the region. </em><br /><br /><strong>Q.</strong> Why has the situation between North Korea and South Korea deteriorated so rapidly?<br /><br /><strong>A.</strong> There are two answers. The more narrow answer regards the sinking of the South Korean naval ship in late March. There was some feeling at the time this was caused by an old mine left over from World War Two. It seemed unlikely the North Koreans would have attacked this ship, and they denied attacking it. But after a multinational investigation, there was a report issued this month concluding the ship was sunk by a torpedo. So that’s an act of aggression, an affirmative act of war. And naturally the South Korean government felt compelled to respond, and the United States, its long-time ally, felt compelled to support South Korea.<br /><br />Now you might ask why North Korea did this in the first place. And here we’re getting to the famously large field of speculation about North Korean motives. There are several possibilities. It may have been an individual decision made by an individual commander of a submarine. Or it may have been the military acting independently. Or perhaps it was something to do, somehow, with internal problems in North Korea regarding their succession process. The bottom line is that North Korea is opaque enough and relations are complex enough that it’s very hard to know for sure why they would want to do this. It’s usually good to bet on the idea that there’s something internal that’s driving the North Koreans.<br /><br /><strong>Q.</strong> President Lee Myung-bak of South Korea seems to have a very different attitude toward North Korea than the earlier president Kim Dae-Jung, who helped initiate the “sunshine policy” of more open relations. How has this change in South Korea affected the countries’ relationship?<br /><br /><strong>A.</strong> Lee came to power as someone running against the sunshine policy, so yet another theory is that the North Koreans are doing this to embarrass Lee and to weaken him politically. But it’s not clear that’s going to be the result here. Lee was the first president to come to power in some years saying his goal was not to increase exchange with the North Koreans, but a lot of this was rhetorical, because he left a lot of policies in place. And if you look at this most recent statement this week about the incident, he said they would cut back all the trade and exchange they could, except the aid to the women and infants. But that wouldn’t apply to the Kaesong development park [an industrial zone in North Korea where South Korean firms have plants], and that’s about half the trade between the two countries. So Lee came in with a different style and strategy toward the North Koreans, but it has not been night and day. <br /><br /><strong>Q.</strong> What do you think is going to happen, and what would you like to see happen?<br /><br /><strong>A. </strong>What one hopes would happen is that the South makes clear its anger at the sinking of the ship, but that we resume the process of dealing with the ongoing issues on the Korean Peninsula. The ship sinking was awful, it’s a provocative act, but we need to keep our eye on the ball here, and in this case the ball is trying to denuclearize North Korea and bring some stability to the peninsula. I’m hoping that each side can still address the ongoing, deeply dangerous security issues that will not improve unless there’s some sort of dialogue with the North. There’s no getting around it: If we want North Korea to get rid of its nuclear arsenal, to reduce the prospects of war, we’re going to have to talk to the North Koreans — even if it’s unpleasant to do so. <br /><br />Another scenario is that the leaders lose control of this tit-for-tat and it spins off in a direction no one wants to go. And sometimes the North Koreans provoke a crisis on the eve of a positive development — that is a negotiating style they have. But it’s hard to see how this crisis will open a door for progress in the relationship at the moment.<br /><br />Right now a positive move probably means resumption of talks on some topic, like a cultural issue or prisoner swap. I think the most likely scenario is that we muddle along and everything gets pushed off six months. Then the question is: Who benefits from the passage of time? Some argue it benefits the North Koreans because they’ll hold onto their nuclear arms longer, and that becomes a fact on the ground everyone accepts. But you could also argue that because North Korea has deep economic problems, deep succession problems, time is not on their side. In any case, the most likely outcome is that we’ll be in a period of uncomfortable stasis.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT receives 3 Cambridge awards]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/cambridge-awards.html</link>
<story_id>15381</story_id>
<featured>0</featured>
<description><![CDATA[Institute is honored for historical preservation and sustainability]]></description>
<postDate>Wed, 26 May 2010 14:00:04 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100527141115-2.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100527141115-2.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100527141115-2.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of MIT Facilities]]></imageCredits>
<imageCaption><![CDATA[The Stata Center stormwater system.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='238'>http://web.mit.edu/newsoffice/images/article_images/20100527141115-1.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of MIT Facilities]]></imageCredits>
<imageCaption><![CDATA[A cross-section of the Stata stormwater system.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[MIT received three awards from the City of Cambridge this month for various preservation, transportation and environmental projects.<br /><br />The Cambridge Historical Commission honored preservation projects across the city at the 14th annual Preservation Recognition Program, held on May 20 at the newly renovated Main Branch of the Cambridge Public Library. MIT was among the nine honorees for its exterior restoration of W1, the old Ashdown House at 305 Memorial Drive, formerly the Riverbank Court Hotel. The program promotes high-quality preservation work in the City and honors property owners who conserve and protect the city’s architecture. Department of Facilities Program Manager Sonia Richards and Senior Planner Thayer Donham accepted the award on behalf of the Institute. Historical Commission Executive Director Charles Sullivan expressed his appreciation to MIT for giving such robust attention to the exterior features of this historical structure.<br /><br />On May 25, the city honored MIT with two GoGreen Awards for its enhanced transportation and commuting programs and for the Stata Center’s innovative stormwater management and re-use system. At the ceremony, city officials praised MIT’s overall alternative-transportation program, and particularly noted the Institute’s bicycle commuter benefits and amenities, including new maintenance stations, showers and storage facilities. MIT Transportation and Parking Operations Manager Larry Brutti accepted the GoGreen Award on behalf of the Institute in the transportation category.<br /><br />In the stormwater management category, city officials recognized the Stata Center system’s overall objective of improving water quality in the Charles River, and specifically cited the system’s ability to collect 50,000 gallons of rainwater for site irrigation and supplemental toilet-flushing activities. Department of Facilities Director of Campus Planning and Design Pamela Delphenich accepted the stormwater management award.<br /><br />MIT previously received the city’s GoGreen Awards in 2001 for transportation programs, in 2004 for climate and energy initiatives, and in 2006 for waste reduction and recycling. MIT has received more GoGreen Awards since the program’s inception in 1999 than any other individual organization. The awards program was held at City Hall Annex, the city’s first green building, which achieved gold status under the U.S. Green Building Council Leadership in Energy and Environment Design (LEED) program.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT, NSTAR team up on energy-efficiency program]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/nstar-mit-0526.html</link>
<story_id>15370</story_id>
<featured>0</featured>
<description><![CDATA['MIT Efficiency Forward' aims to cut the Institute's electricity use by 15 percent over three years]]></description>
<postDate>Wed, 26 May 2010 14:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100525160825-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100525160825-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100525160825-1.jpg</fullURL>
</image>
<body><![CDATA[In an effort to dramatically cut energy use at one of the country’s premier academic and research institutions, NSTAR and MIT are teaming up to launch the single most aggressive efficiency project in NSTAR history. Dubbed “MIT Efficiency Forward,” the program has a goal of cutting electricity use by 15 percent over three years through innovative programs, substantial student, faculty, and staff engagement, and the piloting of new technologies and approaches at MIT. The long-term partnership is a first-of-its kind for both the Institute and the utility and establishes a new approach for sustainable solutions. In the three-year period, the target energy savings is 34 million kilowatt-hours; that’s equivalent to the amount of electricity used by more than 4,500 Massachusetts homes in a given year. <br /><br />“What we are launching with MIT is a bold new plan for confronting climate change and a proposal to officially establish energy efficiency as the ‘first fuel’ in Massachusetts,” said Tom May, NSTAR Chairman, President and CEO. “Aggressive goals require aggressive action, and MIT is demonstrating its leadership in campus sustainability once again. They are taking advantage of every energy-saving tool NSTAR has available and I’m confident the results will be a model — and an inspiration — for all other customers to follow.”<br /><br />Energy efficiency is one of several key strategies being implemented at MIT to demonstrate leading approaches for reducing greenhouse-gas emissions both locally and globally. The Institute has already begun to utilize combined heat and power generation, sustainable design and construction, and significant commuter-benefit programs, among other strategies. NSTAR predicts that over the three years, MIT Efficiency Forward will provide MIT with savings over the lifetime of the projects completed in excess of $50 million through a combination of sustainable new construction, major renovations, and both electric and gas incentive programs to promote new synergies. The company will work with MIT to conduct heating, ventilation and air conditioning (HVAC), electrical, and lab systems improvements, and lighting fixture and control upgrades, in addition to other steps. <br /><br />“MIT Efficiency Forward will capitalize on one of MIT’s core strengths: the passion of our faculty, staff and students to tackle the world’s most challenging problems,” said MIT President Susan Hockfield. “Through this exciting new program, right here on the MIT campus, we will pursue one of the major opportunities to reduce energy consumption: finding smart, sensible, economic approaches to energy efficiency. Our participation in the program signals that the solutions for today’s climate and energy challenges will come not only from our research laboratories and classrooms, but also from practice-based management innovations.”<br /><br />MIT Efficiency Forward will also have substantial student engagement facilitated by the program’s Student Advisory Group. Building on work coordinated by the MIT Energy Initiative’s (MITEI) Campus Energy Task Force using MIT’s campus as a living laboratory, MIT Efficiency Forward will offer students and faculty rich research, educational and learning opportunities. In particular, through close collaboration with the MITEI Energy Education Task Force, unique project-based coursework for freshman, sophomore and advanced student teams will be developed to help to advance the goals of MIT Efficiency Forward. An aggressive, sustained campaign to encourage the MIT community to reduce energy consumption will be a key component of the program, and students will help develop approaches to measure, monitor and verify those savings.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Energy answer: Blowing in the wind?]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/wind-economics-0525.html</link>
<story_id>15367</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers say wind power can make sense for utility companies, starting now]]></description>
<postDate>Tue, 25 May 2010 04:02:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100524143653-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100524143653-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100524143653-1.jpg</fullURL>
</image>
<body><![CDATA[When the federal government approved the Cape Wind project in April, allowing 130 power-generating turbines to be placed in the waters off Cape Cod, it gave a significant boost to the prospects of wind energy. The comparatively high costs of wind power, however, remain a problem. But in a study, MIT researchers have concluded that some of the price problems associated with wind power can be remedied right now, given a couple of changes to the electricity grid. <br /> <br />“Everyone knows advances in technology are critical for more widespread use of clean energy, but effective operations are also vital for profitability and can help us take advantage of current opportunities,” says Jarrod Goentzel, director of the MEng in Logistics (MLOG) program at MIT’s Center for Transportation and Logistics (CTL), who helped direct the study. “Obviously without good technology we won’t get there, but we will get there sooner by operating the technology in a more efficient way.”<br /> <br />A key insight of the study is that wind’s apparent drawbacks as a power source — it only blows intermittently, and in many places blows harder at night than during the day — could actually be used to the advantage of power companies, with one condition. If power grids were equipped with large storage batteries that are commercially available right now, placed near urban areas, they could accumulate energy via wind power during off-peak night hours, then discharge the saved power during peak afternoon hours (when people have their air-conditioning on during the summer, for instance). That would make economic sense for the power-grid operators, which pay higher rates to generators during peak hours, while keeping consumer prices intact.<br /> <br />“With existing battery technology and realistic costs, we wanted to see if it is possible to take advantage of market dynamics to make wind power profitable now,” says Goentzel. He and his colleagues combined information about leading-edge grid-scale batteries with two years of historical data on wind speeds, utility prices and consumer electricity use throughout New England. For power companies, Goentzel says, wind can work, but “it comes down to how you manage the battery: When you charge, when you discharge and where you locate it.” <br /> <br /><strong>Location, location, location</strong><br /> <br />The MIT study began as a piece of research in the 2008-09 academic year by two MLOG students, Prashant Saran and Clayton Siegert, whom Goentzel supervised. Now the paper, “Economic Analysis of Wind Plant and Battery Storage Operation using Supply Chain Management Techniques,” has been accepted for presentation at the July 2010 IEEE Power Engineering Society General Meeting, in Minneapolis.<br /> <br />In New England, retail electricity prices in February 2010 averaged 16.3 cents per kilowatt-hour (a standard industry measure), according to the Department of Energy. The Cape Wind project is slated to begin selling wholesale electricity to National Grid, a utility firm, at 20.7 cents per kilowatt-hour. <br /> <br />In general, however, the cost of wind energy depends on wind speed, location — onshore turbines generate cheaper power than offshore machines, due to installation expenses — and other factors like transmission costs. Nationally, according to the American Wind Energy Association, a trade group, wind costs a wholesale price of 4.8 cents per kilowatt-hour with wind speeds of around 16 miles per hour, and 2.6 cents per kilowatt-hour at about 21 miles per hour. (This factors in the federal government’s renewable energy production tax credit, worth 2.1 cents per kilowatt-hour.)<br /> <br />To calculate costs, the MIT team first received detailed data about current and next-generation products from officials at two companies that build large-scale modular batteries suitable for grid use (the firms asked for anonymity). Then, after scrutinizing the historical data, the researchers noticed something that could make wind power feasible: In all locations, electricity prices vary between peak and off-peak hours, but the spread is greater in heavily populated areas, like Boston, Providence or southern Connecticut. Yet because of civic politics, notes Siegert, “Wind plants are located further away from where the demand is.” People tend not to want windmills spoiling the view from their windows. <br /> <br />To turn wind power into affordable electricity, then, the key is to connect rural wind farms to power-storage devices near cities, rather than locating storage devices near wind farms. “If you put batteries in upstate Maine, yeah, you’re going to get lower prices at night and higher prices during the day, but the difference is not as extreme as in the area around Greenwich or Cos Cob, Connecticut,” observes Siegert. “So if you look strategically at where to place grid-scale batteries, there are huge arbitrage opportunities in some locations.” <br /> <br /><strong>Batteries not included (yet)</strong><br /> <br />To see if wind power would fit into a profitable power-delivery model, the researchers built a <a href="http://web.mit.edu/newsoffice/2010/exp-monte-carlo-0517.html" target="_blank">Monte Carlo simulation model </a>of the grid, plugged in a rich set of data on weather patterns and market prices, and then examined the expected profits. <br /> <br />The two types of large batteries in the model cost $144 million and $60 million, respectively. Given the current range of electricity prices, the researchers’ conclusion is that the second type of battery would pay back its costs after 14 years of summer-level use (when electricity consumption is higher) and 32 years of winter-level use, and would have an operating life of 30 years. <br /> <br />The operating profit, they found, increases sharply when grid batteries charge and discharge dynamically throughout the day depending on conditions. Other energy analysts have studied the battery concept while assuming operators would employ six-hour spans for charging at night and discharging during the day. But consumer use fluctuates more rapidly than that; an energy-delivery program with shorter charging and discharging periods would not only fit demand patterns more closely, but help extend battery life, too. Moreover, adds, Goentzel, “Any technological advances in batteries will only make the business case better.”<br /> <br />One additional policy qualification is needed to make the concept practical, adds Goentzel. Grid operators pay pumped hydro-power facility owners in order to have backup power capacity ready at all times. Applying the same concept to battery-stored power would give businesses incentive to invest in wind farms. “Installed capacity payments are important in making large-scale battery storage viable,” acknowledges Goentzel, “But it’s not some kind of special green energy subsidy, it would just require extending the current policy for pumped hydro to batteries.” <br /> <br />“Having additional energy-storage resources on the grid could potentially improve the economic viability of wind resources in any part of the country, assuming the economic viability of energy storage itself,” says Chris Namovicz, a long-term renewable-energy forecasting expert at the federal government’s Energy Information Administration (EIA), in response to e-mailed questions. <br /> <br />If large-scale batteries are a profitable investment for energy-delivery companies, then, and can be operated in a way that fits the characteristics of wind power, the final question is how much room there is for wind power to grow. The offshore areas of Massachusetts are the windiest in the state. On dry land, New England’s largest contiguous windy area is Eastern Maine. <br /> <br />Namovicz says the EIA projects that as much as 8,500 megawatts of wind energy — enough to power between 1.9 million and 2.6 million homes — is available in New England at economically viable prices. <br /> <br />The critical question the study has answered, Goentzel says, is that “certain operational strategies can help profitably deploy battery storage at scale without special subsidies. The concept is not limited to experimental projects, like putting a small battery on a wind-farm site.” <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Pooling MIT’s resources to ‘rethink’ water]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/water-workshop-0525.html</link>
<story_id>15368</story_id>
<featured>0</featured>
<description><![CDATA[Workshop assesses scope of water research currently underway at Institute, eyes work that lies ahead.]]></description>
<postDate>Tue, 25 May 2010 04:01:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100524145852-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100524145852-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100524145852-1.jpg</fullURL>
</image>
<body><![CDATA[In early May, a ruptured water pipe caused a breach in Boston’s water supply that forced 2 million residents to boil water for several days. That inconvenience should serve as a “striking reminder” of the daily reality for more than one billion people across the world who don’t have access to clean water, President Susan Hockfield told about 280 members of the MIT community at a workshop on Friday. <br /><br />“We have an obligation — a responsibility — to act,” Hockfield said at the workshop, titled “Rethinking Water: A Critical Resource.” The event was intended to provide a glimpse of the range of work currently being done at MIT to address the scarcity of fresh water worldwide, and how MIT must help to address that crisis by designing better water systems and policies to be adopted across different geographic and cultural contexts.<br /><br />With more than 50 faculty members from each of MIT’s five schools working on issues related to water, the Institute already has a “running start” on water-related research and education, Hockfield said.<br /><br />The workshop came just as MIT’s Environmental Research Council (ERC) <a href="http://web.mit.edu/erc-report/ " target="_blank">issued a report</a> outlining six research areas where MIT faculty and students can build on existing strength in multiple disciplines to advance solutions to environmental challenges of global significance. One of those areas is water, a resource described in the report as “arguably more vital than oil, but routinely squandered.”<br /><br />Although MIT faculty and students are already working on a range of water-related issues, such as studying the exchange between the oceans and the atmosphere, developing cost-effective desalinization technologies and creating and implementing innovative water policy mechanisms, much work remains. Hockfield urged workshop participants to use the ERC report as a critical framework to guide continued collaboration across the Institute on research and teaching related to the environment.<br /><br />The morning portion of the workshop was geared toward discussing what MIT is currently doing to address water issues. Faculty members from each of the five schools presented examples of research projects related to water — everything from how the resource is affected by patterns of evaporation and precipitation to how it travels through soil to how it is considered to be a human health hazard in countries like Bangladesh because it carries toxins like arsenic.. During the afternoon session, the workshop featured several panel discussions that focused on MIT’s long-term goals for tacking water issues, such as scaling water solutions.<br /><br />“This is only a start,” said James Wescoat, the Aga Khan Professor of Architecture in MIT’s Department of Architecture, of the workshop, which he helped organize and hoped would be used as a platform to guide future events about water issues at MIT.<br /><br />This fall, the ERC is expected to complete a detailed implementation plan about building a strong environmental initiative at MIT.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[A good many years before Goodyear]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/mayaball-0524.html</link>
<story_id>15361</story_id>
<featured>0</featured>
<description><![CDATA[Mesoamerican people perfected details of rubber processing more than 3,000 years ago, new MIT study suggests.]]></description>
<postDate>Mon, 24 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100521174715-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100521174715-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100521174715-1.jpg</fullURL>
<imageCaption><![CDATA[The Aztec god, Xiuhtecuhtli, as one of the nine Lords of the Night, offers up rubber balls in this drawing.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='172'>http://web.mit.edu/newsoffice/images/article_images/20100521174716-2.jpg</fullURL>
<imageCaption><![CDATA[An image of a ball court at Xochicalco, a pre-Columbian archaeological site in the western part of the Mexican state of Morelos.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Spanish explorers encountering an advanced civilization in Mesoamerica in the 16th century had plenty of things to be astonished about, but one type of object in particular was unlike anything they had ever seen before: rubber balls. No such stretchy, bouncy material existed in the Old World, and they had to struggle to find words to describe it.<br /><br />New research from MIT indicates that not only did these pre-Columbian peoples know how to process the sap of the local rubber trees along with juice from a vine to make rubber, but they had perfected a system of chemical processing that could fine-tune the properties of the rubber depending on its intended use. For the soles of their sandals, they made a strong, wear-resistant version. For the rubber balls used in the games that were a central part of their religious ceremonies, they processed it for maximum bounciness. And for rubber bands and adhesives used for ornamental wear and for attaching blades to shafts, they produced rubber optimized for resilience and strength.<br /><br />All of these, according to the research by Professor Dorothy Hosler and Technical Instructor Michael Tarkanian of MIT’s Department of Materials Science and Engineering, were most likely achieved by varying the proportions of the two basic ingredients, latex from rubber trees and juice from morning-glory vines, which were cooked together. A paper describing the findings will be published soon in the journal <em>Latin American Antiquity</em>. <br /><br />The research builds on a paper that Hosler, Tarkanian and Sandra Burkett, then an assistant professor at MIT, published in <em>Science</em> in 1999 that showed for the first time that the Mesoamerican people could have used the combination of two ingredients to produce rubber. The new work, which draws on a combination of laboratory experiments, recovered artifacts and the descriptions left by early explorers, demonstrates how varying the formula could fine-tune the rubber’s properties.<br /><br />Although Hosler and Tarkanian’s research demonstrates that the Mesoamericans had the raw materials and the basic knowledge to make these different formulations, proving that’s what they actually did would require further evidence, either from contemporaneous accounts or from chemical analysis of samples used for different purposes.<br /><strong><br />Long before Goodyear</strong><br /><br />Charles Goodyear is credited with having invented vulcanization — a chemical process for converting rubber or related polymers into more durable materials — while experimenting with rubber and sulfur in the mid-19th century. But it has long been known that the Aztecs, Olmecs and Maya — the civilizations that, over a span of more than three millennia, dominated the region that is now Mexico and parts of Central America — were adept at making rubber, and that the material was used to produce the large, heavy balls used for the ceremonial games played on stone-walled ball courts. A few such balls have been found in archeological digs in the region — the oldest dating back to 1600 B.C., or more than 3,000 years before Goodyear’s contributions — and though they have become hard and brittle with age, their nature is unmistakable. “They were really spectacular, really enormous,” Hosler says of the Mesoamerican rubber balls, which ranged in size from a few inches to a foot across — the size of a beach ball. <br /><br />Until the new research, nobody had shown that it was possible to obtain the different properties needed for other uses of rubber, simply by varying the recipe’s proportions. Unlike the rubber balls, Mesoamerican rubber-soled sandals have never been found. But they are described in the diaries of the Spanish explorers and missionaries, and their existence is clear from linguistic evidence: The Aztecs used a compound word that clearly blends the words for “rubber” and “sandals.”<br /><br />The ancient rubber material that has survived tends to be so degraded that it can’t be tested for its mechanical properties. So Tarkanian and Hosler set up their own processing facility at MIT, using raw materials collected in field trips to Mexico. They made batches of rubber with varying proportions of the two plant substances, and then subjected the product to a suite of tests to measure wear resistance, elasticity, toughness and other properties.<br /><br />Sure enough, varying the proportions produced different properties. A 50-50 blend of the latex and morning glory produced maximum elasticity, or bounciness, perfect for the rubber balls. Rubber used as an adhesive or for joining other materials (such as ceramic and wood) needs different properties — strength and damping ability — and for that, pure latex seems to work best. For sandals, where wear resistance is the most important quality, a three-to-one mix of latex to morning glory provides the most durable material.<br /><br />The Mesoamericans had plenty of time to work out these properties through trial and error. By the time the Spanish arrived, Tarkanian says, “there was a large rubber industry” in the region, producing 16,000 rubber balls each year, and large numbers of rubber statues, sandals, bands and other products. Most of those were produced in villages in outlying areas, and were shipped to the capital city as a form of tax payment.<br /><br />Hosler has also studied these ancient civilizations’ advanced work in metallurgy, and suggests that they were likely also accomplished practitioners of other kinds of materials processing that have yet to be studied, such as formulating mortars, plasters and paints.<br /><br />Frances Berdan, professor of anthropology at California State University at San Bernardino, says Hosler and Tarkanian’s latest work has implications well beyond rubber. “There are other areas of production where the pre-Hispanic peoples cleverly combined materials to achieve enhanced products. The Tarkanian-Hosler research on ancient rubber should have the effect of directing our attention to the methods used by these peoples, and recognizing that they developed sophisticated answers to their everyday (and also not-so everyday) problems.”<br /><br />John McCloy, a senior research scientist at Pacific Northwest National Laboratory, says that “Tarkanian and Hosler have compiled a compelling case that ancient Mesoamerican peoples were the first polymer scientists, exerting substantial control over the mechanical properties of rubber for various applications.” He adds that “what remains to be done is to find archaeological evidence of rubber footwear in ancient Mesoamerica, and to study the production methods for Mesoamerican rubber as an adhesive and as footwear. It would also be interesting to do chemical analyses on rubber balls, adhesive rubbers, and sandals (if they are found) to see if quantification of morning-glory additives corroborates the laboratory study of the mechanical properties.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[More precise food-allergy diagnoses]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/food-allergies-0521.html</link>
<story_id>15358</story_id>
<featured>0</featured>
<description><![CDATA[It turns out that many people mistakenly think they have food allergies. A new technology aims to erase all doubt.]]></description>
<postDate>Fri, 21 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100520173241-0.png</thumbURL>
<smallURL width='140' height='157'>http://web.mit.edu/newsoffice/images/article_images/w140/20100520173241-0.jpg</smallURL>
<fullURL width='368' height='414'>http://web.mit.edu/newsoffice/images/article_images/20100520173241-0.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='309'>http://web.mit.edu/newsoffice/images/article_images/20100520160944-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Qing Han and Christopher Love]]></imageCredits>
<imageCaption><![CDATA[In this image, each block represents a single cell. Each cell’s type can be distinguished, and its cytokine secretion can be measured. (The image is a composite of actual data and computational models.)]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[About 30 percent of Americans believe they have food allergies. However, the actual number is far smaller, closer to 5 percent, according to a recent study commissioned by the National Institute of Allergy and Infectious Diseases (NIAID). That’s due in large part to the unreliability of the skin test that doctors commonly use to test for food allergies.<br /><br />MIT chemical engineer Christopher Love believes he has a better way to diagnose such allergies. His new technology, described in the June 7 issue of the journal <em>Lab on a Chip</em>, can analyze individual immune cells taken from patients, allowing for precise measurement of the cells’ response to allergens such as milk and peanuts. <br /><br />Using this technology, doctors could one day diagnose food allergies with a simple blood test that would be faster and more reliable than current tests, says Love, an assistant professor of chemical engineering. “With a large number of diagnoses, it’s ambiguous,” he says. “A lot of times it’s almost circumstantial whether you’re allergic to one thing or another.”<br /><strong><br />Measuring single cells</strong><br /><br />In the United States, 6 to 8 percent of children under four, and 4 percent of people five or older, have at least one food allergy, according to the NIAID. Milk, peanuts, eggs and soy are among the most common allergens.<br /><br />Food allergies occur when the body’s immune system mistakes a protein in food for something harmful. This triggers an allergic response that can include rashes, hives, difficulty breathing or gastrointestinal distress. Some allergies can provoke life-threatening anaphylactic shock, which requires immediate treatment.<br /><br />Patients suspected of having food allergies usually undergo a skin test, which involves placing small quantities of potential allergens under the skin of the patient’s arm. If the patient’s blood has antibodies specific to that allergen, immune cells will release histamines that cause itching and redness in the spot where the allergen was placed.<br /><br />Doctors can also perform blood tests that directly measure the presence of particular antibodies in the patients’ blood. However, one drawback to both of these tests is that the presence of antibodies to a particular allergen does not necessarily mean that the patient is allergic to that substance, leading to false positive results.<br /><br />Love’s new technology, developed with funding from the Deshpande Center for Technological Innovation, the Dana Foundation and the NIAID, takes a different approach. Instead of detecting antibodies, his system screens the patient’s immune cells for small proteins known as cytokines. Immune cells such as T cells produce cytokines when an allergic response is initiated, attracting other cells to join in the response. <br /><br />To perform the test, blood must be drawn from the patient, and white blood cells (which include T cells) are isolated from the sample.<br /><br />The cells are exposed to a potential allergen and then placed into about 100,000 individual wells arranged in a lattice pattern on a soft rubber surface. Using a technique known as microengraving, the researchers make “prints” of the cytokines produced by each cell onto the surface of a glass slide. The amount of cytokine secreted by each individual cell can be precisely measured.<br /><br />For food-allergy testing, the cytokines of most interest are IL4, IL5 and IL9.<br /><strong><br />Testing allergic reactions</strong><br /><br />The ability to measure individual immune cells’ cytokine production represents “a great advance,” says Amal Assa’ad, professor of pediatric immunology and allergy at the University of Cincinnati College of Medicine, who was not involved in the research. She adds, however, that clinical studies will be needed to demonstrate the ability to accurately diagnose food allergies. “Any test will have to be tested on multiple patients to see that it truly correlates with clinical allergy,” she says.<br /><br />The “gold standard” for diagnosing a food allergy is to see what happens when the patient is given the food in question (in a controlled setting, to ensure safety), but that is not often done outside of allergy research clinics, says Assa’ad.<br /><br />Love is now working with Dale Umetsu, professor of pediatric immunology at Children’s Hospital Boston, on a project they hope will pinpoint the relationship between cytokine activity and allergic reactions. In that study, children with milk allergies are being given small amounts of milk to desensitize their immune systems to the milk. Using the new technology, the team is tracking how the responses of the patients’ cells change as the patients undergo treatment. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Environmental Research Council issues report]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/environment-init-0520.html</link>
<story_id>15357</story_id>
<featured>0</featured>
<description><![CDATA[Council is asked to develop plan to establish MIT Environmental Initiative]]></description>
<postDate>Thu, 20 May 2010 17:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100520091154-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100520091154-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100520091154-1.jpg</fullURL>
</image>
<body><![CDATA[<em>Provost L. Rafael Reif issued the following letter to the community on Thursday, May 20.<br /></em><br />To the members of the MIT community,<br /><br />In September 2008, I charged the Environmental Research Council (ERC), to support the creation of an Institute-wide Environmental Initiative in keeping with the recommendations of the <a href="http://web.mit.edu/provost/reports/environ.assessment.pdf" target="_blank">May 2008 Zuber report</a>.<br /><br />Chaired by Professor Dara Entekhabi, the ERC has produced a vision for new environmental research and educational opportunities at MIT. <a href="http://web.mit.edu/erc-report/" target="_blank">The report</a> that I share today outlines a number of research areas where MIT faculty and students can build on existing strength in multiple disciplines to advance solutions to environmental challenges of global significance. Today, we take the next steps towards launching a robust and ambitious environmental initiative.<br /><br /><strong>Next step: a plan for implementation</strong><br /><br />I am now charging an expanded ERC, under the continuing leadership of Professor Entekhabi, to take up the next challenge: designing an implementation plan, such as that produced for the MIT Energy Initiative (MITEI) in 2006.  Building on the ERC report released today and the Zuber report that preceded it, the plan should:<br /> 
<ul>
<li>Provide a detailed blueprint for building a strong environmental initiative, with priorities set for staged development over several years</li>
<li> Have broad-based faculty input</li>
<li> Complement and identify synergies with MITEI and other relevant MIT laboratories, centers and programs</li>
<li> Establish benchmarks for progress at three-year and five-year milestones</li>
</ul>
The expanded ERC will consist of the following faculty:<br /> 
<ul>
<li> Dara Entekhabi (CEE) chair</li>
<li> Martin Z. Bazant (ChemE)</li>
<li> Sallie (Penny) Chisholm (CEE)</li>
<li> Catherine L. Drennan (Chem)</li>
<li> Michael Greenstone (Econ)</li>
<li> Jeffrey C. Grossman (DMSE)</li>
<li> Judith Layzer (DUSP)</li>
<li> John H. Lienhard (MechE)</li>
<li> John C. Marshall (EAPS)</li>
<li> Ronald G. Prinn (EAPS)</li>
<li> Harriet Ritvo (Hist)</li>
<li> Leona D. Samson (BE and Biology)</li>
<li> John Sterman (SSM)</li>
<li> J. Phillip Thompson (DUSP)</li>
<li> James Wescoat (Arch)</li>
<li> Richard A. Young (Biology/Whitehead)</li>
</ul>
The implementation report should be completed by October 2010 so that, following an opportunity for community comment, we can begin implementation in January 2011.<br /><br /><strong>Creating an Energy and Environment Board</strong><br /><br />In addition, to help my office define specific steps for establishing a strong and effective campus-wide environmental initiative, I am appointing a senior Energy and Environmental Board. The Board will advise me and the ERC on implementation and on coordination with other campus efforts, most especially <a href="http://web.mit.edu/mitei" target="_blank">MITEI</a>. Consisting of faculty from all five MIT schools, this Board includes the ERC chair and several people in key administrative positions whose cooperation will be essential to the success of our environmental initiative. The members will be: <br /> 
<ul>
<li> Ernest J Moniz (Physics), chair</li>
<li> Andrew Whittle (CEE), vice-chair</li>
<li> Dara Entekhabi (CEE)</li>
<li> Amy Glasmeier (DUSP)</li>
<li> Richard M. Locke (SSM)</li>
<li> Susan S. Silbey (Anthropology)</li>
<li> Maria Zuber (EAPS)</li>
</ul>
<hr />
The disaster unfolding in the Gulf of Mexico offers a searing reminder of the vast scale and significance of the world's environmental challenges and of their inextricable ties to questions of energy, economics and policy. We are grateful to the faculty members who have agreed to serve on the ERC and on the Energy and Environmental Board. As they prepare the Institute to confront the full array of global environmental challenges, I look forward to working with them to accelerate and magnify MIT's contributions.<br /><br />Sincerely,<br />L. Rafael Reif<br />Provost<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Charles Stewart reads the tea leaves]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/3q-stewart-0520.html</link>
<story_id>15348</story_id>
<featured>0</featured>
<description><![CDATA[The head of MIT’s Department of Political Science analyzes Tuesday’s election results and sees reason for concern for Obama and the Democrats]]></description>
<postDate>Thu, 20 May 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100519161040-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100519161040-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100519161040-0.jpg</fullURL>
<imageCaption><![CDATA[Charles Stewart, the head of MIT’s Department of Political Science]]></imageCaption>
</image>
<body><![CDATA[<em>Tuesday’s slate of elections yielded more bad news for incumbents: Pennsylvania’s 30-year senator, Arlen Specter, was ousted in a Democratic primary by Rep. Joe Sestak, while centrist Arkansas Democrat Blanche Lincoln was forced into a runoff by a challenger from the left, state Lieutenant Governor Bill Halter. Meanwhile, in Kentucky, Rand Paul, a Tea Party-approved candidate, routed the GOP establishment’s choice, Trey Grayson. With conservatives and progressives alike defeating moderate rivals, </em>MIT News <em>talked to Charles Stewart, the head of MIT’s Department of Political Science and an expert on elections and voting, to find out his read on the results.</em><br /><br /><strong>Q.</strong> Many media stories about Tuesday’s elections have emphasized that the results represent a wave of “anti-incumbent sentiment.” Yet the candidates who won this week are all over the map, ideologically. So to what extent is it useful to think of these results as being some kind of larger anti-incumbent statement? <br /><br /><strong>A. </strong>While there is some evidence of that, one of the big stories that’s yet to be entirely told is that a lot of the anti-incumbent vote is nonetheless picked up by incumbents themselves running against other incumbents. You do have Rand Paul in Kentucky, a guy who’s not currently a politician, but note that in a lot of these cases what we’re observing is career politicians battling against other incumbents. Politics is still a game played by a professional class. The anti-incumbent sentiment hasn’t translated uniformly into anti-politician sentiment yet, which I think will temper outcomes, and also potentially help to foster dissatisfaction down the road, because most of the people winning are not going to be anti-establishment.<br /><br />I think the different ideological effects are very important. What we’re seeing on the left and the right is dissatisfaction coming from the most engaged, interested, and extreme people in the political spectrum. We’re certainly seeing that with Paul in Kentucky, and also in Arkansas, where Blanche Lincoln wasn’t left enough when it came to health-care reform. And perhaps that’s reflected in Pennsylvania, where the Democratic primary was a complicated thing, but buried in there is the question: Do Democrats want a moderate who might not be dedicated to the party, or do they want somebody who’s been a tried-and-true liberal? So we’re seeing a further pushing of elected politicians to the extremes of the political spectrum. <br /><br /><strong>Q.</strong> Given that the Kentucky senate primary for the GOP was a blowout, where do moderate Republicans go from here? Are they going to keep backing more extreme candidates, look for third-party or independent candidates or seriously consider Democrats?<br /><br /><strong>A.</strong> In every state it’s going to be slightly different, but it could be that the moderate Republicans have already left party in Kentucky, in which case they’re independents, and they’re just going to chose between the two candidates — there or anywhere else. One of the consequences of the 2008 election was a continuing of an ideological sorting of the two parties. There may just not be moderate Republicans any more, or there may just be fewer of them. So then the question becomes, where does the middle of the road go? And what research tells us is that in bad times you punish the party of the incumbent. And I’ve seen nothing to suggest that’s not the big story.<br /><br /><strong>Q.</strong> Clearly the economy, and the employment picture specifically, are going to be critical factors in November. What else is going to be critical in this year’s election?<br /><br /><strong>A.</strong> I think the political scientist in me just has to report and hammer on a non-obvious, enormously robust finding in our discipline, which is that in congressional races, the most important determinant of the outcome in races that involve incumbents — and that’s mostly what we have — is that if you have a challenger who has previously held office, then that incumbent is more likely to lose in November. Then you ask: When are those challengers chosen? Well, they’re chosen right now. So the thing to watch is, how good are the challengers, especially on the Republican side? And my informal reading of news reports suggests there are a lot of high-quality Republicans challengers who have prior electoral experience. And they will run really hard-fought campaigns and be well-funded because donors will regard them as credible candidates. In a sense it doesn’t matter what economy is going to be like specifically in November: The results are being hard-wired right now in terms of the choice of challengers against incumbents. You can predict outcomes in mid-term elections as a function of the economy in November, or the economy April, and the stronger predictor is the economy in April. And the mechanism is the choice of challengers to incumbents. <br /><br />What you might be able to get on the margin in November is that if the economy is on an uptick and Obama does become a little more popular, there may be a little bit of a national swing back to the Democrats. But that’s going to be swamped by the appearance of really good Republican challengers against Democratic incumbents in swing districts. <br /><br />Additionally, voters are enormously short-sighted. We’re seeing in public-opinion polls right now that large pluralities of voters are blaming the current recession on Obama. They’ve forgotten we had a president named Bush. They’ve forgotten that the downturn happened in the fall of 2008. All the bad things happening right now are being heaped on the Obama administration. So you have strong challengers coming out and you have the myopia of the electorate, and both of those are immovable objects, I think, electorally.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Gesture-based computing on the cheap]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/gesture-computing-0520.html</link>
<story_id>15351</story_id>
<featured>0</featured>
<description><![CDATA[With a single piece of inexpensive hardware — a multicolored glove — MIT researchers are making Minority Report-style interfaces more accessible.]]></description>
<postDate>Thu, 20 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100519153226-1.png</thumbURL>
<smallURL width='140' height='108'>http://web.mit.edu/newsoffice/images/article_images/w140/20100519153226-1.jpg</smallURL>
<fullURL width='368' height='285'>http://web.mit.edu/newsoffice/images/article_images/20100519153226-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[The hardware for a new gesture-based computing system consists of nothing more than an ordinary webcam and a pair of brightly colored lycra gloves.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100519153227-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[CSAIL graduate student Robert Wang shows off the new system, which he developed together with associate professor of electrical engineering and computer science Jovan Popovi&#263;.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Ever since Steven Spielberg’s 2002 sci-fi movie <em>Minority Report</em>, in which a black-clad Tom Cruise stands in front of a transparent screen manipulating a host of video images simply by waving his hands, the idea of gesture-based computer interfaces has captured the imagination of technophiles. Academic and industry labs have developed a host of prototype gesture interfaces, ranging from room-sized systems with multiple cameras to detectors <a href="http://web.mit.edu/newsoffice/2009/gestural-computing-1211.html" target="_blank">built into laptops’ screens</a>. But MIT researchers have developed a system that could make gestural interfaces much more practical. Aside from a standard webcam, like those found in many new computers, the system uses only a single piece of hardware: a multicolored Lycra glove that could be manufactured for about a dollar.<br /><br />Other prototypes of low-cost gestural interfaces have used reflective or colored tape attached to the fingertips, but “that’s 2-D information,” says Robert Wang, a graduate student in the Computer Science and Artificial Intelligence Laboratory who developed the new system together with Jovan Popovi?, an associate professor of electrical engineering and computer science. “You’re only getting the fingertips; you don’t even know which fingertip [the tape] is corresponding to.” Wang and Popovi?’s system, by contrast, can translate gestures made with a gloved hand into the corresponding gestures of a 3-D model of the hand on screen, with almost no lag time. “This actually gets the 3-D configuration of your hand and your fingers,” Wang says. “We get how your fingers are flexing.”<br /><br />The most obvious application of the technology, Wang says, would be in video games: Gamers navigating a virtual world could pick up and wield objects simply by using hand gestures. But Wang also imagines that engineers and designers could use the system to more easily and intuitively manipulate 3-D models of commercial products or large civic structures. <br /><br /> 










<em>Robert Wang demonstrates the speed and precision with which the system can gauge hand position in three dimensions — including the flexing of individual fingers — as well as a possible application in mechanical engineering.<br /><strong>Video:</strong> Robert Y. Wang/Jovan Popovi?</em><br /><br /> <strong>Patchwork approach</strong><br /><br />The glove went through a series of designs, with dots and patches of different shapes and colors, but the current version is covered with 20 irregularly shaped patches that use 10 different colors. The number of colors had to be restricted so that the system could reliably distinguish the colors from each other, and from those of background objects, under a range of different lighting conditions. The arrangement and shapes of the patches was chosen so that the front and back of the hand would be distinct but also so that collisions of similar-colored patches would be rare. For instance, Wang explains, the colors on the tips of the fingers could be repeated on the back of the hand, but not on the front, since the fingers would frequently be flexing and closing in front of the palm.<br /><br />Technically, the other key to the system is a new algorithm for rapidly looking up visual data in a database, which Wang says was inspired by the recent work of Antonio Torralba, the Esther and Harold E. Edgerton Associate Professor of Electrical Engineering and Computer Science in MIT’s Department of Electrical Engineering and Computer Science and a member of CSAIL. Once a webcam has captured an image of the glove, Wang’s software crops out the background, so that the glove alone is superimposed upon a white background. Then the software drastically reduces the resolution of the cropped image, to only 40 pixels by 40 pixels. Finally, it searches through a database containing myriad 40-by-40 digital models of a hand, clad in the distinctive glove, in a range of different positions. Once it’s found a match, it simply looks up the corresponding hand position. Since the system doesn’t have to calculate the relative positions of the fingers, palm, and back of the hand on the fly, it’s able to provide an answer in a fraction of a second.<br /><br />Of course, a database of 40-by-40 color images takes up a large amount of memory — several hundred megabytes, Wang says. But today, a run-of-the-mill desktop computer has four gigabytes — or 4,000 megabytes — of high-speed RAM memory. And that number is only going to increase, Wang says.<br /><br /><strong>Changing the game</strong><br /><br />“People have tried to do hand tracking in the past,” says Paul Kry, an assistant professor at the McGill University School of Computer Science. “It’s a horribly complex problem. I can’t say that there’s any work in purely vision-based hand tracking that stands out as being successful, although many people have tried. It’s sort of changing the game a bit to say, ‘Hey, okay, I’ll just add a little bit of information’” — the color of the patches — “‘and I can go a lot farther than these purely vision-based techniques.’” Kry particularly likes the ease with which Wang and Popovi?’s system can be calibrated to new users. Since the glove is made from stretchy Lycra, it can change size significantly from one user to the next; but in order to gauge the glove’s distance from the camera, the system has to have a good sense of its size. To calibrate the system, the user simply places an 8.5-by-11-inch piece of paper on a flat surface in front of the webcam, presses his or her hand against it, and in about three seconds, the system is calibrated.<br /><br />Wang initially presented the glove-tracking system at last year’s Siggraph, the premier conference on computer graphics. But at the time, he says, the system took nearly a half-hour to calibrate, and it didn’t work nearly as well in environments with a lot of light. Now that the glove tracking is working well, however, he’s expanding on the idea, with the design of similarly patterned shirts that can be used to capture information about whole-body motion. Such systems are already commonly used to evaluate athletes’ form or to convert actors’ live performances into digital animations, but a system based on Wang and Popovi?’s technique could prove dramatically cheaper and easier to use.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Now hear this]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/hearing-aid-0520.html</link>
<story_id>15346</story_id>
<featured>0</featured>
<description><![CDATA[3-D imaging technology could lead to hearing aids that fit — and thus function — better than current models.]]></description>
<postDate>Thu, 20 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100518151106-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100518151106-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100518151106-1.jpg</fullURL>
<imageCaption><![CDATA[Hearing aids work best when there is a tight seal between the device and the wearer’s ear canal.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='200' height='200'>http://web.mit.edu/newsoffice/images/article_images/20100518151106-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Federico Frigerio]]></imageCredits>
<imageCaption><![CDATA[This three-dimensional scan of the ear canal, taken with new imaging technology developed at MIT, could be used to manufacture better fitting hearing aids.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[About 36 million Americans suffer from some type of hearing loss. However, only one in five who could benefit from a hearing aid actually wears one, according to the National Institute on Deafness and other Communication Disorders. MIT engineers believe that number could be boosted if there were a better way to fit hearing aids to the wearers’ ears.<br /><br />Getting useful sound amplification from a hearing aid depends on a tight fit between hearing aid and ear canal, but the current method of modeling patients’ ears is messy and not always accurate, potentially leading to a device that fits poorly and offers little benefit.<br /><br />“A lot of people with hearing aids are likely walking around with hearing aids that don’t fit, because they don’t know what they’re supposed to feel like,” says Douglas Hart, MIT professor of mechanical engineering.<br /><br />Hart has patented a new way of scanning the ear canal with 3-D imaging technology — a process that is much faster, easier and more accurate than the plaster-mold technique. He plans to market the technology to hearing-aid manufacturers first, but believes it could also be useful to build fitted earphones for MP3 music players, or custom-fit earplugs for military personnel and other people who work in noisy environments. <br /><br /><strong>3-D scanning</strong><br /><br />The new technology is similar to a recently commercialized 3-D scanning system that Hart developed for dentistry, designed to replace the silicone molds traditionally used to make impressions for dental crowns and bridges. While Hart was working on that imaging system, hearing-aid manufacturers approached him to see what he could do to improve their fitting process.<br /><br />Getting a precise 3-D scan of the ear canal is the “Holy Grail of the hearing aid industry,” says Scott Witt, head of research and development for hearing-aid manufacturer Phonak. “Taking these impressions is still the messiest, least exact part of the process,” he says.<br /><br />Patients who need a hearing aid usually have to spend about an hour with an audiologist, who fills the patient’s ear canal with a gooey silicone substance. After about 15 minutes, the gel hardens into a mold that is removed from the ear and shipped to a hearing-aid manufacturer, who scans the mold and builds a custom-fit hearing aid using a 3-D printer.<br /><br />With this method, it can be difficult to achieve a tight seal between the hearing aid and the patient’s ear canal. A tight seal is necessary to prevent feedback between the microphone and receiver, which can produce squealing sounds annoying to the wearer and anyone standing nearby.<br /><br />With the new MIT system, a very stretchy, balloon-like membrane is inserted into the ear canal and inflated to take the shape of the canal. The membrane is filled with a fluorescent dye that can be imaged with a tiny fiber-optic camera inside the balloon. Scanning the canal takes only a few seconds, and the entire fitting process takes only a minute or two.<br /><br />Witt believes the MIT scanner has more potential than any other proposed imaging system he has seen in the past several years. “What really interested me is, they say they can determine the physical properties of the ear canal, such as how soft the tissue is,” he says.<br /><br />Because the camera captures 3-D images so quickly, it can measure how much the surface of the ear canal deforms when the pressure changes, or how the canal shape changes when the wearer chews or talks. That could help hearing-aid manufacturers design devices that keep their tight seal in those situations.<br /><br />The higher accuracy of digital scans could also eliminate the need for repeated impressions. “So many times we get impressions and have to go back (to the audiologist) and say, ‘We can’t really use this,’” says Witt. <br /><br />The Deshpande Center for Technological Innovation funded the development of the new technology, which Hart described in a 2004 article in the journal <em>Applied Optics</em>. He patented the system in January and has founded a company in the hope of bringing the innovation to market. <br /><br />The researchers have built a prototype scanner to demonstrate the proof of concept, and are now working on a handheld version of the device. Once it’s ready, they plan to do a study comparing the fit of hearing aids built with the new scanner to that of traditional hearing aids.<br /><br />The new technology could be seamlessly integrated into existing manufacturing practices, says Witt. “We could do it right now. The rest of our manufacturing process is set up to receive digital scans,” he says.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Rivest wins faculty’s Killian Award]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/rivest-wins-facultys-killian-award.html</link>
<story_id>15355</story_id>
<featured>0</featured>
<description><![CDATA[MIT encryption pioneer recognized for ‘extraordinary’ contributions in computer science]]></description>
<postDate>Wed, 19 May 2010 21:30:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100519154717-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100519154717-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100519154717-1.jpg</fullURL>
<imageCaption><![CDATA[Ronald L. Rivest, the Andrew and Erna Viterbi Professor in MIT's Department of Electrical Engineering and Computer Science (EECS), is MIT’s James R. Killian Jr. Faculty Achievement Award winner for 2010-2011.]]></imageCaption>
</image>
<body><![CDATA[Ronald L. Rivest, a professor of electrical engineering and computer science who helped develop one of the world's most widely used Internet security systems, is MIT’s James R. Killian Jr. Faculty Achievement Award winner for 2010-2011.<br /><br />The award was announced at the faculty meeting on Wednesday, May 19. Established in 1971 as a tribute to MIT's 10th president, the Killian Award recognizes extraordinary professional accomplishment by an MIT faculty member. The winner is asked to deliver a lecture in the spring term.<br /><br />Rivest, the Andrew and Erna Viterbi Professor in MIT's Department of Electrical Engineering and Computer Science (EECS), is known for his pioneering work in the field of cryptography, computer and network security. <br /><br />“He is one of the founding fathers of modern cryptography, especially of public key cryptography and digital signature methods,” said Alan Oppenheim, the Ford Professor of Engineering in EECS and chair of the Killian Award selection committee, reading from the award citation. “It was the work of Professor Rivest and his colleagues Leonard Adelman and Adi Shamir that led to the design of a public key system, now known universally as the RSA system after its inventors, that was robust to sophisticated attack. The RSA code is a wonderful example of elegant and abstract theory eventually having immense practical impact.”<br /><br />Public-key cryptography is a technology that allows users to communicate securely and secretly over an insecure channel without having to agree upon a shared secret key beforehand. By utilizing mathematical functions that are easy to compute but difficult to invert, messages can be encrypted and sent over a public channel with digital signatures attached, but only the intended recipient has the information to decrypt the message. While the code for RSA is deceptively simple, it has not been broken in the more than four decades since its invention and plays a critical role in the widespread use and success of the Internet and Internet-based commerce.<br /><br />“I was very surprised and feel very honored to receive this award, which is certainly one of the most prestigious that MIT offers,” said Rivest. “The previous awardees form a tremendously impressive group. I hope that I can inspire others, as they have inspired many.”<br /><br />A native of Niskayuna, N.Y., Rivest attended Yale University, where he earned a BS in mathematics in 1969. After receiving his PhD in computer science from Stanford in 1974, he accepted an offer to join the faculty at MIT. It was at MIT where he met Adleman and Shamir, who would become his partners in solving the puzzle of public-key cryptography. The team developed its system in 1977 and founded RSA Data Security in 1983. RSA was acquired in 1996 by Security Dynamics, which in turn was acquired by EMC in 2006.<br /><br />As the active founder of two of the most successful Internet security companies, RSA Security and VeriSign Inc., Rivest’s contributions to the impact of RSA have moved from deep fundamental inquiry to practical development and technology transfer. As noted in one of the Killian Award nomination letters, “Ron has the cultural background required to perceive the larger influence of scientific innovation, and the moral strength, energy and eloquence necessary to drive it towards its proper use.”<br /><br />In addition to his work in cryptography and security, Rivest has made important contributions in many other areas of computer science, including computer-aided design of integrated circuits, data structures and computer algorithms. He is acknowledged as an early contributor to the field of machine learning.<br /><br />Rivest is also known as a dedicated mentor and educator. His textbook Introduction to Algorithms (MIT Press and McGraw-Hill, 1990), co-authored with Thomas Cormen and Charles Leiserson, his fellow MIT professors of computer science, grew out of his undergraduate and graduate courses on computer algorithms. It is currently the second-most-cited reference in all of computer science.<br /><br />Among Rivest’s many contributions at MIT have been his guidance, wisdom and leadership as co-chair of the committee for the re-organization of the Artificial Intelligence Laboratory and the Laboratory for Computer Science. The new combined laboratory, CSAIL, is now the largest laboratory on the MIT campus. Rivest is a member of the lab’s Theory of Computation Group and a founder of its Cryptography and Information Security Group.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: The Carnot Limit]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/explained-carnot-0519.html</link>
<story_id>15347</story_id>
<featured>0</featured>
<description><![CDATA[Long before the nature of heat was understood, the fundamental limit of efficiency of heat-based engines was determined]]></description>
<postDate>Wed, 19 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100518151409-1.png</thumbURL>
<smallURL width='140' height='147'>http://web.mit.edu/newsoffice/images/article_images/w140/20100518151409-1.jpg</smallURL>
<fullURL width='368' height='387'>http://web.mit.edu/newsoffice/images/article_images/20100518151409-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Wikimedia Commons]]></imageCredits>
<imageCaption><![CDATA[Nicolas Léonard Sadi Carnot]]></imageCaption>
</image>
<body><![CDATA[Anytime engineers try to design a new kind of heat-based engine or improve on an existing design, they bump up against a fundamental efficiency limit: the Carnot Limit.<br /><br />The Carnot Limit “sets an absolute limit on the efficiency with which heat energy can be turned into useful work,” says MIT’s Jane and Otto Morningstar Professor of Physics Robert Jaffe, who co-teaches a course on the physics of energy. If engineers are faced with redesigning an engine that is 35 percent efficient, it makes a big difference whether the maximum possible efficiency of such an engine is 50 percent — in which case it may not be feasible to try to push it further — or 80 percent, in which case there is a significant margin for improvement.<br /><br />Nicolas Léonard Sadi Carnot, who was born in France in 1796 and lived for only 36 years, deduced this limit. His insights into the nature of heat, and the limitations on machines that use heat, had an impact that lasts to this day. What makes his accomplishments all the more remarkable is the fact that the nature of heat itself was not understood until long after Carnot’s death. At the time of his research, scientists still subscribed to the later-discredited “caloric” theory of heat, which held that an invisible fluid of that name carried heat from one object to another.  <br /><br />Carnot’s 1824 book “Reflections on the Motive Power of Fire” laid out a set of principles that, in some cases, are still widely used. One of those is the Carnot Limit (also known as Carnot efficiency), which is given by a simple equation: the difference in temperature between the hot working fluid — such as the steam in a power plant — and its cooled-off temperature as it leaves the engine, divided by the temperature in degrees Kelvin (that is, degrees above absolute zero) of the hot fluid. This theoretical efficiency is expressed as a percentage, which can be approached but never actually reached.<br /><br />At the time of Carnot’s work, the best steam engines in the world had an overall efficiency of only about 3 percent. Today, conventional steam engines can reach efficiencies of 25 percent, and gas-fired turbine steam generators in power plants can reach 40 percent or more — compared to a Carnot Limit, depending on the exact heat differences in such plants, of about 51 percent. Today’s car engines have efficiencies of 20 percent or less, compared to their Carnot Limit of 37 percent.<br /><br />Since the limit on efficiency is based on the temperature difference between the heat source and whatever is used to cool the system — usually outside air or a supply of water — it is clear that the hotter the heat source, the higher the possible efficiency. So, for example, Jaffe explains, “a fourth-generation nuclear reactor that heats steam to 1200 degrees Celsius uses a given amount of energy far more efficiently than a geothermal energy source that employs steam at 120 degrees Celsius.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[A look back in time]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/stromatolites-0518.html</link>
<story_id>15336</story_id>
<featured>0</featured>
<description><![CDATA[By linking the odd geometry of bacterial growths to photosynthesis, researchers may have a new way to study Earth’s oldest fossils.]]></description>
<postDate>Tue, 18 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100517155151-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100517155151-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100517155151-0.jpg</fullURL>
<imageCredits><![CDATA[Image: Kate Harris]]></imageCredits>
<imageCaption><![CDATA[Microbial mats growing in hot spring-fed streams, like these here in Yellowstone National Park, offer insights into the evolution of life on early Earth. As mineral-laden water flow over the mat, silica crystals grow, and the mat eventually turns into stone. The lithified mat is an analogue of conical stromatolites that are found in sedimentary rocks that are billions of years old.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100514132002-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Kate Harris]]></imageCredits>
<imageCaption><![CDATA[More microbial mats in Yellowstone]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[About 85 percent of the history of life on Earth has been solely microbial, meaning that single-celled organisms ruled the planet for billions of years before more complex life evolved. One way that geologists try to decipher how these cells functioned as far back as 3 billion years is by studying modern microbial mats, or gooey layers of nutrient-exchanging bacteria that grow mostly on moist surfaces. <br /><br />These layers collect dirt and minerals that crystallize over time. Eventually, the bacteria turn to stone just beneath the crystallized material, thereby recording their history within the crystalline skeletons. Known as stromatolites, the layered rock formations are considered to be the oldest fossils on Earth. While there are many kinds of stromatolites, many geologists believe that the odd triangular patterns of conical stromatolites are the purest record of bacterial growth because they form as a result of only two processes: the growth of bacteria and the growth of mineral crystals.<br /><br />Deciphering the few clues about ancient bacterial life that are seen in these poorly preserved rocks has been difficult, but researchers from MIT’s Department of Earth, Atmospheric and Planetary Sciences (EAPS) and the Russian Academy of Sciences may have found a way to glean new information from the fossils. Specifically, they have linked the even spacing between the thousands of tiny cones that dot the surfaces of stromatolite-forming microbial mats — a pattern that also appears in cross-sectional slices of stromatolites that are 2.8 billion years old — to photosynthesis. The connection could help scientists put a better range on when photosynthesis started. <br /><br />In a <a href="http://www.pnas.org/content/early/2010/05/12/1001973107.abstract?sid=44ad62e8-d916-4c56-a0bd-cc443d823c68" target="_blank">paper published Monday</a> in the <em>Proceedings of the National Academy of Science</em>, the researchers suggest that the characteristic centimeter-scale spacing between neighboring cones that appears on modern microbial mats and the conical stromatolites they form occurs as a result of the daily competition for nutrients between neighboring mats. According to lead author and EAPS graduate student Alexander Petroff, the research suggests a physical understanding of how the morphology, or shape, of a bacterial community records its ability to use light as an energy source.<br /><br />The research, which was funded by NASA, the National Science Foundation and the Solomon Buchsbaum Fund, also provides a new technique for interpreting the patterns of these ancient fossils, according to Petroff. By analyzing the length of the triangular patterns seen in an ancient stromatolite, for example, geologists can now infer more details about the environment in which the microbial mat lived, such as whether it lived in still or turbulent water.  <br /><br /><strong>The physics of photosynthesis</strong><br /><br />Until now, no one had explored the consistent one-centimeter spacing that appears between the tiny cones featured on microbial mats and conical stromatolites that grow in the hot springs of Yellowstone National Park, and at other locations around the world. Petroff and his colleagues, including EAPS professors Daniel Rothman and Tanja Bosak, proposed that the pattern was not coincidental and could pertain to a biophysical process, such as how the bacteria compete for nutrients. <br /><br />By studying the physics of photosynthesis, the researchers formed a better understanding of how a mat consumes nutrients from its surroundings over the course of a day, and then metabolizes, or breaks down, those nutrients for energy.<br /><br />During the daytime, a mat takes in nutrients like inorganic carbon from its immediate surroundings and uses energy from sunlight to build sugars and new bacteria. As these nutrients become locally depleted, the mat starts to consume nutrients from larger distances. At nighttime when it is dark and photosynthesis is not possible, nutrients return to the water immediately surrounding the mat. <br /><br />After studying this process in detail, the researchers reasoned that in order to avoid direct competition for nutrients, the spacing between mats must be influenced by diffusion, or how molecules spread out over time. In this case, diffusion is itself influenced by the amount of time a mat is metabolically active, which varies over the course of a day due to changes in sunlight. Therefore, the spacing between cones records the maximum distance that mats can compete with one another to metabolize nutrients that are spread by diffusion and later replenished at night. After testing this theory on cultures in the lab, the researchers confirmed their hypothesis through fieldwork in Yellowstone, where the centimeter spacing between mats corresponds to their metabolic period of about 20 hours. <br /><br />That the spacing pattern corresponds to the mats’ metabolic period — and is also seen in ancient rocks — shows that the same basic physical processes of diffusion and competition seen today were happening billions of years ago, long before complex life appeared. The spacing between individual mats also reveals other factors about a mat’s environment, the researchers say. For example, if the spacing is larger than one centimeter, this could be evidence that the bacteria lived in a turbulent environment. This is because when bacteria compete for nutrients in stirred, rather than still, water, this motion means they can consume nutrients from greater distances, meaning competition isn’t as fierce, and the mats can be more spread apart from one another.<br /><br />Paul Hoffman, a recently retired Harvard geologist, believes that the theoretical explanation will be of use to geologists because it can be easily observed and tested in the field. But he said it will be difficult to differentiate the patterns seen in the stromatolites studied by Petroff’s team — that is, stromatolites that are produced by biological processes — from those that grow solely from physical processes related to sedimentation. Because the growth of both kinds may be governed by diffusion of nutrients or ions, it is important that future research demonstrates that non-living stromatolites do not exhibit the same spacing relationship as living stromatolites, he said.<br /><br />Petroff agreed with Hoffman, noting that his research about the spacing between the cones of stromatolites is only part of what is required to interpret Earth’s earliest fossils properly, and that much work remains in order to address fundamental questions that have eluded a complete understanding. His group is currently working to address one of those questions — why biological stromatolites form cones instead of other shapes.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Machines that learn better]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/machine-learning-0518.html</link>
<story_id>15344</story_id>
<featured>0</featured>
<description><![CDATA[New math will make it much easier to build machine-learning systems that tackle a wider range of problems.]]></description>
<postDate>Tue, 18 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100517154705-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100517154705-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100517154705-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[Cameron Freer, left, an instructor in pure mathematics; and Daniel Roy, right, a PhD student in the Department of Electrical Engineering and Computer Science.]]></imageCaption>
</image>
<body><![CDATA[In the last 20 years or so, many of the key advances in artificial-intelligence research have come courtesy of machine learning, in which computers learn how to make predictions by looking for patterns in large collections of training data. A new approach called probabilistic programming makes it much easier to build machine-learning systems, but it’s useful for a relatively narrow set of problems. Now, MIT researchers have discovered how to extend the approach to a much larger class of problems, with implications for subjects as diverse as cognitive science, financial analysis and epidemiology.<br /><br />Historically, building a machine-learning system capable of learning a new task would take a graduate student somewhere between a few weeks and several months, says Daniel Roy, a PhD student in the Department of Electrical Engineering and Computer Science who along with Cameron Freer, an instructor in pure mathematics, led the new research. A handful of new, experimental, probabilistic programming languages — one of which, <a href="http://web.mit.edu/newsoffice/2010/ai-unification.html" target="_blank">Church</a>, was developed at MIT — promise to cut that time down to a matter of hours.<br /><br />At the heart of each of these new languages is a so-called inference algorithm, which instructs a machine-learning system how to draw conclusions from the data it’s presented. The generality of the inference algorithm is what confers the languages’ power: The same algorithm has to be able to guide a system that’s learning how to recognize objects in digital images, or filter spam, or recommend DVDs based on past rentals, or whatever else an artificial-intelligence program may be called upon to do.<br /><br />The inference algorithms currently used in probabilistic programming are great at handling discrete data but struggle with continuous data. For an idea of what that distinction means, consider three people of different heights. Their rank ordering, from tallest to shortest, is discrete: Each of them must be first, second, or third on the list. But their absolute heights are continuous. If the tallest person is 5 feet 10 inches tall, and the shortest is 5 feet 8 inches, you can’t conclude that the third person is 5 feet 9 inches: He or she could be 5 feet 8.5 inches, or 5 feet 9.6302 inches or an infinite number of other possibilities.<br /><br />Designers of probabilistic programming languages are thus avidly interested in whether it’s possible to design a general-purpose inference algorithm that can handle continuous data. Unfortunately, the answer appears to be no: In <a href="http://arxiv.org/abs/1005.3014" target="_blank">a yet-unpublished paper</a>, Freer, Roy, and Nate Ackerman of the University of California, Berkeley, mathematically demonstrate that there are certain types of statistical problems involving continuous data that no general-purpose algorithm could solve.<br /><br />But there’s good news as well: Last week, at the International Conference on Artificial Intelligence and Statistics, Roy presented a paper in which he and Freer not only demonstrate that there are large classes of problems involving continuous data that are susceptible to a general solution but also describe an inference algorithm that can handle them. A probabilistic programming language that implemented the algorithm would enable the rapid development of a much larger variety of machine-learning systems. It would, for instance, enable systems to better employ an analytic tool called the Pólya tree, which has been used to model stock prices, disease outbreaks, medical diagnoses, census data, and weather systems, among other things.<br /><br />“The field of probabilistic programming is fairly new, and people have started coming up with probabilistic programs, but Dan and Cameron are really filling the theoretical gaps,” says Zoubin Ghahramani, professor of information engineering at the University of Cambridge. The hope, Ghahramani says, “is that their theoretical underpinnings will make the effort to come up with probabilistic programming languages much more solidly grounded.”<br /><br />Chung-chieh Shan, a computer scientist at Rutgers who specializes in models of linguistic behavior, says that the MIT researchers’ work could be especially useful for artificial-intelligence systems whose future behavior is dependent on their past behavior. For instance, a system designed to understand spoken language might have to determine words’ parts of speech. If, in some context, it notices that a word tends to be used in an uncommon way — for instance, “man” is frequently used as a verb instead of a noun — then, going forward, it should have greater confidence in assigning that word its unusual interpretation.<br /><br />Often, Shan explains, treating problems as having such “serial dependency” makes them easier to describe. But it also makes their solutions harder to calculate, because it requires keeping track of an ever-growing catalogue of past behaviors and revising future behaviors accordingly. Freer and Roy’s algorithm, he says, provides a way to convert problems that have serial dependency into problems that don’t, which makes them easier to solve. “A lot of models would call for this kind of picture,” Shan says. Roy and Freer’s work “is narrowing this gap between the intuitive description and the efficient implementation.”<br /><br />While Freer and Roy’s algorithm is guaranteed to provide an answer to a range of previously intractable problems, Shan says, “there’s a difference between coming up with the right algorithm and implementing it so that it runs fast enough on an actual computer.” Roy and Freer agree, however, which is why they haven’t yet incorporated their algorithm into Church. “It’s fairly clear that within the set of models that our algorithm can handle, there are some that could be arbitrarily slow,” Roy says. “So now we have to study additional structure. We know that it’s possible. But when is it efficient?”<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Monte Carlo simulations]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/exp-monte-carlo-0517.html</link>
<story_id>15338</story_id>
<featured>0</featured>
<description><![CDATA[Mathematical technique lets scientists make estimates in a probabilistic world]]></description>
<postDate>Mon, 17 May 2010 13:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100514142339-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100514142339-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100514142339-1.jpg</fullURL>
<imageCaption><![CDATA[Monte Carlo]]></imageCaption>
</image>
<body><![CDATA[Speak to enough scientists, and you hear the words “Monte Carlo” a lot. “We ran the Monte Carlos,” a researcher will say. What does that mean?<br /><br />The scientists are referring to Monte Carlo simulations, a statistical technique used to model probabilistic (or “stochastic”) systems and establish the odds for a variety of outcomes. The concept was first popularized right after World War II, to study nuclear fission; mathematician Stanislaw Ulam coined the term in reference to an uncle who loved playing the odds at the Monte Carlo casino (then a world symbol of gambling, like Las Vegas today). Today there are multiple types of Monte Carlo simulations, used in fields from particle physics to engineering, finance and more. <br /><br />To get a handle on a Monte Carlo simulation, first consider a scenario where we do not need one: to predict events in a simple, linear system. If you know the precise direction and velocity at which a shot put leaves an Olympic athlete’s hand, you can use a linear equation to accurately forecast how far it will fly. This case is a deterministic one, in which identical initial conditions will always lead to the same outcome. <br /><br />The world, however, is full of more complicated systems than a shot-put toss. In these cases, the complex interaction of many variables — or the inherently probabilistic nature of certain phenomena — rules out a definitive prediction. So a Monte Carlo simulation uses essentially random inputs (within realistic limits) to model the system and produce probable outcomes.<br /><br />In the 1990s, for instance, the Environmental Protection Agency started using Monte Carlo simulations in its risk assessments. Suppose you want to analyze the overall health risks of smog in a city, but you know that smog levels vary among neighborhoods, and that people spend varying amounts of time outdoors. Given a range of values for each variable, a Monte Carlo simulation will randomly select a number within each range, and see how they combine — and repeat the process tens of thousands or even millions of times. No two iterations of the simulation might be identical, but collectively they build up a realistic picture of the population’s smog exposure.<br /><br />“In a deterministic simulation, you should get the same result every time you run it,” explains MIT computer science professor John Guttag in his OpenCourseWare lecture on Monte Carlo simulations. However, Guttag adds, in “stochastic simulations, the answer will differ from run to run, because there’s an element of randomness in it.” <br /><br />The aggregation of data makes it possible to identify, say, a median level of smog exposure. To be sure, Monte Carlo simulations are as good as their inputs; accurate empirical data would be necessary to produce realistic simulation results. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Fly the eco-friendly skies]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/nplus3-0517.html</link>
<story_id>15337</story_id>
<featured>0</featured>
<description><![CDATA[MIT-led team designs airplanes that would use 70 percent less fuel than current models.]]></description>
<postDate>Mon, 17 May 2010 04:00:04 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100514132853-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100514132853-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100514132853-1.jpg</fullURL>
<imageCredits><![CDATA[Image: MIT/Aurora Flight Sciences]]></imageCredits>
<imageCaption><![CDATA[MIT’s D “double bubble” series design concept is based on a modified “tube-and-wing” structure that has a very wide fuselage to provide extra lift. The aircraft would be used for domestic flights to carry 180 passengers in a coach cabin roomier than that of a Boeing 737-800.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100514141252-2.jpg</fullURL>
<imageCredits><![CDATA[Image: MIT/Aurora Flight Sciences]]></imageCredits>
<imageCaption><![CDATA[The MIT team also presented NASA with its design for the H “hybrid wing body” series to replace the 777 class aircraft now used for international flights. The design features a triangular-shaped hybrid wing body aircraft that blends a wider fuselage with the wings for improved aerodyamics. The large center body creates a forward lift that eliminates the need for a tail to balance the aircraft. The plane is designed to carry 350 passengers.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In what could set the stage for a fundamental shift in commercial aviation, an MIT-led team has designed a green airplane that is estimated to use 70 percent less fuel than current planes while also reducing noise and emission of nitrogen oxides (NOx).<br /><br />The design was one of two that the team, led by faculty from the Department of Aeronautics and Astronautics, presented to NASA last month as part of a $2.1 million research contract to develop environmental and performance concepts that will help guide the agency’s aeronautics research over the next 25 years. Known as “N+3” to denote three generations beyond today’s commercial transport fleet, the research program is aimed at identifying key technologies, such as advanced airframe configurations and propulsion systems, that will enable greener airplanes to take flight around 2035. <br /><br />MIT was the only university to lead one of the six U.S. teams that won contracts from NASA in October 2008. Four teams — led by MIT, Boeing, GE Aviation and Northrop Grumman, respectively — studied concepts for subsonic (slower than the speed of sound) commercial planes, while teams led by Boeing and Lockheed-Martin studied concepts for supersonic (faster than the speed of sound) commercial aircraft. Led by AeroAstro faculty and students, including principal investigator Ed Greitzer, the H. Nelson Slater Professor of Aeronautics and Astronautics, the MIT team members include <a href="http://www.aurora.aero/" target="_blank">Aurora Flight Sciences Corporation</a> and <a href="http://www.utc.com/Home" target="_blank">Pratt &amp; Whitney</a>. <br /><br />Their objective was to develop concepts for, and evaluate the potential of, quieter subsonic commercial planes that would burn 70 percent less fuel and emit 75 percent less NOx than today’s commercial planes. NASA also wanted an aircraft that could take off from shorter runways. Designing an airplane that could meet NASA’s aggressive criteria while accounting for the changes in air travel in 2035 — when air traffic is expected to double — would require “a radical change,” according to Greitzer. Although automobiles have undergone extensive design changes over the last half-century, “aircraft silhouettes have basically remained the same over the past 50 years,” he said, describing the traditional, easily recognizable “tube-and-wing” structure of an aircraft’s wings and fuselage.<br /><strong><br />Two planes for two missions</strong><br /><br />The MIT team met NASA’s challenge by developing two designs: the 180-passenger D “double bubble” series to replace the Boeing 737 class aircraft, currently used for domestic flights, and the 350 passenger H “hybrid wing body” series to replace the 777 class aircraft now used for international flights. <br /><br />The engineers conceived of the D series by reconfiguring the tube-and-wing structure. Instead of using a single fuselage cylinder, they used two partial cylinders placed side by side to create a wider structure whose cross-section resembles two soap bubbles joined together. They also moved the engines from the usual wing-mounted locations to the rear of the fuselage. Unlike the engines on most transport aircraft that take in the high-speed, undisturbed air flow, the D-series engines take in slower moving air that is present in the wake of the fuselage. Known as the Boundary Layer Ingestion (BLI), this technique allows the engines to use less fuel for the same amount of thrust, although the design has several practical drawbacks, such as creating more engine stress.<br /><br />According to Mark Drela, the Terry L. Kohler Professor of Fluid Dynamics and lead designer of the D series, the design mitigates some of the drawbacks of the BLI technique by traveling about 10 percent slower than a 737. To further reduce the drag and amount of fuel that the plane burns, the D series features longer, skinnier wings and a smaller tail. Independently, each tweak might not amount to much, but the “little 5-percent changes add up to one big change,” Drela said. Although the plane would travel slightly slower than a 737, he said that some of this time could be recovered because the plane’s wider size should allow for quicker loading and unloading.<br /><br />Not only does the D series meet NASA’s long-term fuel burn, emissions reduction and runway length objectives, but it could also offer large benefits in the near future because the MIT team designed two versions: a higher technology version with 70 percent fuel-burn reduction, and a version that could be built with conventional aluminum and current jet technology that would burn 50 percent less fuel and might be more attractive as a lower risk, near-term alternative.<br /><br />Carl Burleson, the director of the Federal Aviation Agency’s Office of Environment and Energy, said that in addition to its “really good environmental performance,” the D series is impressive because its bubble design is similar enough to the tube-and-wing structure of current planes that it should be easier to integrate into airport infrastructure than more radical designs. “You have to think about how an airport structure can support it,” he said. “ For some other designs, you could have to fundamentally reshape the gates at airports because the planes are configured so differently.”<br /><br />Although the H series utilizes much of the same technology as the D series, including BLI, a larger design is needed for this plane to carry more passengers over longer distances. The MIT team designed a triangular-shaped hybrid wing body aircraft that blends a wider fuselage with the wings for improved aerodyamics.  The large center body creates a forward lift that eliminates the need for a tail to balance the aircraft. <br /><br />The large structure also allows engineers to explore different propulsion architectures for the plane, such as a distributed system of multiple smaller engines. Although the H series meets NASA’s emissions-reduction and runway-length goals, the researchers said they will continue to improve the design to meet more of NASA’s objectives.<br /><br />The MIT team expects to hear from NASA within the next several months about whether it has been selected for the second phase of the program, which will provide additional funds to one or two of the subsonic teams in 2011 to research and develop the technologies identified during the first phase. The researchers acknowledge that some propulsion system technology still needs to be explored. They have proposed evaluating the interactions between the propulsion system and the new aircraft using a large-scale NASA wind tunnel. Even if the MIT designs are not chosen for the second phase, the researchers hope to continue to develop them, including testing smaller models at MIT’s Wright Brothers’ Wind Tunnel and collaborating with manufacturers to explore how to make the concepts a reality. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: Mapping the oil spill]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/oil-balloon-0517.html</link>
<story_id>15339</story_id>
<featured>0</featured>
<description><![CDATA[MIT student leads project using balloons and kites to provide aerial documentation of the Gulf oil slick’s extent and effects]]></description>
<postDate>Mon, 17 May 2010 04:00:03 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100514154849-4.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100514154849-4.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100514154849-4.jpg</fullURL>
<imageCredits><![CDATA[Photos courtesy of Jeffrey Warren and Stewart Long]]></imageCredits>
<imageCaption><![CDATA[Balloon images of coastal wetlands areas such as this one in Louisiana could become useful for later estimates of environmental damage if the area becomes contaminated with oil. ]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='490'>http://web.mit.edu/newsoffice/images/article_images/20100514154848-1.jpg</fullURL>
<imageCredits><![CDATA[Photos courtesy of Jeffrey Warren and Stewart Long]]></imageCredits>
<imageCaption><![CDATA[Stewart Long (right) prepares to launch a balloon-mounted camera from the Chandeleur Islands in Louisiana.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100514154848-2.jpg</fullURL>
<imageCredits><![CDATA[Photos courtesy of Jeffrey Warren and Stewart Long]]></imageCredits>
<imageCaption><![CDATA[MIT graduate student Jeffrey Warren (right) prepares to launch a kite-borne camera from a boat in the Gulf of Mexico. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100514154848-3.jpg</fullURL>
<imageCredits><![CDATA[Photos courtesy of Jeffrey Warren and Stewart Long]]></imageCredits>
<imageCaption><![CDATA[Image of Louisiana coastline taken by digital camera suspended from a balloon. The green line is the tether holding the balloon, which was launched from the boat seen at top center. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100514154849-5.jpg</fullURL>
<imageCredits><![CDATA[Photos courtesy of Jeffrey Warren and Stewart Long]]></imageCredits>
<imageCaption><![CDATA[Orange bands of oil and tar can be seen washing up on the Chandeleur Islands. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='218'>http://web.mit.edu/newsoffice/images/article_images/20100514154849-6.png</fullURL>
<imageCredits><![CDATA[Photos courtesy of Jeffrey Warren and Stewart Long; background: Google Earth]]></imageCredits>
<imageCaption><![CDATA[Using software developed by Jeffrey Warren, images can be stitched together and correctly oriented on a background image of the region. White areas are part of the Chandeleur Islands, and yellow push pins represent GPS readings taken for location reference.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<br /><br />While the world’s news media report on the expanding oil slick in the Gulf of Mexico, MIT Media Lab student Jeffrey Yoo Warren and his collaborators are providing their own useful coverage of the crisis.<br /><br />Warren, Oliver Yeh ’10 and do-it-yourself cartographer Stewart Long have been using inexpensive cameras attached to ordinary helium-filled weather balloons, or even oversized trash bags, to capture aerial photos that are then stitched together and geometrically corrected using software Warren wrote (to compensate for camera angle and distortions) to make very accurate local maps. For example, last January Warren traveled to Peru to help citizens there meet a requirement to provide accurate maps of the lands they occupied in order to obtain the title to the lands — Warren’s mapping software enabled them to create those maps.<br /><br />The team realized that in the Gulf oil spill, future lawsuits, as well as monitoring of environmental impacts from the spill, would depend on extensive, accurate documentation. So earlier this month, as crews tried unsuccessfully to contain the undersea oil leak, Warren and his colleagues took their technology to the Gulf Coast. Working with local groups such as the Louisiana Bucket Brigade, an organization in New Orleans that focuses on industrial pollution, they trained volunteers to make and deploy the low-cost photography systems. “Now, they’re the local leads on this, and we provide the training and technical support,” Warren says. The systems can be suspended from balloons or kites, and all the equipment needed for one monitoring system can be bought for less than $100.<br /><br />For now, as long as the oil remains offshore, they are using the system to provide detailed imagery of the vulnerable coastal wetlands — essentially, the “before” pictures to be used for comparison with any future damage in the event the oil reaches those areas. They have also hitched rides on private planes to get out far enough to document parts of the spreading oil slick. Using GPS tagging and the software they developed to integrate the images, they hope to create an extensive set of documentation that can be used to monitor the spill’s effects, and all the images are being made accessible online for everyone.<br /><br />“To use this in litigation, as well as environmental monitoring, you really need to have maps, not just imagery,” Warren explains. He hopes to return to the site next weekend to continue the project.<br /><br />Shannon Dosemagen, a member of the Louisiana Bucket Brigade, says the mapping tool will be useful for local communities that find themselves confronting the crisis. “As the spill spreads gradually, being able to map plots of land over a stretch of time will be incredibly valuable to show how the oil is impacting the coastal environments and thus livelihoods of coastal populations.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[New water-splitting catalyst found]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/nocera-0514.html</link>
<story_id>15321</story_id>
<featured>0</featured>
<description><![CDATA[Research by MIT’s Dan Nocera expands the list of potential electrode materials that could be used to store energy.]]></description>
<postDate>Fri, 14 May 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100513172334-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100513172334-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100513172334-0.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100511105402-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[Daniel Nocera, the Henry Dreyfus Professor of Energy and Professor of Chemistry]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Expanding on work published two years ago, MIT’s Daniel Nocera and his associates have found yet another formulation, based on inexpensive and widely available materials, that can efficiently catalyze the splitting of water molecules using electricity. This could ultimately form the basis for new storage systems that would allow buildings to be completely independent and self-sustaining in terms of energy: The systems would use energy from intermittent sources like sunlight or wind to create hydrogen fuel, which could then be used in fuel cells or other devices to produce electricity or transportation fuels as needed.<br /><br />Nocera, the Henry Dreyfus Professor of Energy and Professor of Chemistry, says that solar energy is the only feasible long-term way of meeting the world’s ever-increasing needs for energy, and that storage technology will be the key enabling factor to make sunlight practical as a dominant source of energy. He has focused his research on the development of less-expensive, more-durable materials to use as the electrodes in devices that use electricity to separate the hydrogen and oxygen atoms in water molecules. By doing so, he aims to imitate the process of photosynthesis, by which plants harvest sunlight and convert the energy into chemical form.<br /><br />Nocera pictures small-scale systems in which rooftop solar panels would provide electricity to a home, and any excess would go to an electrolyzer — a device for splitting water molecules — to produce hydrogen, which would be stored in tanks. When more energy was needed, the hydrogen would be fed to a fuel cell, where it would combine with oxygen from the air to form water, and generate electricity at the same time. <br /><br />An electrolyzer uses two different electrodes, one of which releases the oxygen atoms and the other the hydrogen atoms. Although it is the hydrogen that would provide a storable source of energy, it is the oxygen side that is more difficult, so that’s where he and many other research groups have concentrated their efforts. In a paper in Science in 2008, Nocera reported the discovery of a durable and low-cost material for the oxygen-producing electrode based on the element cobalt.<br /><br />Now, in research being <a href="http://www.pnas.org/content/early/2010/05/06/1001859107" target="_blank">reported this week</a> in the journal <em>Proceedings of the National Academy of Science</em> (PNAS), Nocera, along with postdoctoral researcher Mircea Dinc? and graduate student Yogesh Surendranath, report the discovery of yet another material that can also efficiently and sustainably function as the oxygen-producing electrode. This time the material is nickel borate, made from materials that are even more abundant and inexpensive than the earlier find.<br /><br />Even more significantly, Nocera says, the new finding shows that the original compound was not a unique, anomalous material, and suggests that there may be a whole family of such compounds that researchers can study in search of one that has the best combination of characteristics to provide a widespread, long-term energy-storage technology.<br /><br />“Sometimes if you do one thing, and only do it once,” Nocera says, “you don’t know — is it extraordinary or unusual, or can it be commonplace?” In this case, the new material “keeps all the requirements of being cheap and easy to manufacture” that were found in the cobalt-based electrode, he says, but “with a different metal that’s even cheaper than cobalt.” The work was funded by the National Science Foundation and the Chesonis Family Foundation.<br /><br />But the research is still in an early stage. “This is a door opener,” Nocera says. “Now, we know what works in terms of chemistry. One of the important next things will be to continue to tune the system, to make it go faster and better. This puts us on a fast technological path.” While the two compounds discovered so far work well, he says, he is convinced that as they carry out further research even better compounds will come to light. “I don’t think we’ve found the silver bullet yet,” he says.<br /><br />Already, as the research has continued, Nocera and his team have increased the rate of production from these catalysts a hundredfold from the level they initially reported two years ago. <br /><br />John Turner, a research fellow at the National Renewable Energy Laboratory in Colorado, calls this a nice result, but says that commercial electrolyzers already exist that have better performance than these new laboratory versions. “The question then is under what circumstances would this system provide some advantage over the existing commercial systems,” he says. For large-scale deployment of solar fuel-producing systems, he says, “the big commercial electrolyzers use concentrated alkali for their electrolyte, which is OK in an industrial setting were engineers know how to handle the stuff safely; but when we are talking about thousands of square miles of solar water-splitting arrays, and individual homeowners, then an alternative electrolyte like this benign borate solution may be more viable.”<br /><br />The original discovery has already led to the creation of a company, called Sun Catalytix, that aims to commercialize the system in the next two years. And his research program was recently awarded a major grant from the U.S. Department of Energy’s Advanced Research Projects Agency — Energy.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Exposing collagen's double life]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/collagen-0514.html</link>
<story_id>15333</story_id>
<featured>0</featured>
<description><![CDATA[Discovery that the rigid structural protein can switch to a floppy shape could lead to new treatments for heart disease.]]></description>
<postDate>Fri, 14 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100513145129-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100513145129-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100513145129-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[MIT associate professor Collin Stultz, a cardiologist and biomedical engineer, studies the structure of collagen.]]></imageCaption>
</image>
<body><![CDATA[Collagen, a type of connective tissue that makes up about 30 percent of the human body, plays many roles. The structural protein is an important component of muscle, skin, bones and cartilage, and forms scar tissue when injuries heal.<br /><br />However, it’s one of collagen’s lesser-known functions that piqued the interest of MIT associate professor Collin Stultz, a cardiologist and biomedical engineer, several years ago. When cholesterol builds up in the arteries, giving rise to plaques, collagen forms a protective layer that envelops the plaques. If collagen fails to hold the plaques together, they burst, spilling out cholesterol, other fatty molecules, and blood-clotting agents — usually with disastrous consequences. <br /><br />“Catastrophic heart attacks — the kind where you’re walking down the street, or watching TV, and then keel over and die — are most often associated with a rupture of the collagen layer,” says Stultz, the W.M. Keck Associate Professor of Biomedical Engineering.<br /><br />Understanding how collagen breaks down could help scientists develop new treatments for atherosclerosis and other diseases that involve collagen, such as arthritis. Over the past eight years, Stultz has published a series of papers that have helped transform the prevailing theories on collagen, which has traditionally been thought of as a rigid, rope-like molecule.<br /><br />In his most recent paper, <a href="http://pubs.acs.org/doi/full/10.1021/bi9021473" target="_blank">published online</a> in the journal <em>Biochemistry</em> last month, Stultz and his colleagues showed that, depending on the temperature, collagen can switch between its usual rigid structure and a much floppier, more flexible shape — a finding that collagen researcher Barbara Brodsky of Robert Wood Johnson Medical School described as “intriguing.” <br /><br />“The concept of multiple conformations is a big contribution to our thinking,” says Brodsky, who was not involved in the study. <br /><br /><strong>A better fit</strong><br /><br />For the past couple of decades, scientists have been trying to figure out the relationship between collagen and the enzymes, known as collagenases, that break it down in the body. Because collagen is a critical component of so many structural elements, such as bone and skin, its degradation is very carefully controlled. <br /><br />Structural studies that require crystallizing the protein (a common technique also used to reveal the double helix structure of DNA) showed that collagen is a tightly wound triple helix. However, that structure puzzled scientists because it offers no access for collagenase enzymes to bind to and break down the protein. “If you just look at the structures, you would say collagen should never be broken up by these enzymes,” says Stultz.<br /><br />Stultz suspected that the low temperatures (10 degrees Celsius) required to crystallize collagen in those studies might be masking some aspect of the protein’s true structure. He did computer modeling that suggested higher temperatures, such as room or body temperature, allow some sections of the collagen molecule to unwind, becoming floppy. That structure also opens up a site that fits collagenase perfectly, allowing it to break down the protein.<br /><br />“We think of collagen as being a very rigid molecule, while in practice there may be regions of collagen that are very floppy and loose,” says Stultz.<br /><br />To test this idea experimentally, Stultz and paper co-authors Ramon Salsas-Escat, a graduate student, and Paul Nerenberg, a recent PhD recipient, exposed room-temperature collagen to a mutated form of collagenase that only recognizes unfolded collagen. Previous studies had shown no degradation under these conditions, but Stultz and his colleagues waited longer (up to six days), and found that some of the collagen was broken down. <br /><br />They also found that the percentage of floppy collagen molecules at any given time depends on the temperature. At room temperature, about one in 1,000 are floppy, and the number should be higher at body temperature.<br /><br />Over the past couple of decades, researchers have tried to develop drugs that inhibit collagenase enzymes, to prevent arterial plaques from rupturing, but no such drugs have been approved. Stultz’s new findings raise the possibility of targeting the collagen itself, rather than the enzyme. That is, they could try to make collagen more rigid, so it won’t be susceptible to collagenase degradation.<br /><br />Stultz is now screening computer databases for small molecules that could stabilize collagen in its tightly wound state, which could also help preserve collagen in arthritic joints. Such drugs may also have an impact on cancer metastasis, because tumor cells must break down collagen in the basement membrane (which lines the surfaces of organs and blood vessels) before they can spread through the body.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Cementing success]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/100k-competition-0514.html</link>
<story_id>15334</story_id>
<featured>0</featured>
<description><![CDATA[Startup that eyes radical shift in cement industry wins MIT $100K business-plan competition, now in its 20th year.]]></description>
<postDate>Fri, 14 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100513174357-1.png</thumbURL>
<smallURL width='140' height='87'>http://web.mit.edu/newsoffice/images/article_images/w140/20100513174357-1.jpg</smallURL>
<fullURL width='368' height='230'>http://web.mit.edu/newsoffice/images/article_images/20100513174357-1.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100513174357-2.jpg</fullURL>
<imageCredits><![CDATA[Still image: AMPS]]></imageCredits>
<imageCaption><![CDATA[C-Crete co-founders Rouzbeh Shahsavari (second from left) and Natanel Barookhian (second from right) are awarded the $100,000 grand prize by keynote speaker Paul Fireman (far left), former CEO of Reebok, and $100K Managing Director Daniel Vannoni (center) and Associate Director Jarrod Phipps (right) at the competition finale at Kresge Auditorium. ]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Judging by history, an MIT-student startup company called C-Crete Technologies, which won this week’s finals of the annual student-run MIT $100K Business Plan Competition, stands a good chance of beating the odds to become a successful business. But history also suggests that some of the other finalists who didn’t win the big prize might end up doing just as well, or even better.<br /><br />C-Crete, started by MIT doctoral student in Civil and Environmental Engineering Rouzbeh Shahsavari and MIT Sloan School of Management MBA candidate Natanel Barookhian, is based on discoveries made last year at MIT about the molecular structure of cement. The production of cement accounts for more than 10 percent of the world’s greenhouse gas emissions, the team members say, and the new nano-engineered type of cement they developed could reduce that by half, could ultimately save as much energy as would be produced by 100 large nuclear power plants — and would be stronger as well.<br /><br />“We believe our technology will make a significant impact on the world, and we look forward to growing a viable, sustainable business to meet these ends,” Barookhian said.<br /><br />This year, the competition received about 200 submissions, which were winnowed down to 28 semifinalists and then six finalists, one in each of six separate tracks. In addition to $20,000 prizes for each finalist team, this year there was also a $10,000 “audience choice” award, selected by the crowd assembled for the competition finale on Wednesday evening at Kresge Auditorium. That award went to a venture called Aukera, which aims to develop a treatment for the disease ALS, based on recent research at Harvard.<br /><br />The contest started off in 1989 as a way to promote interest in MIT’s then newly formed Entrepreneurs Club, which collaborated with the Sloan School’s New Ventures Association to organize the first contest. Initially planned for a $1,000 prize, the inaugural competition was quickly expanded to $10,000, and over the ensuing years grew to $50,000 and finally in 2004 to $100,000. Although it is still called the $100K — the amount of the top prize — this year’s contest awarded a total of $350,000 in prizes.<br /><br />As the prize amounts have grown, so, too, has the competition’s reputation. The <em>San Francisco Chronicle</em> years ago referred to the MIT $100K as “the granddaddy” of business-plan contests, and <em>Inc.</em> magazine said that even as such contests have proliferated nationally and globally, MIT’s remains “more equal than all the others.”<br /><br /><strong>20 years later: ‘Quite a success’</strong><br /><br />In its two decades of existence, the competition has spawned more than 130 companies, which have raised more than $770 million in financing and had a cumulative market value of over $15 billion. Among its biggest success stories are Akamai Technologies, a company that provides speedy Internet connections for a significant fraction of the Web’s worldwide traffic and had revenues of more than $850 million last year, and Silicon Spice, a semiconductor company that was sold for $1.2 billion just five years after entering the competition. (Interestingly, neither of these won the top prize: Akamai was a runner-up in 1998, and Silicon Spice was a finalist that year). <br /><br />Peter Mui ’82, one of the founders of both the competition and the Entrepreneurs Club, said that the idea originated from the realization that at MIT back in the 1980s, there were lots of people with interesting ideas, but few opportunities for people to meet to discuss them or to learn how to turn the ideas into companies, and there was little interaction between the people developing technologies in the School of Engineering and those with expertise in business at the Sloan School. The club was conceived as a way to meet that need, and soon club co-founder Richard Shyduroff, who has taught an MIT seminar on technology startups for the last 20 years, suggested holding a contest. The initial prize was going to be $1,000, but staff at the Technology Licensing Office suggested boosting that to $10,000, and Shyduroff spent months pounding on doors to raise the prize money. “From there, things just snowballed,” Shyduroff said.<br /><br />“We never thought the competition would take on a life of its own,” Mui said. He left MIT after the first year (and is now a consultant working with small startup companies), but Shyduroff and the club’s other co-founder, Douglas Ling SM ’87, kept the contest going.<br /><br />Joost Bonsen ’90 SM ’02, who is now the co-director of the MIT Media Lab’s new Entrepreneurship Program, submitted eight entries in that first year’s competition — a record number of entries that still stands. Bonsen took over the management of the competition in 1993 and 1994. Every year the competition has continued, he said, its student leaders have continued to build and expand on its scope and reach. “Alums from the competition have started business-plan competitions around the world, including Germany, Russia, Ghana and the Philippines,” he said.<br /><br />MIT Professor of Applied Mathematics Tom Leighton, one of the co-founders of Akamai Technologies and now its chief scientist, said the competition made all the difference: “If there hadn’t been a $50K competition, there wouldn’t have been an Akamai,” he said. “We were a research project, and we had no intention of starting a company,” but the competition provided him and his colleagues with some experience and with introductions to a network of people who helped make the company possible — even though they were not even among the top three finishers in their year. This year, he said, the company expects to exceed $1 billion in revenues.<br /><br />Over the 20 years of the contest, organizers estimate that some 5,000 students have competed. “I never imagined it would be as large a vehicle as it turned out to be,” Mui said. “It’s been quite a success.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Forging a digital MIT]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/letter-ruiz.html</link>
<story_id>15330</story_id>
<featured>0</featured>
<description><![CDATA[Letter from Vice President for Finance Israel Ruiz on updates to administrative systems]]></description>
<postDate>Thu, 13 May 2010 14:58:22 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100513110523-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100513110523-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100513110523-1.jpg</fullURL>
</image>
<body><![CDATA[To the members of the MIT community,<br /><br />Reflecting on the economic challenges of the past two years, I am struck by how readily each member of the MIT community embraced the need to adapt to a new future. To me, this readiness was a testament to the uniquely innovative, collaborative and problem-solving spirit of the MIT community. It is what makes us great, and it is what will ensure that MIT remains strong and continues to pursue with resolve its mission to best serve the nation and the world in the 21st century.<br /><br />The Institute-wide Planning Task Force was the centerpiece of efforts across campus to conceive of new ways to operate and advance the MIT mission. A significant theme emerging from the Task Force effort is the overall need for MIT to modernize its administrative systems and processes and to work toward a largely digital MIT. The emergence of this theme dovetails with efforts in the Office of the Vice President for Finance (VPF), in close collaboration with the MIT community and key administrative offices, to act on an ambitious agenda of process modernization to better meet the needs of the community in an increasingly global world. Together, we are forging a digital evolution.<br /><br />During the upcoming months of 2010 and into 2011, we will begin to realize the benefits of our work and of the Task Force, with the launch of new VPF processes and digital systems. I would like to share with you some of these exciting initiatives and invite you to click on the links below to learn more:<br /><br /><strong>HR/Payroll</strong><br /><br />VPF is working with Human Resources (HR), Information Services and Technology (IS&amp;T), and representatives from departments, labs, and centers (DLCs) to significantly enhance the HR/Payroll services we provide to the MIT community.<br /><br /><a href="http://vpf.mit.edu/epaystubs" target="_blank">ePaystubs </a><br />In the first of several HR/Payroll digital initiatives, all faculty, staff, and students will be able to receive electronic paystubs in place of paper paystubs beginning in late June. This new digital process means members of the MIT community will have easy access to current and past paystubs (24 months) through the Money Matters tab in SAPweb Employee Self Service. Union staff members will be able to opt in to the entirely paperless process through this system. Looking ahead, we are working to provide access to electronic W2s for 2010 tax reporting as well.<br /><br /><a href="http://vpf.mit.edu/site/appointment_process_redesign" target="_blank">Appointment Process Redesign </a><br />The Appointment Process Redesign project will transform the way the Institute manages HR/Payroll transactions for faculty and staff, and automate the processing of nearly all of the 17,000 paper-based HR/Payroll transactions processed manually today. The new system is being offered in a pilot program and will be made available Institute-wide in the fall of 2010.<br /><br /><strong>Travel</strong><br /><br /><a href="http://vpf.mit.edu/site/travel_modernization_project" target="_blank">Modernization Project </a><br />After a successful pilot program with a cross-section of MIT travelers, VPF is now implementing the Travel Modernization Project in phases across the Institute. This project will dramatically improve the experience for travelers and the staff who support them in handling 30,000 business trips per year. Our new travel tools and services include the availability of an MIT travel credit card, an online booking tool for arranging trips with the opportunity to benefit from newly negotiated MIT contracts with travel providers, electronic expense reporting to replace the existing paper-based system, and a direct-deposit feature for reimbursements.<br /><br /><strong>Non-Travel Reimbursement</strong><br /><br /><a href="http://vpf.mit.edu/site/sapweb_request_for_payment" target="_blank">SAPweb Request for Payment (RFP)</a><br />We are now piloting an online service to process and reimburse via direct deposit non-travel reimbursements and payments. This service will replace 50,000 paper forms currently processed each year, such as those reimbursements for small out-of-pocket expenses and honoraria. The new SAPweb Request for Payment (RFP) service will be available to the MIT-wide community in July. As of Jan. 1, 2011, VPF will transition fully to this digital process for reimbursement requests.<br /><br /><strong>Cashier</strong><br /><br /><a href="http://vpf.mit.edu/site/evolution_of_cashier_function" target="_blank">Evolution of the Cashier Function </a><br />The MIT Cashier function will take a major step forward on June 30, 2010, with the adoption of more efficient and more secure methods of processing cash and the closing of the Cashier's Window in NE49. Under the redesigned process, DLCs that currently accept or distribute cash will be able to process cash and checks through convenient on-campus banking locations serviced by Bank of America. VPF is working to transition the affected DLCs now to ensure we are all ready to take this step on June 30.<br /><br /><strong>A Foundation for the Future</strong><br /><br />Energized by MIT's innovative spirit and fueled by the momentum from the Task Force, our work does not end with the adoption of these new tools and services. VPF is evaluating options for electronic invoicing and payables, and we will talk more with you about options for electronic goods receipt in lieu of distributing and signing paper invoices. We will also simplify and expedite the requisition-purchase order and sole-source justification processes for procuring goods and services. We are looking at opportunities to streamline our processes within Property and Budget and to continue automation within HR/Payroll. We also remain supportive of community-wide efforts to improve financial reporting and forecasting.<br /><br />Providing the highest quality user experience is of the utmost importance in all VPF process redesigns, and we will continue to align our organization and refine all of our processes and systems to achieve outstanding service to the community. User advisory groups play a critical role in gathering and incorporating community feedback, and we will work continually to balance the need for simplicity and efficiency with providing meaningful choice.<br /><br />As we progress through the coming year, I want to assure you that we will work closely with all those affected by these new tools and services – including students and our colleagues in Student Activities – to assure that we fully support their needs during the transition. I also want to express my appreciation to all those across the MIT community who have participated in user advisory groups and pilot programs and have supported our efforts to evolve our services. Our experience with the Task Force and VPF process modernization efforts assures me that through collaborative innovation we can realize new, more effective ways to operate and advance the MIT mission. I would very much welcome your questions and feedback. You may either email me directly or submit your thoughts through the <a href="http://vpf.mit.edu/site/ask_vpf_a_question" target="_blank">VPF web site</a>.<br /><br />Sincerely,<br />Israel Ruiz<br />Vice President for Finance<br /> <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Stubbe wins prestigious Welch Award]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/stubbe-welch-0513.html</link>
<story_id>15328</story_id>
<featured>0</featured>
<description><![CDATA[Professor of chemistry and biology honored for her work on enzymes involved in DNA and RNA synthesis]]></description>
<postDate>Thu, 13 May 2010 04:01:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100512171817-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100512171817-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100512171817-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Varney ]]></imageCredits>
<imageCaption><![CDATA[MIT biochemist JoAnne Stubbe]]></imageCaption>
</image>
<body><![CDATA[For her groundbreaking work explaining how enzymes — proteins that act as catalysts to speed up reactions millions-fold — have evolved to carry out difficult and ingenious chemistry critical to life, MIT Professor JoAnne Stubbe was today named co-winner of the Welch Award in Chemistry.<br /><br />Stubbe shares the award with Christopher Walsh, a professor at Harvard Medical School and former member of the MIT faculty, where he chaired the chemistry department. She is the second MIT faculty member to win the award, after Alexander Rich, who won in 2008.<br /><br />Stubbe, the Novartis Professor of Chemistry and Biology, has focused most of her career studying the mechanisms of enzymes involved in nucleotide metabolism, central to the biosynthesis of DNA and RNA. Her success in unraveling the specific steps in enzymatic reactions over the past four decades has had profound impacts on fields ranging from cancer-drug development to synthesis of biodegradable plastics.<br /><br />Her most noted work, for which she was awarded the National Medal of Science last year, involves a key class of enzymes that play an essential role in DNA replication and repair. She discovered how those enzymes, called ribonucleotide reductases, use free-radical chemistry to convert the monomeric RNA building blocks (nucleotides) to the monomeric DNA building blocks (deoxynucleotides), essential to make and repair DNA. <br /><br />Stubbe’s detailing of this nucleotide reduction process has led to the design of mechanism-based inhibitors; one of these is the drug gemcitabine, used clinically in the treatment of advanced pancreatic cancer and non-small cell lung carcinomas.<br /><br />The Welch Foundation, based in Houston, is one of the nation's oldest and largest sources of private funding for basic research in chemistry. It will present the $300,000 award and gold medallion to Stubbe and Walsh at the foundation’s annual conference in October.<br /><br />“I am honored to receive the Welch Award, but I couldn’t have done any of this without many creative collaborators and my outstanding and creative students and postdocs,” said Stubbe. “I have always been fascinated by how nature has evolved over millions of years to do these very tough chemical reactions. Enzymes do a lot of pretty cool things and I love discovering exactly how they do it.”<br /><br />Born in Champaign, Ill., Stubbe earned her BS in chemistry from the University of Pennsylvania and her doctorate from the University of California, Berkeley. She taught at Williams College before returning to research at Brandeis (working as postdoc in 1995 Welch Award recipient Robert Abeles’ lab), Yale and the University of Wisconsin-Madison. She joined the MIT faculty in 1987, recruited by co-recipient Walsh. <br /><br />Walsh began his career at MIT with studies of “suicide substrates,” a class of enzyme inactivators with major implications for how drugs work. After 15 years at MIT, he has spent the last 22 years at Harvard Medical School, where he now studies the biosynthesis of natural product antibiotics and the chemical logic and enzymatic machinery of how they are made, in order to identify new antibiotics, antitumor agents and immunosuppressants and to improve the efficiency of production.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Building organs block by block]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/tissue-legos-0513.html</link>
<story_id>15326</story_id>
<featured>0</featured>
<description><![CDATA[Tissue engineers create a new way to assemble artificial tissues, using ‘biological Legos’ — cells transformed into bricks.]]></description>
<postDate>Thu, 13 May 2010 04:00:03 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100512131100-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100512131100-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100512131100-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Javier Gomez Fernandez]]></imageCredits>
<imageCaption><![CDATA[A half sphere of polymer cubes built by researchers at the MIT-Harvard Division of Health Sciences and Technology.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='490'>http://web.mit.edu/newsoffice/images/article_images/20100512131100-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Javier Gomez Fernandez]]></imageCredits>
<imageCaption><![CDATA[Researchers at the MIT-Harvard Division of Health Sciences and Technology built this tubular tissue by encasing cells in polymer "bricks" and attaching them to a tube-shaped template.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Tissue engineering has long held promise for building new organs to replace damaged livers, blood vessels and other body parts. However, one major obstacle is getting cells grown in a lab dish to form 3-D shapes instead of flat layers.<br /><br />Researchers at the MIT-Harvard Division of Health Sciences and Technology (HST) have come up with a new way to overcome that challenge, by encapsulating living cells in cubes and arranging them into 3-D structures, just as a child would construct buildings out of blocks.<br /><br />The new technique, dubbed “micromasonry,” employs a gel-like material that acts like concrete, binding the cell “bricks” together as it hardens. Ali Khademhosseini, assistant professor of HST, and former HST postdoctoral associate Javier Gomez Fernandez describe the work in a paper <a href="http://www3.interscience.wiley.com/journal/123410503/abstract" target="_blank">published online last week</a> in the journal <em>Advanced Materials</em>.<br /><br />The tiny cell bricks hold potential for building artificial tissue or other types of medical devices, says Jennifer Elisseeff, associate professor of biomedical engineering at Johns Hopkins University, who was not involved in the research. “They’re very elegant and have a lot of flexibility in how you grow them,” she says. “It’s very creative.”<br /><strong><br />Controlled structure</strong><br /><br />To obtain single cells for tissue engineering, researchers have to first break tissue apart, using enzymes that digest the extracellular material that normally holds cells together. However, once the cells are free, it’s difficult to assemble them into structures that mimic natural tissue microarchitecture.<br /><br />Some scientists have successfully built simple tissues such as skin, cartilage or bladder on biodegradable foam scaffolds. “That works, but it often lacks a controlled microarchitecture,” says Khademhosseini, who is also an assistant professor at Brigham and Women’s Hospital. “You don’t get tissues with the same complexity as normal tissues.”<br /><br />The HST researchers built their “biological Legos” by encapsulating cells within a polymer called polyethylene glycol (PEG), which has many medical uses. Their version of the polymer is a liquid that becomes a gel when illuminated, so when the PEG-coated cells are exposed to light, the polymer hardens and encases the cells in cubes with side lengths ranging from 100 to 500 millionths of a meter.<br /><br />Once the cells are in cube form, they can be arranged in specific shapes using templates made of PDMS, a silicon-based polymer used in many medical devices. Both template and cell cubes are coated again with the PEG polymer, which acts as a glue that holds the cubes together as they pack themselves tightly onto the scaffold surface.<br /><br />After the cubes are arranged properly, they are illuminated again, and the liquid holding the cubes together solidifies. When the template is removed, the cubes hold their new structure.<br /><br />Gomez Fernandez and Khademhosseini used this method to build tubes that could function as capillaries, potentially helping to overcome one of the most persistent problems with engineered organs — lack of an immediate blood supply. “If you build an organ, but you can’t provide nutrients, it is going to die,” says Gomez Fernandez, now a postdoctoral fellow at Harvard. They hope their work could also lead to a new way to make artificial liver or cardiac tissue.<br /><br />Other researchers have developed a technique called organ printing to create complex 3-D tissues, but that process requires a robotic machine that is not in widespread use. The new technique does not require any special equipment. “You can reproduce this in any lab,” says Gomez Fernandez. “It’s very simple.”<br /><br />To get to the point where these engineered tissues could become clinically useful, “the short-term next step is really looking at different cell types and the viability of tissue growth,” says Elisseeff. The researchers are now doing that, and they are also exploring the use of different polymers that could replace PEG and offer more control over cell placement. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[When good enough is better]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/situationally-award-0513.html</link>
<story_id>15327</story_id>
<featured>0</featured>
<description><![CDATA[By exploiting a simple but counterintuitive trick, a new system finds sections of computer programs where accuracy can be traded for speed.]]></description>
<postDate>Thu, 13 May 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100512170952-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100512170952-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100512170952-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[“It used to be that people used computers for computations where there was a single, hard, logical right answer,” says Martin Rinard, a professor of computer science at MIT. “Now, the landscape is changing.” When you do a Google search, for instance, the exact order of the first few results may not matter as much as getting an answer quickly. In the Internet age, when Web servers are performing computations for thousands of users at once, and sending the results across thousands of miles of optical fiber, programs that can efficiently find adequate solutions to a problem are often preferable to ones that inefficiently find the perfect solution.<br /><br />Researchers in Rinard’s group at the Computer Science and Artificial Intelligence Laboratory have developed a system that automatically looks through computer code for areas where a little bit of accuracy can be traded for significant increases in speed. In one set of tests, the system halved the time that it takes to encode video data for transmission over the Internet, with no noticeable effect on the video quality (see the related video). But the same approach could have advantages for any system that needs to process data in real time, such as stock traders’ analytic software, or location-tracking or environmental-monitoring systems that use networks of sensors. It could also pay dividends for systems that need to look for patterns in huge masses of data, such as the recommendation engines at sites like Netflix or Amazon.<br /><br />The researchers unveiled their system last week, at the International Conference on Software Engineering in South Africa. In a <a href="http://people.csail.mit.edu/misailo/papers/MisailovicETAL10QosProfiling.pdf" target="_blank">paper they presented there</a>, they concentrated on a version of the system that alerts programmers to sections of their code where accuracy could be traded for speed. But the system could just as readily make that tradeoff automatically, on the fly. For instance, video-chat software running on a laptop could use the standard method of encoding data when the laptop’s processor was otherwise idle. But if the processor got overburdened trying to handle several applications at once, the video-chat software could switch to the less computationally intensive version of the encoder.<br /><br /><strong>Cutting corners</strong><br /><br />The researchers’ system exploits a remarkably simple trick. A large computer program will usually feature numerous instances of what’s called a loop — a process that’s repeated over and over again. Suppose that you were writing a program that needed to find the average of a long list of numbers. To begin with, the program would simply step through the list, adding each number to a running total: That’s the loop. Then it would divide the total by the length of the list.<br /><br />The CSAIL researchers who developed the new system — Rinard, research scientist Stelios Sidiroglou-Douskos and graduate students Hank Hoffmann and Sasa Misailovic — call their new technique “loop perforation” because it punches holes in loops: It simply skips every other step in the loop, or every two steps, or whatever it can get away with skipping without sacrificing too much accuracy. In the case of the program for averaging numbers, it might skip the first number on the list but add the second to the running total; skip the third but add the fourth; and so on. Since it’s adding up only half as many numbers, it takes only half as much time. But if the numbers follow a fairly normal probabilistic distribution, the answer will be close to what it would have been, anyway: The average height of 1,000 randomly selected Americans is probably not that far from the average height of 500 of them.<br /><br />The researchers’ system is itself a loop: it searches through a program, perforating each loop in turn, executing the program and using standard measures to gauge the effect on performance. It then determines which loops’ perforation provides the greatest increases in speed with the smallest drop in quality.<br /><br /><strong>Loopy insight</strong><br /><br />Loop perforation is so simple, and its effects so dramatic, that it may seem odd that it isn’t already in wide use. But for computer scientists, it’s very counterintuitive. “There’s a visceral sort of reaction against it,” says Hoffmann, “because people spend a lot of time thinking, ‘This is the best way to do this.’ And what we’re saying is that you can throw out half of what you thought was the best way to do that, and it still produces a reasonable answer.” Hoffmann recalls, for instance, that one of the applications on which the researchers tested their system was a machine-learning application, which learned to perform a classification task by looking for patterns in sample data. “Sasa knew a lot more than we did about what that app was doing,” Hoffmann says. When the system suggested the perforation of one particular loop, “Sasa was like, ‘This is wrong. You can’t do that.”  But the application seemed to work perfectly well with the perforated loop. “It seems like the closer you are to the application, the more reluctant you are to accept these things,” Hoffman says.<br /><br />“I like the simplicity of the technique and also the generality of it,” says Cristian Cadar, a lecturer in the Department of Computing at Imperial College London. “You can apply it to a wide variety of programs.” But, Cadar adds, “the main impediment to adoption of this technique, at least in the automatic form, is that developers are reluctant to adopt a technique where they don’t exactly understand what it does to the program.”<br /><br />To allay developers’ fears, Rinard and his group are working to explain the technique’s success with greater mathematical rigor. And since loop perforation is such uncharted territory, Rinard suggests, its theoretical exploration could have totally unforeseen consequences. “You never know what’s going to happen when you understand something,” he says. “The great aspect of science is these unpredictable, unanticipated breakthroughs that come just because you’re curious.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Directed evolution]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/explained-directed-evolution-0513.html</link>
<story_id>15320</story_id>
<featured>0</featured>
<description><![CDATA[Speeding up protein evolution in the lab can yield useful molecules that nature never intended.]]></description>
<postDate>Thu, 13 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100511104630-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100511104630-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100511104630-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Kelly Orcutt]]></imageCredits>
<imageCaption><![CDATA[An antibody (gray) produced through directed evolution binds to its target molecule.]]></imageCaption>
</image>
<body><![CDATA[In nature, evolution takes place over eons: A slow accumulation of adaptations gradually produces new traits and species. But evolution can also happen on a small and fast scale in the laboratory.<br /><br />The approach is called “directed evolution,” and scientists are using it to generate proteins that do not occur in nature — for example, cancer drugs, new microbial enzymes for converting agricultural waste to fuel, or imaging agents for magnetic resonance imaging (MRI).<br /><br />Most protein structures are so complex that it’s nearly impossible to predict how altering their structure will affect their function. So the trial-and-error approach of directed evolution is usually the fastest way to come up with a new protein with desirable traits, says Dane Wittrup, an MIT professor of chemical and biological engineering who uses directed evolution to discover new antibodies that target cancer cells.<br /><br />Such experiments often yield proteins that researchers never would have come up with on their own. “It’s like Christmas every morning,” says Wittrup.<br /><br />For example, let’s say you want to create an antibody that will bind to a certain protein found on tumor cells. You start with a test tube full of hundreds of millions of yeast cells, engineered to express a variety of mammalian antibodies on their surfaces. Then you add probes containing the molecule you want your new protein to target, allowing you to pick out the proteins that bind to it. <br /><br />Next, you take the proteins that bind the best and mutate them, in hopes of generating something even better. This can be done by irradiating the cells, or by forcing them to replicate their DNA in a way that is prone to mistakes. Those new proteins are screened the same way, and each time, the best candidates are used to create more proteins. “At the end, you have proteins that bind very tightly and specifically,” says Wittrup. “In the lab, it’s the same rules as natural evolution, but we get to set the criteria for who survives.”<br /><br />Wittrup and his students recently created a new antibody that binds tightly to tumor cells and to a radioactive compound used for chemotherapy, potentially allowing for very precise targeting of the cancer treatment.<br /><br />First developed about 15 years ago, directed evolution has become common in many labs, and easy enough that a first-year graduate student can produce a suitable protein in a couple of months, Wittrup says. <br /><br />He and others at MIT, including Bruce Tidor, professor of biological engineering and computer science, have also tried to design proteins more precisely, using computer models to predict how changes in a protein’s sequence will affect its structure and function. In 2007, their simulation successfully produced a new version of the cancer drug cetuximab that binds to its target with 10 times greater affinity than the original. However, this approach is very expensive and only works when researchers start with a great deal of information about the protein interactions being modeled.<br /><br />“In a limited way, we could do rational design,” says Wittrup. “Fifty years from now, maybe everyone will be doing that.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Interview: Jerome Milgram on oil spills]]></title>
<author><![CDATA[Jordan Lewis, Mechanical Engineering]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/qa-miligram-0512.html</link>
<story_id>15316</story_id>
<featured>0</featured>
<description><![CDATA[As the oil spill off the Louisiana coast widens, the mechanical engineering emeritus professor discusses floating booms, containment caps and more]]></description>
<postDate>Wed, 12 May 2010 12:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100510145003-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100510145003-1.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100510145003-1.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA]]></imageCredits>
<imageCaption><![CDATA[The oil slick as seen from space by NASA's Terra satellite on May 1.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='300' height='400'>http://web.mit.edu/newsoffice/images/article_images/20100512130006-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: MIT Museum]]></imageCredits>
<imageCaption><![CDATA[Department of Mechanical Engineering Emeritus Professor Jerome (Jerry) Milgram]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<em>Emeritus Professor Jerome (Jerry) Milgram has researched the science of oil spills on the ocean surface and in deep water wells since 1970. He has overseen the containment of ruptured wells in the Gulf of Mexico and holds many patents for ameliorating, containing and collecting ocean spills. In a recent phone interview with Jordan Lewis of the Department of Mechanical Engineering, Milgram discussed current efforts to contain the widening oil spill off the Louisiana coast. </em><br /><br /><strong>Q.</strong> What are the possible measures for stopping the flow of oil from a ruptured underwater well?   <br /><br /><strong>A.</strong> To stop the flow, the engineers would like to quickly drill a relief well. This would involve a separate well drilled in a strategic location into the reservoir formation.  Once the well is functioning, a series of quick, high-pressure steps are followed to “kill” the well. First, salt water is pumped into the reservoir, followed by mud and finally concrete. The mud and concrete will seal the space between the oil-bearing formation and the sea bottom if the process is successful. What is unique about the presently leaking well is that it is in very deep water in the range of 5,000 feet. This depth will increase the complexity and overall time required to complete necessary steps. <br /><br /><strong>Q.</strong> The engineers have attempted to use a cone shape “cap” to cover the well and control the flow. Are you familiar with this technique and will it be effective?<br /><br /><strong>A.</strong> I was involved in using a “cap” in 1979 to control the IXTOC oil well blowout in the Gulf of Mexico, roughly 50 kilometers north from Ciudad del Carmen, Mexico. I had flow- and pressure-measuring equipment on the device. In that incident, there was a large rupture on the seafloor and the spill into the Gulf was massive. The mitigation crews from the oil company lowered a large, concrete, conical shaped “funnel” onto the ruptured well-head to try to control the flow out of the well. In this case, the water depth was about 200 feet deep, much shallower than the recent blowout. In each case the oil and natural gas bearing formation was thousands of feet beneath the seabed. <br /><br />Once the cap or containment dome was “placed,” there were several issues that affected the operation. First, the cap bottom was somewhat elevated above the ocean floor and the gap allowed for excessive water to enter the system and for excessive oil and gas to escape from it. Other problems included lack of a large ship into which the collect oil and water could be pumped.   <br /><br /><strong>Q. </strong>The first containment cap used to cover the ruptured well was unsuccessful due to the problems underneath the submerged cap. Can you explain what happened and if this can be avoided?<br /> <br /><strong>A.</strong> When methane, a major component of natural gas, and water are present at low temperature and high pressure, they combine to form solid crystals. Each hydrate crystal is made up of a cage of water molecules filled with a variable number of methane molecules. New oil wells often release a considerable amount of natural gas along with the oil. The hydrates form groups that appear like snowflakes and have an ice-like consistency when their concentration in the water is low. With high concentrations of hydrates in the water, the result has a consistency of snow-ice-water slush. The slush is known to be rigid enough to clog pipelines in some circumstances. <br /> <br />In the case of the first attempted containment cap, the concentration of gas hydrate clogged either the hole at the top of cap, or a short section of pipe that was attached to the top of the cap. In a normally functioning oil riser, the hydrates would decompose into natural gas and water in the upper part of the riser where the pressure is lower. This would form a powerful gas-lift pump. I am not in a position to say if such a system could work effectively with a very large amount of gas hydrate entering at the bottom of the riser, seeping in from under the cap.<br /><br /><strong>Q.</strong> We see in the news that oil is making landfall and the inflatable booms are often shown awash on the beaches. How effective are the inflatable booms at corralling the oil?  <br /><br /><strong>A.</strong> Inflatable booms are capable of slowing the spread of oil and concentrating the spill within the arc of their reach. Under appropriate circumstances, use of booms increases evaporation of the lighter fractions of the oil before the slick comes ashore. To be most effective, the booms need to work in connection with a skimming mechanism to retrieve the oil sitting on top of the water. This is often done with a barge or tanker that has plumbing capable of recovering and holding the oil/water mix as it is removed from the ocean’s surface. Booms are not perfect. The motion of the waves allows for some amount of oil to seep beneath the boom and bypass the containment system — that is inevitable. The ideal boom would have the oil-skimming ability built into them, but the cost of booms with such a capability is significantly greater than that of commonly used booms. <br /><br /><strong>Q.</strong> Planes have been dropping a chemical dispersant onto the oil slicks. Is it better to disperse the oil or attempt to corral the oil? These two ideas tend to compete with one another.<br /><strong><br />A.</strong> Dispersants have benefits. In theory, they break up the clumps within the oil sheen and increase the surface area of the oil. With greater surface area, small oil-dispersant water droplets can be distributed throughout the water column allowing for a higher rate of bacterial degradation and evaporation of the oil. Both of these processes — evaporation and bacterial decomposition — occur naturally, and that is what is being encouraged. Unfortunately, these are slow-acting processes, and they depend heavily on the oil composition, sunlight, movement of the waves and other factors. Again, with the movement of the waves, this is good for mixing the dispersants but troublesome for the booms attempting to contain the oil. Additionally, we are not very familiar with the negative implications of adding 100,000-plus gallons of dispersants into the ocean. The environmental impact is not fully understood at this time.<br /><br /><strong>Q.</strong> Some critics would claim that the response to the recent spill has been slow or ineffective. How would you respond to that?  <br /><strong><br />A.</strong> Preparing for an event of this magnitude is nearly impossible. Maintaining a stand-by crew of specialists that would respond to a ruptured well is just not going to be realistic. The time between incidents and the cost of the specialized equipment/training is far too great to just have waiting around. Also, it is important to note that the circumstances for all ruptured wells are very different. From what I’ve seen and read in the news, the response so far has been addressing the problem. What I have not seen in the media are any tankers or barges filled with oil that has been skimmed from the surface. Ideally, oil from the slicks would be captured and brought to shore for processing.<br /><br /><strong>Q.</strong> Can the oil from the spill be used after it has been mixed with salt water?  <br /><br /><strong>A. </strong>Yes, there are still many uses for the oil but in most cases all the water and salt must be removed from it — a difficult, expensive and time-consuming task in view of the large quantity of material. With the quantities of oil present in the Gulf, tremendous storage capacity would be required to hold the oil/water mixture before it could be processed. I am not sure if there is a facility in the United States capable of the task. A refinery that has this oil/water separation capability is something worthy of consideration for future responses.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[NASA chief defends Obama’s space plan]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/bolden-visit-0512.html</link>
<story_id>15319</story_id>
<featured>0</featured>
<description><![CDATA[In MIT visit, Charles Bolden touts proposed expansion in R&D funding]]></description>
<postDate>Wed, 12 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100511163938-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100511163938-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100511163938-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Bill Litant]]></imageCredits>
<imageCaption><![CDATA[NASA Administrator Charles F. Bolden Jr. discusses the agency’s future during a visit to MIT on Monday, May 10.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100511163938-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Bill Litant]]></imageCredits>
<imageCaption><![CDATA[Phillip Cunio, an AeroAstro doctoral student, explains the TALARIS (Terrestrial Artificial Lunar and Reduced Gravity Simulator) project to Bolden. The MIT student project is aimed at designing, building and operating a prototype test vehicle for robotic lunar landers as part of MIT's participation in the Google Lunar X-Prize.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In a lecture on Monday at MIT, NASA Administrator Charles F. Bolden Jr. defended President Barack Obama’s controversial plans for the U.S. space agency’s future and touted the president’s plan to invest billions of dollars in basic science research.<br /><br />Some in Congress have criticized Obama’s proposal to cancel the Constellation program, which would have sent humans to the moon by 2020, saying such a move will effectively cede U.S. space leadership to other nations. But Bolden noted that the White House’s plan would also invest an additional $6 billion in NASA over the next five years, including a 60-percent increase in earth sciences research funding, as well as a 20-percent increase in planetary sciences research. Such an expansion could revitalize NASA’s ties with institutions like MIT, which has played an instrumental role in the agency since NASA was founded in 1958.<br /><br />“The frustration for me is that we always talk about the cancellation of Constellation,” former astronaut Bolden said of his appearances before Congress and interviews with the media, in which he has been grilled over the president’s plan. “But we are adding an incredible amount of money for research.”<br /><br />Bolden was at MIT to deliver the 20th annual Massachusetts Space Grant Consortium (MSGC) public lecture, titled “Looking Ahead to the Future of NASA.” Headquartered at MIT, the MSGC’s primary goal is to represent NASA in Massachusetts by supporting space exploration and research by the state’s students and teachers. <br /><br />David Mindell, the Frances and David Dibner Professor of the History of Engineering and Manufacturing and director of MIT’s Program in Science, Technology, and Society, said that Bolden’s defense of Obama’s proposal wasn’t surprising, given that it’s his job to do so. “But the proposal does recommend a fresh approach that, though risky, could reinvigorate human spaceflight in the U.S. and restart research — at MIT and many other places — that had been sacrificed for the Constellation program,” he said.<br /><br />Professor of Aeronautics and Astronautics and Engineering Systems Dava Newman agreed, saying that the proposal would strengthen — not weaken — U.S. leadership in space. “The budget for science, engineering and technology development, testbeds and flight experiments is extraordinary, and if realized, will help NASA once again become the agency to realize exploration (both human and robotic) and major technological breakthroughs both in space and here on Earth,” Newman said. “It's very exciting to think of this future investment in science, engineering and technology and to think that MIT students and faculty will be part of the community to shape NASA's future and to realize this vision.<br /><br /><strong>Looking ahead</strong><br /><br />During his talk, Bolden said NASA was going through what he called a “difficult, but very interesting” period. As a former astronaut who completed four space flights, Bolden expressed sadness about the prospect of ending NASA’s space-shuttle fleet, admitting he is “emotionally attached” to the shuttle program. But he insisted that NASA is “committed” to Obama’s new era of space exploration, which calls for a flexible path approach for NASA to gain progressively more experience, such as a lunar fly-by or exploration of asteroids, before making a trip to Mars. The plan also calls for developing a “heavy-lift” system to launch spacecraft into deep space, as well as technologies to protect humans from long-term radiation. In the future, NASA would lease vehicles from private companies to ferry astronauts to and from the International Space Station.<br /><br />“The president, with my full agreement, made a change — a big change,” Bolden said of Obama’s decision to undertake a new direction for NASA, adding that the agency’s fundamental goal “to boldly advance the human presence beyond the cradle of Earth,” has not changed, and that Mars remains an “especially compelling target.” <br /><br />Bolden outlined several tracks that NASA has proposed to achieve its goals, such as developing robotic technologies to scout new targets and test precision landings. He said the agency remains focused on using the International Space Station to learn more about human health issues, referring to ongoing work by ISS researchers to develop a salmonella vaccine.<br /><br />He pledged NASA’s commitment to develop a commercial launch industry for carrying humans into low Earth orbit, but said that the agency was still fine-tuning specific operations details, such as whether a crew would be trained at NASA facilities. He also said the agency was honoring Obama’s request to collaborate with other countries like Saudi Arabia to foster science research.<br /><br />When pressed to name a timetable for a manned mission to Mars, Bolden said it was “pretty vague,” but that if NASA started to develop the architecture for a heavy-lift launch vehicle right now, it could be as soon as the early 2020s that a spacecraft orbits the moon, and maybe 2025 for a spacecraft or robot to land on an asteroid. Those advances could make travel to Mars a reality by 2030, he said.<br /><br />Regardless of a timetable, Bolden insisted that NASA’s future must include increased collaboration with research institutions like MIT, noting that, “we can’t carry out this work without engaging the public.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: The euro mess]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/3q-forbes-euro-0511.html</link>
<story_id>15318</story_id>
<featured>0</featured>
<description><![CDATA[MIT Sloan economist Kristin Forbes on the recent troubles roiling the international financial markets]]></description>
<postDate>Tue, 11 May 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100510174712-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100510174712-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100510174712-0.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100510172102-2.jpg</fullURL>
<imageCaption><![CDATA[Kristin Forbes PhD ’98, the Jerome and Dorothy Lemelson Professor of Management at the MIT Sloan School of Management]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<em>Over the weekend, European political leaders and central bankers finally fashioned a package of measures designed to ward off the sovereign-debt problems — felt most acutely in Greece — that had shaken markets in recent weeks. The underlying economic and fiscal challenges facing some European Union countries remain in place, however. MIT News asked international finance scholar Kristin Forbes PhD ’98, the Jerome and Dorothy Lemelson Professor of Management at the MIT Sloan School of Management, about the situation in Europe. From 2003 to 2005, Forbes served as a member of the White House’s Council of Economic Advisors, under President George W. Bush, and she worked in the U.S. Treasury Department from 2001 to 2002; she is also a research associate at the National Bureau of Economic Research. </em><br /><br /><strong>Q.</strong> With world markets in tumult last week, there was a lot of talk of “contagion” stemming from fears about the situation in Greece and Europe. Your research has indicated multiple reasons why markets react this way, so in this case, what accounted for the phenomenon?<br /> <br /><strong>A.</strong> Contagion from the situation in Greece should not have come as a surprise. There are a number of mechanisms that transmit crises from one country to another, i.e., that cause contagion. One form of contagion that has been particularly powerful in this crisis is a “wake-up call” effect.  The main idea is that events in one country can provide information about risks in other countries. If a country with certain macroeconomic characteristics is discovered to be susceptible to a crisis, then investors “wake up” and reassess other countries as having similar risks. Investors now realize that even though Greece is in the euro area, its large budget deficit, high debt levels, and uncompetitive cost structure will mean a prolonged period of slow (or even negative) growth and probably a debt default. This has made investors more aware that being in the eurozone does not guarantee economic stability, and that other countries with large deficits and debt and uncompetitive cost structures could also be in store for prolonged recessions and even debt restructuring.<br /><br /><strong>Q.</strong> In that case, to what extent do other EU countries still face these dangers?<br /><br /><strong>A.</strong> Greece’s problems are certainly not unique — although they are more severe than in the other EU nations. Greece has run large budget deficits and seen wages and costs increase rapidly for years. This has eroded Greece’s competitiveness and led to a huge government debt burden of about 115 percent of GDP (and external debt burden of about 75 percent of GDP) at the end of 2009. Other countries, such as Portugal, Ireland, Italy and Spain (commonly referred to as the PIIGS when Greece is included) share many of these characteristics. All of these countries have high costs relative to other major members of the EU such as Germany. This means that their exports are not competitive, and they import more than they export, which they fund by borrowing from abroad. The most recent global crisis aggravated total government borrowing, and it has become increasingly difficult for these countries to keep finding people willing to finance their debts. <br /><br />But on a more positive note, Portugal, Ireland, Italy and Spain are not yet in nearly the predicament of Greece. For example, Portugal, which is probably the most vulnerable of the group outside of Greece, still had a government debt burden of only 77 percent of GDP at the end of 2009 (although its external debt was slightly higher than Greece’s at about 95 percent of GDP). Ireland started with lower government and external debt ratios and has already made a series of very difficult spending cuts and tax increases — a sharp contrast to Greece’s slow response and violent protests, which suggest Greece may have more difficulty implementing its fiscal consolidation. Italy’s budget deficit is less than half that of Greece. Hopefully these other nations will use this period to reduce their budget deficits and regain competitiveness so that they can avoid being in as difficult a situation as Greece is today.<br /><br /><strong>Q.</strong> The Economist recently said that Europe’s leaders have “three years to save the euro.” What is the future of the euro, since countries like Greece and Germany are in such different fiscal situations? And does it make sense for Greece to stick with the euro?<br /><br /><strong>A.</strong> Although staying within the euro will force Greece to make a number of difficult adjustments, this is the “least bad” of a number of very bad options. If Greece left the euro, borrowing costs would spike even further, Greece could no longer finance its deficits, investors would likely panic and sell Greek stocks and bonds, and the Greek banking system could easily collapse. Greece has no option except to stay in the euro. Another question is whether the countries with sounder fiscal positions —  such as Germany — will want to remain in the euro if it involves paying for the past profligacy of Greece. Will German taxpayers — who have been more responsible savers and seen low wage growth for years — be willing to write a series of checks to Greece? The German government realizes that keeping the Euro together is important for economic and political stability in Europe — as well as to support key markets that buy German exports. But if this situation continues for years, it will be interesting to see how long German taxpayers will continue to support this arrangement.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Mysterious quantum forces unraveled]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/casimir-0511.html</link>
<story_id>15317</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers find a way to calculate the effects of Casimir forces, offering a way to keep micromachines’ parts from sticking together.]]></description>
<postDate>Tue, 11 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100510174608-0.png</thumbURL>
<smallURL width='140' height='171'>http://web.mit.edu/newsoffice/images/article_images/w140/20100510174608-0.jpg</smallURL>
<fullURL width='368' height='449'>http://web.mit.edu/newsoffice/images/article_images/20100510174608-0.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Alejandro Rodriguez]]></imageCredits>
<imageCaption><![CDATA[New computational techniques developed at MIT confirmed that the complex quantum effects known as Casimir forces would cause tiny objects with the shapes shown here to repel each other rather than attract.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100510172752-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Sandia National Laboratories]]></imageCredits>
<imageCaption><![CDATA[Though negligible at larger scales, Casimir forces can cause the moving parts of micromachines, like the one shown here, to stick together.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Discovered in 1948, Casimir forces are complicated quantum forces that affect only objects that are very, very close together. They’re so subtle that for most of the 60-odd years since their discovery, engineers have safely ignored them. But in the age of tiny electromechanical devices like the accelerometers in the iPhone or the micromirrors in digital projectors, Casimir forces have emerged as troublemakers, since they can cause micromachines’ tiny moving parts to stick together. <br /><br />MIT researchers have developed a powerful new tool for calculating the effects of Casimir forces, with ramifications for both basic physics and the design of microelectromechanical systems (MEMS). One of the researchers’ most recent discoveries using the new tool was a way to arrange tiny objects so that the ordinarily attractive Casimir forces become repulsive. If engineers can design MEMS so that the Casimir forces actually prevent their moving parts from sticking together — rather than causing them to stick — it could cut down substantially on the failure rate of existing MEMS. It could also help enable new, affordable MEMS devices, like tiny medical or scientific sensors, or microfluidics devices that enable hundreds of chemical or biological experiments to be performed in parallel.<br /><br /><strong>Ghostly presence</strong><br /><br />Quantum mechanics has bequeathed a very weird picture of the universe to modern physicists. One of its features is a cadre of new subatomic particles that are constantly flashing in and out of existence in an almost undetectably short span of time. (The Higgs boson, a theoretically predicted particle that the Large Hadron Collider in Switzerland is trying to detect for the first time, is expected to appear for only a few sextillionths of a second.) There are so many of these transient particles in space — even in a vacuum — moving in so many different directions that the forces they exert generally balance each other out. For most purposes, the particles can be ignored. But when objects get very close together, there’s little room for particles to flash into existence between them. Consequently, there are fewer transient particles in between the objects to offset the forces exerted by the transient particles around them, and the difference in pressure ends up pushing the objects toward each other.<br /><br />In the 1960s, physicists developed a mathematical formula that, in principle, describes the effects of Casimir forces on any number of tiny objects, with any shape. But in the vast majority of cases, that formula remained impossibly hard to solve. “People think that if you have a formula, then you can evaluate it. That’s not true at all,” says Steven Johnson, an associate professor of applied mathematics, who helped develop the new tools. “There was a formula that was written down by Einstein that describes gravity. They still don’t know what all the consequences of this formula are.” For decades, the formula for Casimir forces was in the same boat. Physicists could solve it for only a small number of cases, such as that of two parallel plates. In recent years, researchers around the world attacked the problem of finding Casimir forces between more general shapes and materials. For instance, in 2006, MIT physics professors Robert Jaffe and Mehran Kardar — with whom Johnson continues to collaborate — and Thorsten Emig of the University of Köln in Germany showed how to calculate the forces acting between a plate and a cylinder; the next year, they demonstrated solutions for multiple spheres. Meanwhile, Johnson and his collaborators explored various numerical methods that can be applied to a wide variety of geometries. However, the full power of existing tools for classical electromagnetic calculations had not yet been brought to bear on the Casimir problem.<br /><br /><strong>The power of analogy</strong><br /><br />In a paper appearing this week in <em>Proceedings of the National Academy of Sciences</em>, Johnson, physics PhD students Alexander McCauley and Alejandro Rodriguez (the paper’s lead author), and John Joannopoulos, the Francis Wright Davis Professor of Physics, describe a way to solve Casimir-force equations for any number of objects, with any conceivable shape.<br /><br />The researchers’ insight is that the effects of Casimir forces on objects 100 nanometers apart can be precisely modeled using objects 100,000 times as big, 100,000 times as far apart, immersed in a fluid that conducts electricity. Instead of calculating the forces exerted by tiny particles flashing into existence around the tiny objects, the researchers calculate the strength of an electromagnetic field at various points around the much larger ones. In their paper, they prove that these computations are mathematically equivalent.<br /><br />For objects with odd shapes, calculating electromagnetic-field strength in a conducting fluid is still fairly complicated. But it’s eminently feasible using off-the-shelf engineering software.<br /><br />“Analytically,” says Diego Dalvit, a specialist in Casimir forces at the Los Alamos National Laboratory, “it’s almost impossible to do exact calculations of the Casimir force, unless you have some very special geometries.” With the MIT researchers’ technique, however, “in principle, you can tackle any geometry. And this is useful. Very useful.” <br /><br />Since Casimir forces can cause the moving parts of MEMS to stick together, Dalvit says, “One of the holy grails in Casimir physics is to find geometries where you can get repulsion” rather than attraction. And that’s exactly what the new techniques allowed the MIT researchers to do. In a <a href="http://web.mit.edu/~alexrod7/www/papers/LevinMc10.pdf" target="_blank">separate paper published in March</a>, physicist Michael Levin of Harvard University’s Society of Fellows, together with the MIT researchers, described the first arrangement of materials that enable Casimir forces to cause repulsion in a vacuum.<br /><br />Dalvit points out, however, that physicists using the new technique must still rely on intuition when devising systems of tiny objects with useful properties. “Once you have an intuition of what geometries will cause repulsion, then the [technique] can tell you whether there is repulsion or not,” Dalvit says. But by themselves, the tools cannot identify geometries that cause repulsion.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Moving in circles]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/briggs-poverty-0510.html</link>
<story_id>15313</story_id>
<featured>0</featured>
<description><![CDATA[MIT scholar’s new book scrutinizes the successes and failures of a unique government experiment meant to help America’s urban poor.]]></description>
<postDate>Mon, 10 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100507155608-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100507155608-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100507155608-1.jpg</fullURL>
<imageCaption><![CDATA[The cover to Xavier de Souza Briggs's book, "Moving to Opportunity: The Story of an American Experiment to Fight Ghetto Poverty."]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='502'>http://web.mit.edu/newsoffice/images/article_images/20100509220503-1.jpg</fullURL>
<imageCaption><![CDATA[Xavier de Souza Briggs, associate professor of sociology and urban planning and associate director of the White House's Office of Management and Budget.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In 1994, the United States government launched an ambitious social experiment: Federal officials offered thousands of families who were receiving housing subsidies — and living in some of the nation's highest-crime neighborhoods — the opportunity to move away.<br /><br />And so, housing vouchers in hand, about 5,000 American families in five cities — Baltimore, Boston, Chicago, Los Angeles and New York — moved to nicer urban and suburban locales, looked for jobs, entered new schools, and renewed their lives, courtesy of the government’s $80 million “Moving to Opportunity” (MTO) program. <br /><br />However, geography is not destiny. The MTO experiment did not so much transform lives as produce unanticipated results: It helped girls more than boys, for instance. Parents enjoyed major drops in anxiety and depression — but not gains in incomes. All told, better surroundings appear valuable but not sufficient conditions for climbing out of poverty.<br /><br />“Changing your neighborhood does not change your social world,” says Xavier de Souza Briggs, an associate professor of sociology and urban planning at MIT. Now Briggs has co-authored a new book analyzing the program, <em>Moving to Opportunity: The Story of an American Experiment to Fight Ghetto Poverty</em>, along with Susan Popkin a senior fellow at the Urban Institute in Washington, and John Goering, a professor at Baruch College in New York; the volume was published by Oxford University Press this spring.<br /><br /><strong>Safer, not richer</strong><br /><br />MTO was constructed as a randomized scientific experiment. MTO families received housing vouchers while another set of people — the “control group” — remained in public housing. But as Briggs notes, the persistence of social networks was a powerful force shaping the lives of MTO families even after they left the inner city. Teenage boys often visited their old neighborhoods and fell back into gangs; needy relatives still weighed on the MTO families. <br /><br />“Boys in particular remain embedded in wider networks and frequently can’t free themselves from violence and drugs,” explains Briggs, speaking from Washington, where he is currently working in the White House as associate director of the Office of Management and Budget. On the other hand, young women benefitted from a lessened danger of physical assault and sexual abuse. “It gives us reason to think the exposure of women to predatory behavior, and the resulting freedom from the fear of predation due to MTO, is very important,” adds Briggs. <br /><br />All told, MTO families moved into areas with a collective violent-crime rate that was just one-quarter of the rate in the urban neighborhoods they left behind. Yet as the book notes, four to seven years after MTO started, there were “no significant impacts on employment, earnings, or receipt of public assistance.” Later evaluations of MTO did find some “modest” improvements. Working women under 33 who moved earned $33 more per week (showing the program’s unequal gender effects). MTO adults who moved to suburban neighborhoods earned about $75 per week more. <br /><br />But as the authors state, leaving crime-ridden areas “did not, in fact, mean relocating to a job-rich zone” for most MTO families, largely due to “backbreaking” rents in safer neighborhoods. Nationally, 6 million households spend more than half their income on rent or live in “severely substandard” housing, up from 3.6 million in 1978. In the case of MTO, the vouchers helped the families pay rent, but did not wholly subsidize rents. During a time period, after 1994, when housing prices rose dramatically in the metropolitan areas used in the experiment, many MTO families moved back to their old neighborhoods because they could not afford to live in thriving areas. <br /><br /><strong>What is to be done?</strong><br /><br />Other researchers are impressed that Briggs, Popkin and Goering have turned the data-intensive project into a book-length narrative, which is based heavily on interviews they conducted with MTO participants. “It helps readers understand the problems of ghetto poverty, and tells the story of MTO, warts and all,” says Margery Austin Turner, a vice president at the Urban Institute, a nonprofit research center in Washington.<br /><br />Turner, a former official in the Department of Housing and Urban Development (which oversaw MTO), acknowledges that relocation has not by itself been a panacea for poverty. But she defends MTO, calling it “a rigorous test of some hypotheses.” The experiment, Turner says, was “a success because we’ve learned a tremendous amount from it about how complex the lives of low-income families are.” <br /><br />That complexity leaves Briggs and his colleagues contemplating a lengthy list of policy ideas to address urban poverty in the future, beyond housing. The parents in MTO were often under severe emotional stress; Briggs believes the implementation of universal health-care coverage in 2013 will help the poor get treatment for their mental-health problems.<br /><br />Moreover, the MTO families were so preoccupied with safety, Briggs says, they also struggled to evaluate whether or not the schools in their new neighborhoods would be a good match for their children, something relatively low-cost initiatives might remedy. “People are often information-poor, not just money-poor,” says Briggs. “We should try to coach people on assessing schools.” <br /><br />Similarly, Briggs adds, the poor urban parents in MTO needed better job-seeking skills. “Rich, poor, white, black, brown — all people have similar ideas about what it takes to get ahead,” says Briggs. “But they differ in knowing about self-presentation, how to succeed in job interviews, how to go about networking. These are foreign ideas to many people.” <br /><br />“Moving to Opportunity” thus starts with housing, but ends with a broader focus. MTO’s “costs and risks are low,” the authors write. But as Briggs and his colleagues acknowledge, one tool for fighting poverty is not enough. In the future it will be necessary “to make relocation part of a larger recipe for getting ahead — for escaping poverty.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[New project aims for fusion ignition]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/fusion-ignition-0510.html</link>
<story_id>15311</story_id>
<featured>0</featured>
<description><![CDATA[MIT-led Ignitor reactor could be the world’s first to reach major milestone, perhaps paving the way for eventual power production.]]></description>
<postDate>Mon, 10 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100507160514-1.png</thumbURL>
<smallURL width='140' height='182'>http://web.mit.edu/newsoffice/images/article_images/w140/20100507160514-1.jpg</smallURL>
<fullURL width='368' height='479'>http://web.mit.edu/newsoffice/images/article_images/20100507160514-1.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Bruno Coppi]]></imageCredits>
<imageCaption><![CDATA[Exterior view of the Ignitor fusion reactor, whose core will be built in Italy and external housing built outside Moscow, where it will be installed.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='437'>http://web.mit.edu/newsoffice/images/article_images/20100507160516-2.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Bruno Coppi]]></imageCredits>
<imageCaption><![CDATA[Internal cutaway view of the reactor, showing the donut-shaped cavity that houses the hot plasma where fusion will take place.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Russia and Italy have entered into an agreement to build a new fusion reactor outside Moscow that could become the first such reactor to achieve ignition, the point where a fusion reaction becomes self-sustaining instead of requiring a constant input of energy. The design for the reactor, called Ignitor, originated with MIT physics professor Bruno Coppi, who will be the project’s principal investigator.<br /><br />The concept for the new reactor builds on decades of experience with MIT’s Alcator fusion research program, also initiated by Coppi, which in its present version (called Alcator C-Mod) has the highest magnetic field and highest plasma pressure (two of the most important measures of performance in magnetic fusion) of any fusion reactor, and is the largest university-based fusion reactor in the world.<br /><br />The key ingredient in all fusion experiments is plasma, a kind of hot gas made up of charged particles such as atomic nuclei and electrons. In fusion reactors, atomic nuclei — usually of isotopes of hydrogen called deuterium and tritium — are forced together through a combination of heat and pressure to overcome their natural electrostatic repulsion. When the nuclei join together, or fuse, they release prodigious amounts of energy. <br /><br />Ignitor would be about twice the size of Alcator C-Mod, with a main donut-shaped chamber 1.3 meters across, and have an even stronger magnetic field. It will be much smaller and less expensive than the major international fusion project called ITER (with a chamber 6.2 meters across), currently under construction in France. Though originally designed to achieve ignition, the ITER reactor has been scaled back and is now not expected to reach that milestone.<br /><br />The Ignitor reactor, Coppi says, will be “a very compact, inexpensive type of machine,” and unlike the larger ITER could be ready to begin operations within a few years. Its design is based on a particularly effective combination of factors that researchers  unexpectedly discovered during the many years of running the Alcator program, and that were later confirmed in experiments at other reactors. Together, these factors produce especially good confinement of the plasma and a high degree of purity (impurities in the hot gases can be a major source of inefficiency). The new design aims to preserve these features to produce the highest plasma current densities — the amount of electric current in a given area of plasma. The design also has additional structures needed to produce and confine burning fusion plasmas in order to create the conditions needed for ignition, Coppi says. <br /><br />A project at Lawrence Livermore National Laboratory in California, called the National Ignition Facility, uses a completely different approach to fusion, bombarding a tiny pellet of fuel with laser beams to create the heat and pressure needed for fusion. But some physicists disagree on the exact meaning of fusion ignition, and whether the results of the planned experiments at NIF can truly be considered ignition. ”I don’t understand how they define ignition,” Coppi says of what is being planned at NIF.<br /><br />Coppi plans to work with the Italian ministry of research and Evgeny Velikhov, president of the Kurchatov Institute in Moscow, to finalize the distribution of tasks for the machine, the core of which is to be built in Italy and then installed in Troitsk, near Moscow, on the site of that institute’s present Triniti reactor. Velikhov, as it happens, is also the chair of the ITER council. Coppi says of these two different programs,  “there’s no competition, we are complementary.”<br /><br />Although seen as a possible significant contributor to the world’s energy needs because it would be free of greenhouse-gas emissions, practical fusion power remains at least two decades away, most scientists in the field agree. But the initial impetus for setting up the Alcator reactor in the 1970s had more to do with pure science: “It was set up to simulate the X-ray stars that we knew at that time,” says Coppi, whose research work has as much to do with astrophysics as with energy. Stars are themselves made of plasma and powered by fusion, and the only way to study their atomic-level behavior in detail is through experiments inside fusion reactors. <br /><br />Once the reactor was in operation, he says, “we found we were producing plasmas with unusual properties,” and realized this might represent a path to the long-sought goal of fusion ignition. <br /><br />Roscoe White, a distinguished research fellow at the Princeton Plasma Physics Laboratory, says that “the whole point of Ignitor is to find out how a burning plasma behaves, and there could be pleasant or unpleasant results coming from it. Whatever is learned is a gain.  Nobody knows exactly how it will perform, that is the point of the experiment.” But while its exact results are unknown, White says it is important to pursue this project in addition to other approaches to fusion. “With our present knowledge it is very risky to commit the program to a single track reactor development — our knowledge is still in flux,” he says.<br /><br />In addition, he says, “the completion of ITER, the only currently projected burning plasma experiment, is decades off. Experimental data concerning a burning plasma would be very welcome, and could lead to important results helping the cause of practical fusion power.” Furthermore, the Ignitor approach, if all goes well, could lead to more compact and economical future reactors: Some recent results from existing reactors, plus new information to be gained from Ignitor, “could lead to reactor designs much smaller and simpler than ITER,” he says.<br /><br />Coppi remains especially interested in the potential of the new reactor to make new discoveries about fundamental physics. Quoting the late MIT physicist and Institute Professor Bruno Rossi, Coppi says, “whenever you do experiments in an unknown regime, you will find something new.” The new machine’s findings, he suggests, “will have a strong impact on astrophysics.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Seeing the forest for the trees]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/object-recog-0507.html</link>
<story_id>15308</story_id>
<featured>0</featured>
<description><![CDATA[Object recognition systems that break images into ever smaller parts should be much more efficient and may shed light on how the brain works.]]></description>
<postDate>Fri, 07 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100506170429-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100506170429-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100506170429-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='199' height='511'>http://web.mit.edu/newsoffice/images/article_images/20100506170429-2.jpg</fullURL>
<imageCredits><![CDATA[Images: Long (Leo) Zhu]]></imageCredits>
<imageCaption><![CDATA[A new object recognition system developed by researchers at MIT and UCLA looks for rudimentary visual features shared by multiple examples of the same object. Then it looks for combinations of those features shared by multiple examples, and combinations of those combinations, and so on, until it has assembled a model of the object that resembles a line drawing.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Object recognition is one of the core topics in computer vision research: After all, a computer that can see isn’t much use if it has no idea what it’s looking at. Researchers at MIT, working with colleagues at the University of California, Los Angeles, have developed new techniques that should make object recognition systems much easier to build and should enable them use computer memory more efficiently.<br /><br />A conventional object recognition system, when trying to discern a particular type of object in a digital image, will generally begin by looking for the object’s salient features. A system built to recognize faces, for instance, might look for things resembling eyes, noses and mouths and then determine whether they have the right spatial relationships with each other. The design of such systems, however, usually requires human intuition: A programmer decides which parts of the objects are the right ones to key in on. That means that for each new object added to the system’s repertoire, the programmer has to start from scratch, determining which of the object’s parts are the most important.<br /><br />It also means that a system designed to recognize millions of different types of objects would become unmanageably large. Each object would have its own, unique set of three or four parts, but the parts would look different from different perspectives, and cataloguing all those perspectives would take an enormous amount of computer memory.<br /><br />In a paper that they’ll present at the Institute of Electrical and Electronics Engineers’ Conference on Computer Vision and Pattern Recognition in June, postdoc Long (Leo) Zhu and Professors Bill Freeman and Antonio Torralba, all of MIT’s Computer Science and Artificial Intelligence Laboratory, and Yuanhao Chen and Alan Yuille of UCLA describe an approach that solves both of these problems at once. Like most object-recognition systems, their system learns to recognize new objects by being “trained” with digital images of labeled objects. But it doesn’t need to know in advance which of the objects’ features it should look for. For each labeled object, it first identifies the smallest features it can — often just short line segments. Then it looks for instances in which these low-level features are connected to each other, forming slightly more sophisticated shapes. Then it looks for instances in which these more sophisticated shapes are connected to each other, and so on, until it’s assembled a hierarchical catalogue of increasingly complex parts whose top layer is a model of the whole object.<br /><strong><br />Economies of scale</strong><br /><br />Once the system has assembled its catalogue from the bottom up, it goes through it from the top down, winnowing out all the redundancies. In the parts catalogue for a horse seen in profile, for instance, the second layer from the top might include two different representations of the horse’s rear: One could include the rump, one rear leg and part of the belly; the other might include the rump and both rear legs. But it could turn out that in the vast majority of cases where the system identifies one of these “parts,” it identifies the other as well. So it will simply cut one of them out of its hierarchy.<br /><br />Even though the hierarchical approach adds new layers of information about digitally depicted objects, it ends up saving memory because different objects can share parts. That is, at several different layers, the parts catalogues for a horse and a deer could end up having shapes in common; to some extent, the same probably holds true for horses and cars. Wherever a shape is shared between two or more catalogues, the system needs to store it only once. In their new paper, the researchers show that, as they add the ability to recognize more objects to their system, the average number of parts per object steadily declines.<br /><br />Although the researchers’ work promises more efficient use of computer memory and programmers’ time, “it is far more important than just a better way to do object recognition,” says Tai Sing Lee, an associate professor of computer science at Carnegie Mellon University. “This work is important partly because I feel it speaks to a couple scientific mysteries in the brain.” Lee points out that visual processing in humans seems to involve five to seven distinct brain regions, but no one is quite sure what they do. The researchers’ new object recognition system doesn’t specify the number of layers in each hierarchical model; the system simply assembles as many layers as it needs. “What kind of stunned me is that [the] system typically learns five to seven layers,” Lee says. That, he says, suggests that it may perform the same types of visual processing that takes place in the brain.<br /><br />In their paper, the MIT and UCLA researchers report that, in tests, their system performed as well as existing object-recognition systems. But that’s still nowhere near as well as the human brain. Lee says that the researchers’ system currently focuses chiefly on detecting the edges of two-dimensional depictions of objects; to approach the performance of the human brain, it will have to incorporate a lot of additional information about surface textures and three-dimensional contours, as the brain does. Zhu adds that he and his colleagues are also pursuing other applications of their technology. For instance, their hierarchical models naturally lend themselves not only to automatic object recognition — determining what an object is — but also automatic object segmentation — labeling an object’s constituent parts.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Tackling cancer on a new front]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/cancer-india-0507.html</link>
<story_id>15303</story_id>
<featured>0</featured>
<description><![CDATA[New program at MIT’s Koch Institute targets the growing cancer problem in India.]]></description>
<postDate>Fri, 07 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100506150509-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100506150509-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100506150509-1.jpg</fullURL>
</image>
<body><![CDATA[Cancer, which has been predominantly a disease of industrialized nations, is rapidly encroaching on the developing world as people live longer and diagnostic technology improves. Cancer now kills more people in developing countries than HIV, tuberculosis and malaria combined, according to the World Health Organization.<br /><br />In India, about a million new cases are diagnosed every year, and that number is projected to triple in the next 20 years. Efforts are now under way in India to make cancer research a priority — an endeavor that is getting a boost from a new program at MIT’s David H. Koch Institute for Integrative Cancer Research.<br /><br />The new program, funded by Kiran Mazumdar-Shaw, chair and managing director of Biocon, one of India’s largest biotechnology firms, will bring Indian scientists to MIT to train for two years. Those researchers will then return to India to help jump-start cancer research programs there.<br /><br />“As an entrepreneur whose company is pursuing cancer research, I am acutely aware of the need to augment cancer research in the country at a basic scientific level,” says Mazumdar-Shaw. “If we are to develop new onco-therapies for Indian patients, we must create a strong research ethos amongst the medical and scientific communities engaged in the area of cancer.”<br /><br />Cancer cases in developing nations now make up more than 50 percent of the global total, compared to 15 percent in 1970, according to the World Health Organization. There are several reasons for that, says Sudhir Borgonha GM ’01, CEO of Translational Medicine India, which runs clinical trials in India. Cancer diagnostic technology is becoming more accessible in India, and the population is becoming more aware of the need for early screening, which leads to more diagnoses.<br /><br />Advances in combating infectious diseases like malaria have also contributed to higher cancer rates. “As people live longer, you’re going to see a lot more diseases that you didn’t see before, when people were dying younger of things like TB and malaria,” says Borgonha, who is not involved in the new program.<br /><br /><strong>New connections</strong><br /><br />Mazumdar-Shaw, who launched Biocon from her garage in 1978, also recently funded one of the largest cancer hospitals in India, the Mazumdar-Shaw Cancer Centre in Bangalore. After joining the Koch Institute’s Leadership Council in 2008, she came up with the idea of creating a formal linkage between researchers in India and at the Koch Institute.<br /><br />This new program, the first of its kind, should help spread the Koch Institute’s novel approach of bringing biologists and engineers together to battle cancer, says Tyler Jacks, director of the Koch Institute.<br /><br />Over the next five years, starting in early 2011, the Mazumdar-Shaw Oncofellows Program will bring about 10 fellows at the postdoctoral level to the Koch Institute. As part of the program, MIT faculty will also travel to India to interact with scientists there.<br /><br />“People need experience and exposure to collaboration before they can start their own programs,” says Borgonha. “There’s a tradition at schools like MIT that have always had people come and go back and create their own mini-MITs, or mini-Oxfords.”<br /><br />Scientists returning to India will likely encounter some challenges common in developing nations — many layers of bureaucracy, outdated systems and lack of experienced middle-level professionals — but these will be overcome with time, says Borgonha, adding that India does have many advantages that will help balance the obstacles: “There is adequate capital, enough mentorship, good role models, experienced senior level managers (CEO and board level), adequate thought leaders and infrastructure solutions.”<br /><br />Indian scientists who participate in the new Koch Institute program also could help bring new treatments and diagnostic technologies to India faster, says Vinod Raina, professor of medical oncology at the All India Institute of Medical Sciences, and a member of the committee that will select the new fellows. “After the communications and wireless revolution in the last 10 years, India is now at the threshold of a biotechnology revolution,” he says.<br /><br />There is a particularly urgent need for more research into the types of cancer most common in India, including head and neck cancer (often the result of tobacco use) and cervical cancer, which are not as prevalent in the West, says Raina.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[New insights into the mystery of natural HIV immunity]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/ragon-paper-0506.html</link>
<story_id>15298</story_id>
<featured>0</featured>
<description><![CDATA[A new finding from the Ragon Institute of MGH, MIT and Harvard may have implications for designing an effective AIDS vaccine.]]></description>
<postDate>Thu, 06 May 2010 17:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100505105238-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100505105238-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100505105238-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100505105337-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[Arup Chakraborty, professor of chemical engineering, chemistry and biological engineering]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[When people become infected by HIV, it’s usually only a matter of time, barring drug intervention, until they develop full-blown AIDS. However, a small number of people exposed to the virus progress very slowly to AIDS — and some never develop the disease at all.<br /><br />In the late 1990s, researchers showed that a very high percentage of those naturally HIV-immune people, who represent about one in 200 infected individuals, carry a gene called HLA B57. Now a team of researchers from the Ragon Institute of Massachusetts General Hospital, MIT and Harvard has revealed a new effect that contributes to this gene’s ability to confer immunity.<br /><br />The research team, led by MIT Professor Arup Chakraborty and Harvard Professor Bruce Walker of MGH, found that the HLA B57 gene causes the body to make more potent killer T cells — white blood cells that help defend the body from infectious invaders. Patients with the gene have a larger number of T cells that bind strongly to more pieces of HIV protein than people who do not have the gene. This makes the T cells more likely to recognize cells that express HIV proteins, including mutated versions that arise during infection. This effect contributes to superior control of HIV infection (and any other virus that evolves rapidly), but it also makes those people more susceptible to autoimmune diseases, in which T cells attack the body’s own cells.<br /><br />This new knowledge, described in the May 5 <a href="http://www.nature.com/nature/journal/vaop/ncurrent/full/nature08997.html" target="_blank">online edition</a> of <em>Nature</em>, could help researchers develop vaccines that provoke the same response to HIV that individuals with HLA B57 muster on their own, says Walker, who is director of the Ragon Institute and a professor at Harvard Medical School.<br /><br />“HIV is slowly revealing itself,” says Walker. “This is another point in our favor in the fight against the virus, but we have a long way to go.”<br /><br /><strong>Natural resistance</strong><br /><br />Chakraborty, a professor of chemical engineering, chemistry and biological engineering who specializes in theoretical and computational studies of the immune system, undertook this study after Walker told him about the phenomenon of HLA B57-induced immunity. Chakraborty was also intrigued by the fact that people who carry the HLA B57 gene also are more likely to develop autoimmune disorders.<br /><br />Chakraborty, Walker and their colleagues focused on killer T cells, one of two types of T cells that play an important role in the immune response. Most killer T cells are genetically unique and recognize different pieces of foreign proteins, known as epitopes, attached to the surface of cells that have been infected by viruses or bacteria.<br /><br />After a killer T cell grabs hold of such a protein, it becomes activated and starts sweeping the body for more cells that express the same protein, so it can kill them. It also clones itself to produce an army of T cells targeting the invader.<br /><br />The new Ragon Institute study shows that individuals with the HLA B57 gene produce larger numbers of killer T cells that are cross-reactive, meaning they can attack more than one epitope associated with HIV, including mutants that arise to escape activated killer T cells. <br /><br />The finding offers hope that researchers could design a vaccine to help draw out cross-reactive T cells in people who don’t have the HLA B57 gene. “It’s not that they don’t have cross-reactive T cells,” says Chakraborty. “They do have them, but they’re much rarer, and we think they might be coaxed into action with the right vaccine.”<br /><br />The work is a valuable contribution to scientists’ understanding of HIV, says David Baltimore, professor of biology and former president of Caltech.<br /><br />“This is a remarkable paper because it starts from a clinical observation, integrates it with experimental observations, generates a valuable model and derives from the model a deep understanding of the behavior of the human immune system. Rarely does one read a paper that stretches the mind so surprisingly far,” says Baltimore, a Nobel laureate in physiology or medicine who now studies HIV and human T cell interactions.<br /><br /><strong>Weeding out</strong><br /><br />Chakraborty and colleagues had previously developed computational models of T-cell development in the thymus, an organ located behind the breastbone through which T cells must pass in order to become mature killers. There they undergo a selection process designed to weed out cells that might attack the body’s own cells (which display pieces of human proteins on their surface). T cells must also demonstrate that they can bind weakly to some human protein fragments. Only a tiny percentage of T cells pass these tests and are allowed to leave the thymus and circulate in the body to defend against viruses,  other diseases, and cancerous cells. <br /><br />Inside the thymus, T cells are exposed to “self-peptides” — small human protein fragments — bound to HLA proteins. Chakraborty and co-workers had previously shown that the diversity of self-peptide fragments presented in the thymus influences the kinds of T cells a person can produce. The type and number of self-peptides expressed are determined by the HLA genes, which have hundreds of distinct forms, including HLA B57. Each person carries up to six of them (three inherited from each parent).<br /><br />Using data from previous studies, the Ragon team found that HLA B57 protein presents fewer types of self-peptides than most other HLA proteins. (HLA B27 is another protein that presents few types of self-peptides and also appears to protect against HIV and promote autoimmune disorders.) In this study, Chakraborty and postdoctoral fellow Elizabeth Read and graduate student Andrej Kosmrlj, lead authors of the paper, used their computer model to study what happens when maturing T cells are exposed to only a small diversity of self-peptides in the thymus.<br /><br />T cells with receptors that bind strongly to any of the self-peptides in the thymus are forced to undergo cell suicide, because of their potential to attack the body’s own cells. Chakraborty and co-workers showed that this means that, for most individuals, most of the body’s T cells have receptors that bind to targeted viral proteins via a number of weak interactions, with each interaction making a significant contribution to the binding. Thus, a single mutation to an HIV peptide can potentially evade the immune response.<br /><br />A different scenario unfolds in people who have the HLA B57 gene. Using their computer model, Chakraborty and colleagues showed that, because those individuals’ T cells are exposed to fewer self-peptides in the thymus, T cells with receptors that mediate strong binding to viral proteins via just a few important contacts are more likely to escape the thymus. This makes these T cells more cross-reactive to targeted HIV peptide mutants, because as long as those points in the viral proteins don’t mutate, the T cells are still effective. The model also showed that once those T cells are released into the bloodstream, they can effectively attack HIV proteins, even when the virus mutates. <br /><br />This model also explains why people with the HLA B57 gene have autoimmune problems: Their T cells are more likely to bind strongly to human peptides not encountered in the thymus.<br /><br />The computational studies explained many puzzles, but also made a prediction: Individuals with HLA genes that result in a display of fewer self-peptides should control HIV (and other viruses like hepatitis C virus) better. To test this prediction, the researchers studied nearly 2,000 patients — 1,100 “HIV controllers” and 800 who progressed normally to AIDS, and confirmed that this appears to be true.<br /><br /><strong>‘A horrific global problem’</strong><br /><br />The melding of complementary approaches — clinical studies, basic immunology and computations rooted in engineering and the physical sciences — exemplified by this study is part of the mission of the Ragon Institute, founded last year to support discovery of an effective AIDS vaccine.<br /><br />The idea is to break down the “scientific silos” in which many researchers work, says Walker. “Because of the Ragon funding, Arup and I had a conversation we never would have otherwise had,” he says. “We probably never would have met.”<br /><br />There are now a few dozen researchers working on Ragon-funded HIV studies, and Walker believes those new collaborations can lead to many more successes. “There are people out there who have never worked on HIV problems that have something to immediately contribute, and this is a great example of that,” he says. “We have not yet brought the full potential of scientific knowledge to bear on trying to do something about HIV, which remains a horrific global problem.”<br /><br />The research was also partly funded by Chakraborty’s NIH Director’s Pioneer Award. “Combining fundamental understanding of molecular mechanisms underlying immune function with computational methods for scanning large numbers of sequences allowed the development of a powerful explanation for a problem of great importance to public health. Such interdisciplinary research results are one of the goals of the NIH Director’s Pioneer Award program,” says Jeremy M. Berg, director of the National Institute of General Medical Sciences.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Genes as fossils]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/gene-fossils-0506.html</link>
<story_id>15300</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers discover the DNA responsible for creating fossil-like molecules found in ancient rocks.]]></description>
<postDate>Thu, 06 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100505161351-1.png</thumbURL>
<smallURL width='140' height='149'>http://web.mit.edu/newsoffice/images/article_images/w140/20100505161351-1.jpg</smallURL>
<fullURL width='368' height='391'>http://web.mit.edu/newsoffice/images/article_images/20100505161351-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Ryan C. Hunter]]></imageCredits>
<imageCaption><![CDATA[The Rhodopseudomonas palustris strain TIE-1 upon shallow sediments from the Sippewissett Salt Marsh near Woods Hole, Mass. These sediments are well known for colorful microbial assemblages dominated by purple phototrophic bacteria.]]></imageCaption>
</image>
<body><![CDATA[When exactly did oxygen first appear in Earth’s atmosphere? Although many physical and chemical processes are thought to be responsible for that profound transformation, scientists  have tried to answer at least part of that question by looking for the origin of oxygenic photosynthesis — the process that organisms use to split water to make oxygen — in rocks that are billions of years old. One way they try to pinpoint the start of that process is by searching for biological links between the distant past and the present. Specifically, they study molecules known as biomarkers that are produced by modern organisms and can be traced to the origins of certain biological processes because they are found in rocks that are 2.5 billion years old.<br /><br />One biomarker that had been proposed for such research is a type of lipid, or fat molecule, known as 2-methylhopanoid. This substance was thought to be a good biomarker because it has been found in ancient rocks (where it is referred to as 2-methylhopane) and is also produced in the modern environment by cyanobacteria, which are oxygen-producing bacteria located in shallow marine environments.<br /><br />But it turns out that these molecules might not be the best biomarkers for oxygenic photosynthesis, according to a recent collaboration between MIT’s Department of Biology and Department of Earth, Atmospheric and Planetary Sciences (EAPS). In a paper <a href="http://www.pnas.org/content/early/2010/04/20/0912949107.abstract?sid=0866e2d4-cd64-4ca9-937a-3cd4f7631559" target="_blank">published last week</a> in the <em>Proceedings of the National Academy of Sciences</em>, the group reported it had discovered the gene and related protein that are responsible for producing 2-methylhopanoids. Because this DNA can be traced to bacteria that do not produce oxygen, 2-methylhopanoids cannot confidently be used as biomarkers for oxygenic photosynthesis, the researchers say.  <br /><br />The research, which was supported by NASA, the National Science Foundation and the Agouron Institute, counters previous work of co-author <a href="http://eaps.mit.edu/geobiology/people/rsummons.html " target="_blank">Roger Summons</a>, an EAPS professor of geobiology, who first proposed in 1999 that 2-methylhopanoids could be a biomarker for cyanobacteria. That work was called into question in 2007 when researchers in the lab of co-author <a href="http://web.mit.edu/biology/www/facultyareas/facresearch/newman.html " target="_blank">Dianne K. Newman</a>, the John and Dorothy Wilson Professor of Biology and Geobiology, in collaboration with Alex Sessions at the California Institute of Technology, discovered a type of bacterium that doesn’t produce oxygen but does produce 2-methylhopanoids. <br /><br />To determine whether this was a chance finding or whether different kinds of bacteria produce 2-methylhopanoids, Summons, Newman, Sessions and several postdoctoral researchers joined forces to figure out which genes and proteins are involved in making the lipids. Knowing this gene, the researchers could then search the genome databases for other bacteria that also produce these molecules. They could also learn more about the purpose of the molecules, such as whether they emerged in response to some sort of environmental stress billions of years ago. “This is an excellent example of how genes themselves can also be used as fossils,” said lead author Paula Welander, a postdoc from Newman’s lab, who explained that previous surveys of bacteria that produce 2-methylhopanoids  were limited because they were done in the laboratory under arbitrary growth conditions which did not necessarily elicit their production. Moreover, because there are lots of bacteria that biologists can’t yet grow in the lab due to technical limitations, this means they might not be aware of some that can produce 2-methylhopanoids. By taking a molecular genetics approach to analyzing lipid production, the group was able to circumvent these limitations.<br /><br />Ann Pearson, a biogeochemist at Harvard who was not involved in the research, said that although scientists still have a long way to go before understanding the earliest origins of these biomarker molecules, the discovery of this DNA is “crucial” because now scientists know where to look as they start to “fill in the holes” about the earliest history of oxygenic photosynthesis. <br /><br /><strong>Unraveling 2-methylhopanoids</strong><br /><br />What makes 2-methylhopanoids unusual from other lipids that are produced by bacteria is that they have an extra methyl group (one carbon and three hydrogen atoms). To determine which gene and protein are responsible for adding the methyl group, the MIT researchers analyzed the genome of the bacterium that Newman’s lab had previously discovered also produces 2-methylhopanoids. <br /><br />Once they found a gene cluster that is responsible for making lipids, they generated mutants with deletions in genes encoding proteins likely to be responsible for adding the methyl group. After pinpointing the gene that encodes this protein, they searched the genome databases to determine what other types of bacteria contain this specific gene. It turns out there are three major groups containing many bacteria, including cyanobacteria, but the researchers were not able to trace the complex history of the protein to determine which bacteria carried the oldest version. <br /><br />In addition to determining this history, the researchers are trying to decipher the function of 2-methylhopanoids, which could reveal something about what microbial life and its environment were like on Earth billions of years ago.  They are currently conducting microbiology and molecular biology experiments to examine the features of membranes that contain the molecules, and they are also researching the environments where these molecules are found today. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Pattern seen in governments’ currency policies]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/migrant-money-0505.html</link>
<story_id>15292</story_id>
<featured>0</featured>
<description><![CDATA[Small-time money transfers from migrants shape key decisions on foreign exchange, research shows.]]></description>
<postDate>Wed, 05 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100504160210-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100504160210-1.jpg</smallURL>
<fullURL width='368' height='246'>http://web.mit.edu/newsoffice/images/article_images/20100504160210-1.jpg</fullURL>
</image>
<body><![CDATA[Every day around the world, vast numbers of migrants wire money back to their home countries, trying to support families and friends in need. In fact, these transfers of money — “remittances” — constitute a significant part of the global economy. <br /><br />Consider that in El Salvador, Haiti, Honduras, and Jordan, the level of remittances exceeds 15 percent of each nation’s gross domestic product (GDP), the value of all goods and services produced annually. In 2004, 42 countries in the developing world received remittances greater than 5 percent of GDP. Or try this for perspective. There are about 31,000 McDonald’s franchises around the world, often serving as symbols of unstoppable globalization. But there are 410,000 worldwide offices of the money-transfer firm Western Union, notes David Andrew Singer, an assistant professor of political science at MIT, and remittances are a rapidly growing phenomenon. Money transfers to developing countries totaled $31 billion in 1990, but more than $300 billion in 2007.<br /><br />Indeed, as Singer has discovered in new research, migrants send so much cash sloshing around the globe that it has a major impact on one of the highest-level decisions governments in the developing world make: The more remittances that flow into a country, the more likely that country is to fix its exchange rate, a critical policy matter that often dictates how much control a country has over the state of its own economy. <br /><br />“This is a tremendously important, highly political decision,” says Singer. “It affects everybody in society, whether they know it or not.” And yet, as Singer points out in a new paper, “<a href="http://web.mit.edu/polisci/faculty/facfiles/Singer_APSR.pdf" target="_blank">Migrant Remittances and Exchange Rate Regimes in the Developing World</a>,” to be published in the <em>American Political Science Review</em>, in order to predict what a country’s exchange-rate policy will be, it makes sense to look beyond the corridors of power and follow the money moving through those Western Union offices, one wire transfer at a time.<br /><br /><strong>Fixed rate or floating currency?</strong><br /><br />Many people think about exchange rates only when they are traveling abroad. Americans, for instance, know a strong dollar helps their purchasing power in foreign countries. But there is a clear benefit to having a relatively lower-value currency: It allows a country’s exports to be purchased more widely around the globe, a particular benefit in hard economic times. So there is no one-size-fits-all rule about whether it is good to have a strong currency. It depends. <br /><br />A separate but related question is whether a country should fix its currency at a certain value, or let it fluctuate in international markets. A government that opts for flexibility can cope with a sudden economic downturn by devaluing its currency, in order to keep its businesses selling goods abroad — which means those firms are still employing people and keeping the national economy churning. (Although, in such circumstances, imports will become more expensive.) <br /><br />By contrast, notes Singer, “When you fix your exchange rate, you can’t adjust your monetary policy to changing economic circumstances. You tie your own hands.” <br /><br />And yet, developing countries have particular reasons to set their currencies at fixed rates. “With a fixed exchange rate, a country can attract investment or establish stable import-export relationships,” says Singer. “If I’m a foreign investor interested in a developing country and I want to set up a production facility, it might be easier if I’m assured the exchange rate isn’t going to fluctuate, so I know the value of my investment isn’t going to change dramatically day to day.” <br /><br />That is why, Singer observes, “developing countries really face a difficult dilemma when it comes to their exchange-rate policy. The trade-off is between flexibility and stability.” <br /><br /><strong>Follow the money</strong><br /><br />In that case, how do developing countries arrive at a decision? This is where Singer has found a distinct pattern. Remittances are a standard line item on governments’ balance-of-payments records, reported to both the International Monetary Fund and the World Bank. After looking at World Bank data for 74 countries over most of the last three decades, Singer noticed that in countries with fixed exchange rates, remittances are 7.9 percent of GDP, while in countries with floating exchange rates, remittances are just 3.5 percent of GDP.<br /><br />This is because the remittances provide liquidity, a flow of money, helping governments ease the costs of having fixed exchange rates. Crucially, as Singer notes, the pattern of money transfers is largely “counter-cyclical” — that is, it increases when a developing country’s economy is sputtering. The central bank of the Philippines, for instance, reported that remittances increased 11 percent in November 2009, compared to the previous November, a rise largely due to migrants sending money home after two large typhoons battered the country. <br /><br />“Remittances have this wonderful characteristic, which is that they tend to increase when times are bad in the receiving country,” says Singer. “What a government might normally do in those circumstances is to make credit easily available to spur investment, but if you fix your exchange rate, you can’t do that. But because migrants know their families are struggling, they increase the amount of money they send home.” And that money comes without strings attached, unlike loans; it is just much-needed cash, washing through the local economy.<br /><br /><strong>Off the record</strong><br /><br />Because many factors affect large-scale economic decisions, Singer scrutinized the data to see that he had identified a cause-and-effect relationship, not a mere correlation. Even when accounting for matters of government stability, forms of government, and a country’s reliance on exports, Singer found, the connection between remittances and exchange-rate policy remains significant.<br /><br />Singer’s colleagues say his results are significant, but unexpected to the extent that it is forcing them to closely scrutinize and reevaluate the possible cause-and-effect relationships at work in this area. <br /><br />“It’s an important paper because it flies in the face of how we think about this relationship,” says David Leblang, a professor of politics at the University of Virginia. “But the truth is, I didn’t believe it at first. My gut tells me that exchange rates influence remittances — that people will send more money home if they have favorable fixed rates. But this paper makes people like myself question those assumptions. And that’s the purpose of academic research.”<br /><br />For his part, Singer holds to the notion that remittances cause exchange rates to be fixed, not that fixed rates increase the flow of remittances. He notes that the pattern of emigration from the developing world to advanced industrial countries — from which migrants send high levels of remittances home — does not correspond to existing exchange-rate regimes, meaning there seems to be no overarching pattern of migrants being motivated by preexisting policies. Moreover, survey data of migrants’ behavior indicates they do not transfer less money when their original countries have floating exchange rates. In general, Singer asserts, migrants “behave like family members” when sending money home, not “like financial investors.”  <br /><br />Perhaps, Leblang notes, if central bankers acknowledged that remittances were behind their policy decisions, it would add another layer of support to Singer’s thesis. However, having spoken to many government bankers about the issue, Singer believes that policymakers are loathe to talk publicly about the topic.<br /><br />“The central bankers and treasury officials understand how important this is,” says Singer. “But it’s very difficult to get a central banker to say, ‘Yes, this type of capital flow helps me decide to continue fixing the exchange rate.’ If you have a fixed exchange rate, the last thing you want to do is generate uncertainty about whether you can maintain it. So if you say you have a fixed rate because of remittances, and remittances decline, it would cause other countries to lose confidence in you.”  <br /><br />For now, the numbers tell a highly suggestive story. And Singer believes the data on remittances can form the basis for future research, on the relationship between money transfers and domestic policy throughout the developing world. Globalization, he writes, is not a simple force at work, but more like a “tug of war with various capital flows pulling policymakers in different directions.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Rapid analysis of DNA damage now possible]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/dna-damage-0504.html</link>
<story_id>15282</story_id>
<featured>0</featured>
<description><![CDATA[Technology offers a new way to test potential cancer drugs, detect effects of hazardous agents in our environment.]]></description>
<postDate>Tue, 04 May 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100503151143-1.png</thumbURL>
<smallURL width='140' height='96'>http://web.mit.edu/newsoffice/images/article_images/w140/20100503151143-1.png</smallURL>
<fullURL width='368' height='254'>http://web.mit.edu/newsoffice/images/article_images/20100503151143-1.png</fullURL>
<imageCredits><![CDATA[Credit: David Wood]]></imageCredits>
<imageCaption><![CDATA[MIT researchers have developed an array of microscopic wells that can capture single cells and analyze the amount of DNA damage in each cell. 
]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='274'>http://web.mit.edu/newsoffice/images/article_images/20100503151143-2.jpg</fullURL>
<imageCredits><![CDATA[Credit: David Weingeist and David Wood]]></imageCredits>
<imageCaption><![CDATA[When an electric field is applied to the gel where the cells sit, damaged DNA migrates across the gel, taking on the shape of a comet. The longer, brighter tails indicate more DNA damage.
]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100504095955-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[(From left) Associate Professor Bevin Engelward, graduate student David Weingeist, postdoctoral fellow David Wood and Professor Sangeeta Bhatia developed a new technology that can detect DNA damage in single cells.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Our DNA is under constant attack from many sources: Radiation, ultraviolet light, and contaminants in our food and in our environment can all wreak havoc on our genetic material, potentially leading to cancer and other diseases. Analyzing DNA damage is critical to understanding those diseases, as well as seeking new treatments, but current tools for detecting DNA damage make for tedious and time-consuming work.<br /><br />Now a team of MIT bioengineers has devised a new way to rapidly reveal DNA damage under a variety of conditions, promising to make such analysis a routine aspect of applications such as drug screening and epidemiological studies of the effects of environmental agents.<br /><br />The new technique is based on a 30-year-old test known as the “comet assay” — named for the comet-shaped smear that the damaged DNA forms during the test. However, the new technology can analyze a much greater number of cells, at a much faster rate, than the traditional comet assay. <br /><br />The technology, the result of a collaboration between researchers in the Harvard-MIT Division of Health Sciences and Technology (HST) and MIT’s Department of Biological Engineering, could offer a new approach for epidemiologists to detect dangerous environmental exposures long before they cause cancer, for clinicians to provide better cancer treatment and for researchers in the pharmaceutical industry to identify new drugs and screen out hazardous drugs. <br /><br />“We expect this could enable studies on a scale that hasn’t been possible before,” says David Wood, a postdoctoral fellow in HST who is lead author of a paper being published in this week’s <em>Proceedings of the National Academy of Sciences</em> that describes the new technique. Wood worked closely on the project with David Weingeist, graduate student in the Department of Biological Engineering. <br /><br />“A critical feature of our technology is that it can be used to detect genotoxic [mutation-causing] agents in the environment, even if only very basic research equipment is available,” says Bevin Engelward, associate professor of biological engineering and co-senior author of the paper with Sangeeta Bhatia, professor of HST and electrical engineering and computer science and member of the David H. Koch Institute for Integrative Cancer Research and Howard Hughes Medical Institute. “Enabling the prevention of genotoxic exposures in developing nations could ultimately improve the health of millions,” she added. <br /><br /><strong>Chasing comets</strong><br /><br />The comet assay is based on gel electrophoresis, a commonly used lab test in which an electric field is applied to DNA placed on a slab of polymer gel, forcing the DNA to move across the gel. During electrophoresis, damaged DNA travels farther than undamaged DNA, producing a comet-shaped smear. (The damaged DNA forms the “tail” of the comet.)<br /><br />This test is generally considered very sensitive, but is also quite laborious. Each experimental condition (for example, a test of how a particular drug affects DNA) requires its own microscope slide, and each slide (containing hundreds of cells) must be visually inspected by the researcher. This limits the number of experimental conditions that can be studied.<br /><br />To create the new test platform, the researchers from the Bhatia and Engelward laboratories developed the concept of microwells printed into the electrophoresis gel. Using a micropatterning technique developed by Wood and Bhatia, a grid of thousands of tiny indentations is created on the gel. Each cell is placed in its own indentation, giving it an “address” that stays constant throughout the process. Furthermore, each gel can be subdivided into distinct environments by placing a 96-well plate over the gel. Each well confines about a thousand cells, allowing the researchers to add different drugs or other chemicals to each well to compare their effects on DNA damage and repair.<br /><br />This setup allows dozens of experimental conditions to be tested on just one slide, and it enables slides to be automatically analyzed using custom-designed imaging software.<br /><br />The new technique is “a major achievement in the development of the comet assay, more than 30 years after its creation,” says Jean Cadet, a DNA researcher at the Center for Atomic Energy in Grenoble, France, who was not involved in the research. Cadet says he anticipates that the new test will become widespread, particularly for epidemiological studies.<br /><br />The technology was designed to be compatible with basic laboratory equipment, so virtually any laboratory can use it.<br /><br />To demonstrate the potential usefulness of the new system, the researchers evaluated three compounds that have been proposed as potential inhibitors of DNA repair. Such compounds could be used to boost the effectiveness of chemotherapy agents by preventing cancer cells from repairing the DNA damage caused by chemotherapy. Their study supported earlier predictions that two of the compounds, known as myricetin and DOPA, appear to halt DNA repair, while a third, called NCA, has little effect.<br /><br />The MIT team is now working with researchers in Thailand to study the effects of air pollutants on DNA in the cells of people who live in highly polluted regions, compared to inhabitants of cleaner areas. In a related study, they are teaming up with researchers at Boston University Medical School to compare cells from smokers and nonsmokers. They also hope to correlate the DNA-repair abilities of individual cells with the genes expressed in those cells.<br /><br />This work was supported by the MIT Center for Environmental Health Sciences, the National Institute of Environmental Health Sciences (Program in Gene-Environment Interactions), and the National Institutes of Health.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Water emergency is over]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/httpmitedunewsoffice2010water-0501html.html</link>
<story_id>15283</story_id>
<featured>0</featured>
<description><![CDATA[Affected communities now have clean tap water; instructions given for flushing pipes]]></description>
<postDate>Tue, 04 May 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100504095755-0.png</thumbURL>
<smallURL width='140' height='138'>http://web.mit.edu/newsoffice/images/article_images/w140/20100504095755-0.jpg</smallURL>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100504095755-0.jpg</fullURL>
</image>
<body><![CDATA[Earlier this morning, Governor Deval Patrick announced that the "boil water" order that had been in effect since Saturday for communities east of Weston, Massachusetts has ended. All water served by the the Massachusetts Water Resources Authority (MWRA) is now safe to drink. <br /><br /> Residents in affected communities (which did not include Cambridge, but which did include Boston and Brookline) are being advised by the MWRA to flush their pipes before consuming tap water. Visit MIT’s <a href="http://emergency.mit.net/">Emergency Information page</a> for links to further information. <br /><br /> On Saturday, the Massachusetts Water Resources Authority (MWRA) reported that a 10-foot water pipe in Weston, Mass., had ruptured. Because of the rupture, all communities east of Weston whose water is provided by the MWRA were receiving water from a backup system of reservoirs. The water from this system was not fit for drinking or cooking. Governor Deval Patrick stated that this water must boiled for one minute before being drunk. <br /><br /> Cambridge has its own water supply, and thus was not affected by the emergency.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Should Google stay in China?]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/google-china.html</link>
<story_id>15278</story_id>
<featured>0</featured>
<description><![CDATA[A panel discussion suggests that if the goal is to promote Chinese democracy, a censored Google is better than no Google at all.]]></description>
<postDate>Fri, 30 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100505113456-0.png</thumbURL>
<smallURL width='140' height='63'>http://web.mit.edu/newsoffice/images/article_images/w140/20100505113456-0.jpg</smallURL>
<fullURL width='368' height='165'>http://web.mit.edu/newsoffice/images/article_images/20100505113456-0.jpg</fullURL>
</image>
<body><![CDATA[On Jan. 12, Google announced that, because of a series of sophisticated attacks that seemed to originate in China and targeted the Gmail accounts of Chinese human-rights activists, it would cease to censor the results of searches performed using its Chinese search engine, Google.cn. In March, it began redirecting search queries sent to <a href="http://www.google.cn" target="_blank">Google.cn</a> to its search engine in Hong Kong, which is excluded by international treaty from the censorship laws that prevail in mainland China.<br /><br />On April 28, a group of MIT experts convened at the MIT Sloan School of Management for a panel discussion titled “Should Google Stay in China?” Two of the panel members, Sloan professor Yasheng Huang and Xiaojian Zhao, a Knight Science Journalism Fellow, answered the panel’s titular question with an unequivocal “yes”; the other two panelists, David Clark of the Computer Science and Artificial Intelligence Lab and Craig Simons, another Knight Fellow, were less direct but certainly offered arguments that bolstered their colleagues’ positions.<br /><br />The three panel members who are experts on China — Huang, Zhao and Simons — all agreed that the Internet had been a powerful democratizing force within the country. Simons, who for five years was Asia bureau chief for Cox Newspapers, pointed out that until 2004, the Chinese government had released statistics on the number of protests that occurred within the country each year. In 2004, Simons said, that number was 74,000, up from 10,000 10 years earlier. “My feeling — and, I think, the feeling of a number of journalists in China — is that the number of incidents has continued to rise,” Simons said. “That was one reason they no longer release the information.” The rise in public dissent, Simons suggested, was largely because of the Internet, which had proved an “incredible tool for mobilizing people across localities and across class lines.”<br /><br />Zhao, a reporter in the Beijing bureau of <em>Southern Weekly</em>, which <em>The New York Times</em> has called “China's most influential liberal newspaper,” agreed, citing a phenomenon that, she said, the Chinese refer to as “human-flesh search engines.” In China, corruption is rampant in local government, she explained, but official complaints must be lodged with the central state, through a woefully inefficient process that local officials frequently impede. Human-flesh search engines, she explained, are networks of Internet users who broadcast and publicize incidents of local corruption nationwide. “I think that human-flesh search engines are one of the most effective ways for ordinary people to go after corrupt local officials in China,” Zhao said. She even suggested that the central government welcomed them, as a way “to supervise the local governors.”<br /><br />Huang added that it may be difficult for Westerners to understand quite how revolutionary the Internet has been in China, because in China, “the traditional channels of communication are terrible.” The Internet didn’t just allow companies and media outlets to convey information more efficiently to existing markets and audiences, he said; it created entirely new markets and audiences. “Thirty years of economic growth, foreign direct investment, $2 trillion of foreign-exchange reserves, exports, the rising middle class, urbanization,” Huang said. “The Internet has given China more transparency and more democracy and more political accountability than all these other things combined.”<br /><br />Clark, who had served as the Internet’s chief protocol architect for most of the 1980s, was on the panel as a technical expert, and he pointed out that there was no technical reason that an attack on Google’s mail servers should require the withdrawal of its search engine from mainland China; indeed, he said, the mail servers that were attacked weren’t in China to begin with. He also pointed out that China was by no means the only country whose government inserted itself into the operation of the Internet. “If I were to describe a country in which you cannot attach to the Internet without completely identifying yourself — if you go into a hot spot, you have to give them your identity card — You might say, ‘What kind of country is that?’ And the answer is Italy. Not China,” he said.<br /><br />Given that the Internet is a democratizing force in China, and that Google’s withdrawal from mainland China didn’t make the servers that had been attacked any safer, there still remained the question of whether the promotion of Chinese democracy required Google in particular rather than just the Internet in general. While Google.cn was still operating, after all, it accounted for only about 25 to 30 percent of the search market in China. The other 70 to 75 percent belonged to Baidu, China’s homegrown search engine.<br /><br />On that topic, Huang was blunt: “I don’t trust Baidu,” he said. He said that companies could pay to have their URLs included in the top results of a Web search on Baidu. “I can’t point to an academic study, but I think their rule of thumb was, before Google left, the top three results on Baidu were paid,” Huang said. “Now the top 10 are, potentially because they don’t have to compete with Google.” During the question-and-answer session that followed the discussion, he expanded on this point, in the accustomed manner of a business-school professor: “It’s the Internet that is promoting democracy and promoting transparency. So then the issue is, how do we make the Internet better? And my view is that we make the Internet better by having more competition.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Diamond nominated for Federal Reserve post]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/diamond-fed-0429.html</link>
<story_id>15271</story_id>
<featured>0</featured>
<description><![CDATA[MIT economist and Institute Professor is Obama’s choice for the U.S. central bank’s seven-person board of governors.]]></description>
<postDate>Thu, 29 Apr 2010 15:19:21 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100429112406-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100429112406-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100429112406-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
</image>
<body><![CDATA[MIT economist and Institute Professor Peter Diamond PhD ’63 has been nominated by U.S. President Barack Obama to serve on the Board of Governors of the Federal Reserve, the central bank of the United States. If confirmed to what would be a 14-year term, Diamond would be one of seven governors on the board.<br /><br />As a member of the Fed’s board, Diamond would automatically serve on the Federal Open Market Committee (FOMC), the group that influences economic activity, employment and inflation by setting key interest rates. At the moment, interest rates are low in an effort to spur lending, investment and growth following the deep recession that began in late 2007. <br /><br />The Fed also studies national and international economic conditions and helps supervise the financial services industry. Congress is currently considering whether to augment the Fed’s regulatory responsibilities as part of its drive to reform the financial system.<br /><br />In addition to nominating Diamond, Obama today announced he was naming two others to serve on the Federal Reserve’s central board: Janet Yellen, currently president of the Federal Reserve Bank of San Francisco (nominated to be vice chair of the Fed), and Sarah Raskin, a lawyer who is Maryland’s commissioner of financial regulation. <br /><br />“The depth of experience these individuals bring in economic and monetary policy, financial regulation, and consumer protection will make them tremendous assets at the Fed,” Obama said today <a href="http://www.whitehouse.gov/the-press-office/president-obama-announces-intent-nominate-three-individuals-board-governors-federal" target="_blank">in a statement released by the White House</a>. “I am grateful they have chosen to dedicate their talents to serving the American people.”<br /><br /><strong>A broad portfolio</strong><br /><br />Diamond, a former chair of MIT’s Department of Economics, has made research advances in both macroeconomics and microeconomics during a wide-ranging career, studying subjects including growth, taxation and labor market searches. In recent decades he has analyzed social insurance programs closely and become a prominent authority on Social Security.<br /><br />Among the many books and papers Diamond has published, he is co-author of the volume <em>Saving Social Security: A Balanced Approach</em> (Brookings Institution Press, 2004), along with Peter Orszag, currently director of the Office of Management and Budget at the White House. <br /><br />In recent years, Diamond has taken an interest in behavioral economics, the branch of economics that borrows from psychology and sociology to explain why people do not always make rational decisions.<br /><br />Although Diamond's name has been floated as a possible Fed nomination for weeks, he has refrained from commenting publicly about the position, in advance of Senate confirmation hearings. Diamond again declined to comment today. <br /><br />The nomination is an “inspired decision,” said Henry Aaron, a senior fellow in economic studies at the Brookings Institution in Washington, who expressed hope that Congress will confirm Diamond expediently. “Peter brings extraordinary intelligence, judgment, and creativity to a position that carries with it the power to influence the lives of people in the United States and abroad.”<br /><br />Diamond’s long-time colleagues applauded the nomination. Ricardo Caballero, chair of the MIT Department of Economics, said, “Peter represents the very best that an academic economist has to offer to Washington: a superb and open mind, an insatiable appetite for understanding the institutional details of a problem and policy, and a spirit of service.”<br /><br /><strong>From mathematics to economics</strong><br /><br />After receiving his undergraduate degree from Yale, Diamond entered MIT as graduate student in mathematics before transferring to the economics program. He completed his doctoral thesis under the supervision of Robert Solow, currently Institute Professor, Emeritus, at MIT. In 1966, Diamond joined MIT as an associate professor. He was promoted to full professor in 1970, and named Institute Professor, MIT's highest faculty honor, in 1997.<br /><br />Diamond served as president of the American Economic Association in 2003, and has been a visiting scholar at Cambridge University, Oxford University and Harvard University, among other institutions. Diamond has received Guggenheim and Fulbright fellowships; he is also a fellow of the American Academy of Arts and Sciences and a member of the National Academy of Sciences. <br /><br />Diamond, 69, is retiring from MIT this spring. On Friday, April 30, Diamond’s retirement will be commemorated with the premiere of a work composed by another of MIT's Institute Professors, John Harbison.<br /><br />While Obama is currently attempting to fill multiple vacancies in the leadership of the Federal Reserve, the U.S. Senate confirmed Ben Bernanke PhD ’77 to his second term as Federal Reserve chairman in January of this year, a position that has a four-year term. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Stephen Connors on offshore wind farms]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/wind-farm-0429.html</link>
<story_id>15270</story_id>
<featured>0</featured>
<description><![CDATA[What the federal approval of the Cape Wind project will mean for Massachusetts and the nation]]></description>
<postDate>Thu, 29 Apr 2010 13:49:17 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100429095419-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100429095419-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100429095419-1.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='180' height='241'>http://web.mit.edu/newsoffice/images/article_images/20100429095419-2.jpg</fullURL>
<imageCaption><![CDATA[Stephen Connors, director of the Analysis Group for Regional Energy Alternatives (AGREA) at the MIT Energy Initiative]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<em>Stephen Connors is director of the Analysis Group for Regional Energy Alternatives (AGREA) at the MIT Energy Initiative. He is a graduate of what is now the Wind Energy Center at UMass-Amherst and an inaugural board member of the U.S. Offshore Wind Collaborative. MIT News asked him about the impact of the U.S. Department of the Interior’s <a href="http://www.doi.gov/news/doinews/Secretary-Salazar-Announces-Approval-of-Cape-Wind-Energy-Project-on-Outer-Continental-Shelf-off-Massachusetts.cfm" target="_blank">announcement on April 28</a> that it will allow a 130-turbine project called Cape Wind, to be built in Nantucket Sound. The project would be the nation’s first offshore wind farm.</em><br /><br /><strong>Q.</strong> How important do you think this decision is in terms of opening the door to a broader development of offshore wind resources in the U.S.?<br /><br /><strong>A.</strong> Being the first, the Cape Wind project in Nantucket Sound has been instrumental for getting an offshore wind industry rolling in the United States. When first proposed in 2001, it was not clear whose job it was at both the state and federal levels to grant the permits, let alone how. This was especially true for the environmental impact assessment, since there was no real baseline of ocean environmental criteria or magnitude of acceptable impacts on lobsters, clams, fish, birds and whales. It was several years into the permitting before the federal government passed a law selecting the Department of Interior's Minerals Management Service (MMS) as the coordinating federal authority for granting the final permit, although the Army Corps of Engineers, EPA and the Coast Guard all played important evaluation and approval roles.<br /><br />Early in Cape Wind's permitting process there were several proposed offshore projects, such as off Long Island. These were all mothballed or never got started as the Cape Wind experience dragged out. However, once Cape Wind had the majority of its Massachusetts permits, and the federal review process with MMS was moving forward, other serious projects and state initiatives from South Carolina to Maine and the Great Lakes have been appearing.<br /><br />While is has been a bumpy ride for Cape Wind, they appear to have steered their way through the now-charted regulatory waters, and others are following in their wake. This has been helped by the continued development of European offshore wind, especially in the areas of wind turbine technology and construction techniques.<br /><br /><strong>Q.</strong> Given the many years of ups and downs this project has been through, is this decision really the final word, or are there any remaining possible pitfalls?<br /><br /><strong>A.</strong> It appears to be. The Department of the Interior-MMS permit was Cape Wind's last remaining permit. However, that doesn't mean the ruling cannot be challenged. But Cape Wind has withstood numerous challenges already, so there is some confidence that most challengers' arguments have been heard. In anticipation of the project's approval, several groups announced their intention to challenge MMS's ruling, including the Aquinnah Wampanoag Tribe of Martha's Vineyard and the Alliance to Protect Nantucket Sound. Secretary of the Interior Ken Salazar's decision to approve the project puts to the side the recommendation of the Advisory Council on Historic Preservation that the project's impact on "historic properties" was too great. So that is a likely source for the next round of challenges, even though Salazar's permit requires Cape Wind to perform an archeological survey of the wind farm's seabed location and several other requirements.<br /><br /><strong>Q.</strong> How significant is the offshore wind potential in this country, and are there any remaining potential roadblocks or challenges to developing that potential?<br /><br /><strong>A.</strong> The U.S. Department of Energy's 2008 "20 Percent Wind Energy by 2030" report estimated that there is 54 gigawatts of offshore wind potential, roughly 4 percent of the nation's projected electricity demand if fully developed. Many states including Massachusetts, New Jersey and Delaware have been developing marine utilization plans, including zoning for offshore wind. These take into account fishing, shipping and visual impacts, so some "pre-siting" has been occurring. The cost and performance of offshore wind turbines is still an unknown.<br /><br />Most commercial experience in offshore wind has been "wet-foot" wind, namely larger, “marine-ized” versions of land-based wind turbines mounted directly to the ocean floor in relatively shallow waters. This is fine for Europe's North Sea where shallow waters reach far out into the ocean. Along the Northeast Coast of the U.S., many of the shallow waters remain close to shore, inviting local opposition. One alternative is the floating wind turbine, or what MIT Professor Paul Sclavounos calls the "invisible wind turbine." Using design tools developed for offshore oil rigs, Sclavounos has been designing floating structures for wind turbines that can be towed out and hooked up. Several European companies have just begun scale demonstrations of floating wind turbines. With ocean depth and sea floor geology less of an issue, it is hoped that floating offshore wind technologies will allow us to tap a larger portion of the ocean wind resource. This is especially important for the U.S. Northeast Corridor. With both high population densities and more forested and/or protected mountains than farmland, the opportunity for large, land-based wind installations is small. If the New England and Mid-Atlantic coasts are going to have lots of wind-generated electricity, it will need to be offshore.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Fund helps energy efficiency bloom across campus]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/greening-0422.html</link>
<story_id>15228</story_id>
<featured>0</featured>
<description><![CDATA[Alumni gifts support the greening of MIT, and the resulting savings are being reinvested in similar projects.]]></description>
<postDate>Thu, 29 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100421233729-1.png</thumbURL>
<smallURL width='140' height='115'>http://web.mit.edu/newsoffice/images/article_images/w140/20100421233729-1.jpg</smallURL>
<fullURL width='368' height='304'>http://web.mit.edu/newsoffice/images/article_images/20100421233729-1.jpg</fullURL>
</image>
<body><![CDATA[One hurdle colleges and universities face as they try to improve energy efficiency is figuring out how to pay for it. The long-term savings from these kinds of projects can be significant for a large institution — done right, such a plan could save MIT several million dollars on its energy bill each year. But these energy projects require substantial up-front investment, and that can be challenging for schools struggling with budget cuts.<br /><br />MIT is on a path to meet this challenge, thanks to a $1 million gift from Jeffrey Silverman '68 about one year ago. With this money, the Institute has established a fund to support campus energy and efficiency projects that have rapid "paybacks" — or savings that accrue and then can be reinvested into additional projects. <br /><br />Silverman was intrigued by this giving opportunity because it allowed him to make a major difference at his alma mater while also leveraging his initial investment so that it could grow and be used in different ways. "The idea of providing the seed money that was going to create savings and then get reinvested into more savings interested me," he said. He was also impressed that the fund was designed so that the savings would be rigorously measured, documented and verified.<br /><br />Silverman, a successful commodities trader, first heard about the investment opportunity from Theresa M. Stone, MIT's executive vice president and treasurer, who also cochairs the Campus Energy Task Force. The task force was established by the MIT Energy Initiative to help MIT "walk the talk" on energy use. Stone budgeted $500,000 in seed money in 2008 to promote energy conservation work in response to a review by the Department of Facilities and a student team from the MIT Sloan School of Management's Laboratory for Sustainable Business. That review determined that the Institute could save about $6 million each year — or 10 percent of its annual energy bill — through conservation projects that have quick paybacks. Stone's team then developed a list of appropriate payback projects, estimating they would cost MIT $14 million in one upfront investment. <br /><br />Silverman was eager to contribute to Stone's effort, and so in April 2009 he formed the Silverman Evergreen Energy Fund. David Desjardins '83, a consultant and investor who is also passionate about campus energy issues, has since donated an additional $500,000 to the effort. <br /><br />"The gifts made by Jeff and David marked votes of confidence in our commitment to implement disciplined, measurable improvements designed to improve campus energy efficiency," Stone said. "Their seed money continues to bear fruit for the MIT community and for those who value our example."<br /><br />To date, the fund has paid to upgrade the lighting systems in the Ray and Maria Stata Center for Computer, Information, and Intelligence Sciences, as well as the Stratton Student Center. Both projects required a combined investment of nearly $600,000 and have resulted in estimated annual savings of about $185,000, meaning they will have paid for themselves after about three years.<br /><br />Another major focus of the Silverman fund has been to recalibrate the nearly 200 fume hoods in the Dreyfus Chemistry Building (Building 18). Fume hoods are massive ventilation devices that protect researchers from potential chemical exposure by sucking up air and exhausting it outside — and they require a lot of energy. A significant amount of this energy consumption can be reduced by lowering the volume of air that moves through the hoods while still providing the same level of protection. This project cost about $430,000 and will save about $160,000 annually.<br /><br />The savings from the first round of projects financed by the Silverman fund will be reinvested into a second round of energy conservation work. These additional projects will most likely include more lighting retrofits and fume hood work. They could also involve strategies for reducing heating, ventilation, and air-conditioning needs in unoccupied spaces. After this second round of investments is deployed, Stone will examine the fund's effectiveness in meeting the Institute's goals for payback opportunities. <br /><br />Information on other innovative campus energy programs at MIT and a newly released task force update report is available at <a href="http://web.mit.edu/mitei/campus" target="_blank">http://web.mit.edu/mitei/campus.</a><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[In The World: Clean Water for Ghana]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/itw-home-water-0429.html</link>
<story_id>15266</story_id>
<featured>0</featured>
<description><![CDATA[MIT students help a researcher build a factory that could provide water filters for 1 million people in northern Ghana]]></description>
<postDate>Thu, 29 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100428155938-2.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100428155938-2.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100428155938-2.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy of Susan Murcott]]></imageCredits>
<imageCaption><![CDATA[Several women from a village near Tamale, Ghana, test the filters that Pure Home Water will soon begin manufacturing.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100428154956-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Alexandr Nisichenko]]></imageCredits>
<imageCaption><![CDATA[Two children in Ghana show the differences between water that has been cleaned with the Kosim filter, left, and water that has not.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100428160909-3.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy of Susan Murcott]]></imageCredits>
<imageCaption><![CDATA[A consultant and several local villagers help the MIT team start construction of the factory this past January. Full production is expected to begin in late summer.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Nearly 1 billion people do not have access to clean drinking water. The problem is particularly dire in Ghana, where diarrhea causes 25 percent of all deaths of children below the age of five each year, according to UNICEF. The figure is even higher in northern Ghana, where about half the population get its water from wells, ponds and streams that often contain disease-causing microorganisms. <br /><br />Part of the problem is that large, centralized water filtration and sanitation systems aren’t designed to reach remote areas like northern Ghana, according to Susan Murcott, a senior lecturer in MIT’s Department of Civil and Environmental Engineering (CEE). Murcott has been distributing affordable ceramic water filters in the region through <a href="http://www.purehomeh2o.com" target="_blank">Pure Home Water (PHW)</a>, a nonprofit she co-founded with local partners in 2005. To date, PHW has sold and distributed the filters to more than 100,000 people, and the goal is to reach 1 million people by 2015.<br /><br />Murcott has taught water and sanitation infrastructure in developing countries for more than a decade and believes that a household-scale device is crucial for fighting poverty in remote regions that aren’t connected to urban water and wastewater treatment plants. “Engineering schools in the West teach students mainly how to build large, urban, centralized systems, but these systems can never reach the people most in need,” she said. “This is a huge disconnect, and my work has been about making people aware that while the treatment processes are the same, they can design small systems, either household-scale or community scale, that will reach the bottom billion.”<br /><br />Locally branded as the "Kosim" filter, (meaning “pure water” in the local Dagbani language), the flower pot-shaped vessel has thousands of microscopic pores on the bottom that drain water while trapping parasites and bacteria. Although other small systems have been successful in other developing countries, the Kosim filter is the only one that has been embraced by northern Ghana’s residents, who are willing to pay as much as $5 for a filter even though they earn less than $1 per day on average. One reason for the popularity is that PHW is the only NGO testing and monitoring the quality of the water treated by its filters. The organization also employs trained field personnel to help integrate the filter into poor households.<br /><br />More than 40 MIT engineering and Sloan School of Management students, many of whom have traveled to Ghana thanks to the support of the CEE Masters of Engineering Program, the Global Entrepreneurship Lab and the Public Service Center, have helped PHW by conducting product-research and consumer studies. They are currently helping the organization become locally and financially self-sufficient by building a factory in northern Ghana. Until now, PHW would procure, sell, distribute and monitor filters it bought from a factory located more than 12 hours away, but the filters suffered from lack of quality control and often broke. By making their own filters, Murcott is confident her team can improve their quality while also reducing their cost of production from $16 to $10.<br /><br />Because the filters cost more for PHW to produce than people can typically pay for them, owning a factory is essential for PHW to become a sustainable business that does not operate at a loss or rely solely on outside grants, according to Gordon Adomdza, a Ghanaian business professor at Northeastern University who advises PHW. With its own factory and equipment, PHW can diversify its offerings to other clay products like bricks and tiles that can be sold for profit. “They can then reinvest that money to help supplement the filter deployment,” said Adomdza, who is optimistic about PHW’s future as a self-sustaining organization.<br /><br /><strong>From the ground up</strong><br /><br />In January, Murcott, four students, a factory consultant and local workers spent one month building the factory, as well as the kilns and other manufacturing equipment. The construction also involved testing the production capabilities of the site to ensure that the filters could be produced through an unusual technique that Murcott learned a decade ago while researching household filters made in Nicaragua by nonprofit Potters for Peace. <br /><br />Instead of the traditional method of mixing clay, forming it into a shape and heating it in a kiln, the Potters for Peace method entails breaking the clay into small pieces that are dried in the sun for a few days. After the pieces are ground into a powder and mixed with rice husk or sawdust, the mixture is molded into a pot with a press and placed in a kiln for eight hours. During the firing process, combustible particles from the rice husk and sawdust burn off, leaving tiny pores small enough to keep out bacteria and parasites but large enough to allow one to 2.5 liters of water to flow through per hour. <br /><br />The filter design and production process are constantly being evaluated to improve performance and durability. While helping with the construction, Reed Miller, a graduate student in CEE, tested how different amounts of rice husk and sawdust, as well as differently shaped pots, affected the bacterial removal and flow rates of the filters. Although his research is ongoing, Miller said his trip to Ghana was a great way to apply engineering principles he had learned in the classroom to real-world problems. “It was quite rewarding to create a finished product literally from the ground up,” he said. <br /><br />Murcott and two students will return to Ghana in late May to finish construction of the factory with local Pure Home Water partners before the rainy season begins in June, and full factory production is scheduled to begin later this summer. But even that is just the tip of the iceberg for Murcott, who estimates that there are only 25 similar factories operating in 18 countries. “My dream is that there should be factories like this in all of UNICEF’s so-called ‘priority countries,’ of which there are 60,” she said. “That would be a good start.”<br /><br /><em><a href="http://web.mit.edu/newsoffice/topic/in-the-world.html">In The  World</a> </em><em>is a column that explores the ways members of the MIT community are developing technology — from the appropriately simple to the cutting edge — to help meet the needs of communities around the planet, especially those in the developing world. If you have suggestions for future columns, please e-mail <a href="mailto:newsoffice@mit.edu">newsoffice@mit.edu</a>.</em><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Mitchel Resnick]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/3q-resnick-scratch-0428.html</link>
<story_id>15256</story_id>
<featured>0</featured>
<description><![CDATA[Creator of the popular Scratch programming language discusses Apple’s decision to disallow Scratch viewer for iPhones and iPads.]]></description>
<postDate>Wed, 28 Apr 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100427140239-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100427140239-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100427140239-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[Mitchel Resnick, director of the Lifelong Kindergarten group at the Media Laboratory.]]></imageCaption>
</image>
<body><![CDATA[<em>The free, open-source Scratch programming language, developed by the Lifelong Kindergarten group at the MIT Media Lab by a team led by Mitchel Resnick, has found great popularity around the world as a way for young people (and adults) to learn about programming by manipulating colorful onscreen modules to create games, slideshows, interactive storybooks and other creative projects. More than a million such projects have been uploaded to the Scratch website for public sharing. But Apple this month, in a decision that immediately stirred controversy, rejected an App (developed independently by a programmer in Canada) that would allow viewing of these projects on iPhones and iPads. </em><br /><br /><em>In an interview with MIT News, Resnick shared his thoughts about Apple’s move — and what might happen next.</em><br /><br /><strong>Q.</strong> What is your reaction to the decision by Apple not to allow this Scratch viewer on iPads and iPhones?<br /><br /><strong>A.</strong> We on the Scratch team are disappointed at Apple’s decision. Our feeling is that it’s especially important for young people to have the opportunity to design, create and express themselves with new interactive media technologies — not just to view these things, but to create them and interact with them. We think it should be open to as many people as possible, and we’re interested in finding ways for people to design and share interactive media on all types of platforms, not just the iPad and the iPhone. It’s important for all platforms to support these opportunities. Right now, it’s a viewer that was not allowed, but over time we certainly would want to have the opportunity to include the ability to create interactive media on these platforms. From this decision, it would seem like they’re not going to allow that.<br /><br /><strong>Q.</strong> Is it purely a matter of Apple’s policies, or are there also technical difficulties in bringing the full Scratch programming system to platforms like smart phones and tablets?<br /><br /><strong>A.</strong> Apple doesn't allow languages like Flash or Java, and it doesn't allow apps that interpret or execute code  If you want kids to be able to create Scratch projects, there is no technical issue, it’s just Apple’s policies. We’ve done some work on a Flash version of Scratch — it’s not finished, but if Apple decided to allow Flash, we could make a version of Scratch for the iPad.  <br /><br /><strong>Q.</strong> Have you had any contact with Apple about the company’s decision?<br /><br /><strong>A.</strong> I haven’t personally. The specific Scratch viewer — we didn’t create that, it was done independently. The person who created it has had some interaction, and he has informed us about that. Over time, as we move farther along in creating our own versions, we may contact them.<br /><br />We're working on developing a new generation, which we call Scratch 2.0. We’ve started to get some funding. It will be able to do authoring inside web browsers, so you don’t have to do uploading and downloading [the present version requires users to download the software before creating projects]. We’d certainly prefer to do versions that could run across lots of platforms without needing conversion, so I think any restrictions on all kinds of platforms, not just Apple’s, could influence what we decide [on whether to use Flash or another language to create this version]. Our goal is to have the full range of capabilities of the existing version, plus more. By creating programs online, it allows you to do new things, like having it integrate with social media. For example, the program could react in a particular way when someone writes on my Facebook wall, or it could directly send out a Tweet, or could pull real-time data from the web such as a weather forecast or the latest baseball statistics. There are a variety of new features we expect it could have.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Computing, Sudoku-style]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/computation-0428.html</link>
<story_id>15257</story_id>
<featured>0</featured>
<description><![CDATA[Computer scientists generally see computation as something like following a recipe. Alexey Radul sees it as more like a puzzle with interconnecting parts.]]></description>
<postDate>Wed, 28 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100427171640-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100427171640-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100427171640-0.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='200' height='301'>http://web.mit.edu/newsoffice/images/article_images/20100427150028-2.jpg</fullURL>
<imageCaption><![CDATA[Alexey Radul]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[When Alexey Radul began graduate work at MIT’s Computer Science and Artificial Intelligence Lab in 2003, he was interested in natural-language processing — designing software that could understand ordinary written English. But he was so dissatisfied with the computer systems that natural-language researchers had to work with that, in his <a href="http://web.mit.edu/~axch/www/phd-thesis.pdf" target="_blank">dissertation</a>, he ended up investigating a new conceptual framework for computing. The work, which Radul is now pursuing as a postdoc in the lab of Gerald Sussman, the Matsushita Professor of Electrical Engineering, is still in its infancy. But it could someday have consequences for artificial-intelligence research, parallel computing and the design of computer hardware.<br /><br />Artificial-intelligence systems, Radul explains, often tackle problems in stages. A natural-language program trying to make sense of a page of written text, for instance, first determines where words and sentences begin and end; then it identifies each word’s probable part of speech; then it diagrams the grammatical structure of the sentences. Only then does it move on to stages with names like “scope resolution” and “anaphora.” The process might have a dozen stages in all.<br /><br />In a multistage process, however, errors compound from stage to stage. “Even if they’re really good stages, they’re 95 percent,” Radul says. “Ninety-five percent is considered extraordinary.” If each stage is 95 percent accurate, a five-stage process is 77 percent accurate; a 20-stage process — by no means unheard-of in AI research — is only 36 percent accurate.<br /><br />Systems that can feed information from later stages back to earlier stages can correct compounding errors, but they’re enormously complicated, and building them from scratch is prohibitively time consuming for most researchers. A few such single-purpose systems have been designed for particular applications, but they can’t easily be adapted to new problems.<br /><br /><strong>Branching out</strong><br /><br />Radul envisioned a new type of computer system that would handle multidirectional information flow automatically. Indeed, not only would it pass information forward and backward through stages of a multistage process, but it would pass data laterally, too: The results of one stage could be fed into, say, two others, which would attack a problem from different directions simultaneously, reconciling their answers before passing them on to the next stage. At that point, the stages of a process wouldn’t really be stages at all, but computational modules that could be arranged in parallel or in series, like elements in an electrical circuit. Programmers would simply specify how each module was connected to those around it, and the system would automatically pass information around until it found solutions that satisfied the constraints imposed by all the modules. <br /><br />This reconception of programming, however, required a commensurate reconception of computation. Classically, a computer is thought of as having two main parts: a logic circuit and a memory. The logic circuit fetches data from memory, performs an operation on the data, and ships back the results. Then it moves on to the next chunk of data. In Radul’s system, on the other hand, multiple logic circuits and memory cells are arranged in a large network. Any given logic circuit can exchange data with several different memory cells, and any given memory cell can exchange data with several different logic circuits.<br /><br />The danger with this arrangement is that logic circuits storing data in the same memory cell may arrive at contradictory conclusions. Which conclusion should the memory cell store? Instead of working together to solve a problem, the logic circuits could end up simply overwriting each other’s data.<br /><br />In the prototype system that he developed for his doctoral dissertation, Radul solved this problem by devising memory cells that don’t so much store data as gradually accumulate information <em>about</em> data. One logic circuit, for instance, might conclude that the value of some variable is between five and 15; the memory cell will register that the number it’s storing falls within that range. Another logic circuit, with access to the same memory cell, might conclude that the value of the variable is between 10 and 20; the memory cell would thus contract the range of the value it’s storing to between 10 and 15. A good analogy might be someone solving a Sudoku puzzle, who’s identified two or three candidate values for a puzzle square and jots them in the corner, expecting to winnow them down as new information comes to light.<br /><br /><strong>Owning up</strong><br /><br />A programmer using Radul’s system is free to decide what kinds of data about data the memory cells will store. But in his prototype, Radul enabled the memory cells to track where data comes from, a capacity that he thinks could be useful in a wide range of applications. In explaining this aspect of the system, Radul assigns the logic circuits arbitrary names. Say that a group of three logic circuits — Alice, Bob and Carol — converged on a value between 10 and 15 for some variable, but a fourth circuit — Dave — assigned the variable a value of 237. The system could warn the entire network that the results of Dave’s calculations are suspect and should be given less weight until new information propagating through the network brings them in line with everyone else’s. (It’s also possible, however, that the new information could vindicate Dave and force Alice, Bob and Carol to revise their initial conclusions.)<br /><br />Again, the Sudoku analogy might help. Sudoku solvers sometimes make mistakes; but once they’ve identified a mistake, it may already have propagated across the whole puzzle. Radul’s system would, in effect, automatically back out all the other errors that flow from the original mistake.<br /><br />Radul’s network of logic circuits and memory cells is an abstraction: It describes how information flows through a computer system, not necessarily the design of the system’s hardware. It so happens, however, that computer chip manufacturers have reached the point where the only way to improve performance is to add more “cores” — or logic circuits — to each chip. Splitting up programming tasks so that they can run, in parallel, on separate cores, is a problem that has bedeviled computer scientists. But a mature version of Radul’s framework would allow programmers to specify computational problems in a way that automatically takes advantage of parallelism.<br /><br />“All of computing — all of it, object-oriented, parallel, all those kinds of computing,” says Daniel Friedman, a professor of computer science at Indiana University, “they put all the responsibility on the programmer.” With a system like Radul’s, however, “large hunks of the responsibility would likely go away.” Friedman cautions that “there’s a huge amount of research to be done to demonstrate all that. All they’ve done so far is demonstrate it on a very small scale.” But, he adds, “this is spectacular stuff. I’m just looking for the right student to come along to get all fired up about it.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[New phenomenon found in internal waves]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/peacock-waves-0428.html</link>
<story_id>15258</story_id>
<featured>0</featured>
<description><![CDATA[MIT team shows that waves inside oceans, air and stars are filtered and reflected by layers.]]></description>
<postDate>Wed, 28 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100427151320-1.png</thumbURL>
<smallURL width='140' height='90'>http://web.mit.edu/newsoffice/images/article_images/w140/20100427151320-1.jpg</smallURL>
<fullURL width='368' height='237'>http://web.mit.edu/newsoffice/images/article_images/20100427151320-1.jpg</fullURL>
<imageCaption><![CDATA[Morning Glory clouds in Australia are one example of internal waves in the atmosphere.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='373'>http://web.mit.edu/newsoffice/images/article_images/20100427155352-1.jpg</fullURL>
<imageCaption><![CDATA[Image from laboratory experiment on internal waves shows both reflection and transmission of waves through a boundary between layers of water with differing densities]]></imageCaption>
</image>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100427155354-3.jpg</fullURL>
<imageCaption><![CDATA[Thomas Peacock, an associate professor of mechanical engineering, on the deck of a research ship during a recent trip to study internal waves in the ocean.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Internal waves — huge but nearly invisible ripples that occur in the oceans, the atmosphere and stars — can play an important role in climate change and other processes, but there is plenty about them that remains to be understood. In a significant new finding, however, MIT researchers have discovered that some important aspects of these waves can be analyzed using the same techniques physicists have developed to study the passage of light. <br /><br />“There’s a direct mathematical connection between how these waves travel in the ocean and the atmosphere, and the classical properties of an optical interferometer,” says Associate Professor of Mechanical Engineering Thomas Peacock. Optical interferometers use a pair of partially reflecting parallel mirrors to selectively block or transmit specific wavelengths of light, a technique widely used in fiber-optic communications systems, lasers and astronomical instruments. The method works because the beam of light is reflected back and forth between the two mirrors, and depending on the precise spacing between the mirrors the light beams that emerge at the other side will either reinforce each other (if the peaks of the waves match up) or weaken each other (if the peak of one wave lines up with the trough of another).<br /><br />Because the optical method is well studied and understood, this new insight should make it easier for researchers to probe the mysteries of internal waves in the air and sea. It also could enhance the accuracy of global climate models by helping scientists understand where the energy and momentum carried by such waves is deposited in the oceans. <br /><br />The new findings are described in a paper <a href="http://prl.aps.org/abstract/PRL/v104/i11/e118501" target="_blank">published recently</a> in <em>Physical Review Letters</em>, written by Peacock and mechanical engineering doctoral student Manikandan Mathur. The research was funded by the NSF and the Office of Naval Research.<br /><br /><strong>Layer by layer</strong><br /><br />Internal waves behave similarly to the waves on the surface of a body of water, but consist of the movement of regions within the water (or air) that have relatively different densities because of variations in temperature or composition (such as the amount of dissolved salt). Such waves can be very large, with heights of up to 100 meters and widths of hundreds of miles. In the ocean, such waves are usually generated when tides cause water to move over submerged mountains on the ocean floor; in the air, they are produced when wind blows over mountain ranges, or are triggered by large thunderstorms.<br /><br />Internal waves play an important role in mixing the different layers of water in the oceans, which in turn affects climate. The rate of such mixing remains one of the greatest areas of uncertainty in present climate models.<br /><br />The oceans are vertically stratified — that is, there is an increasing density with depth — because colder water and saltier water are heavier and so tend to sink. The boundaries between these layers of water with different strength of stratification, like the boundaries between two different kinds of glass, can cause waves propagating through them to bend.<br /><br />The key insight described in this research is that pairs of these boundaries behave like giant interferometers: When encountering boundaries between layers that have different density gradients, internal waves undergo the same kinds of reflections and strengthening or weakening effects as light waves in an optical interferometer. As a result, these layers selectively transmit or block waves of specific wavelengths.<br /><br />Peacock and Mathur calculated exactly how these interactions should work, based on classical optical theory, and then confirmed experimentally that this is indeed what happens by building an internal wave interferometer.<br /><br />To do that, they performed tests in a wave tank that was carefully prepared using salt-stratified water, in which they generated waves of precisely controlled wavelengths. They suspended microscopic glass beads in the water, which reflected beams of laser light they shone through the tank. This allowed the researchers to detect the movements of the invisible waves within the water, and it showed that the semi-reflective layers do indeed filter out specific wavelengths of the waves — just as in optics such mirrors can selectively allow passage of only certain colors.<br /><br /><strong>A link to light</strong><br /><br />This connection between the behavior of internal waves and the behavior of light had not previously been demonstrated, Mathur says. Now that they have shown the relationship, “it’s possible there are more connections between optics and internal waves. There may be other techniques already established in optics that we may be able to use to study internal waves.” <br /><br />Earlier work by Peacock and his students revealed details of how the waves are generated, but the new work provides significant new information about how they propagate through the oceans and the atmosphere. And although Peacock and his students did not specifically study such processes in their tests, the work could also help astrophysicists understand related processes in the interiors of stars.<br /><br />Thierry Dauxois, director of research at France’s National Center for Scientific Research (CNRS) in Lyon, says this work is significant both as an advance in basic scientific understanding, and for its practical implications in understanding global heat-distribution mechanisms. “The interferometry might have an effect in real systems, with possible geophysical applications in the ocean or in the atmosphere, and even possibly in stars where internal waves do exist,” he says. “Working on both aspects, fundamental physics and applications, is I think very appealing.”<br /><br />Dauxois, who was not involved in this research, adds that there is a crucial remaining question to be solved: “How these internal waves dissipate is a key question, largely open nowadays, and extremely important.”<br /><br />Peacock says he is working on addressing that problem as part of his ongoing research. For now, he says, the next step is to use the analytical methods developed in this research to make more specific predictions about particular regions of the ocean, and ultimately test those predictions in the field.  <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[New views at the nanoscale]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/nano-mri-0427.html</link>
<story_id>15245</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers are building a microscope that uses MRI technology to image viruses and other tiny biological structures.]]></description>
<postDate>Tue, 27 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100426121153-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100426121153-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100426121153-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Martino Poggio, University of Basel]]></imageCredits>
<imageCaption><![CDATA[In this diagram, viruses (colored orange) cling to the gold surface (yellow) at the end of a silicon cantilever. A magnetic tip (blue) creates a magnetic field that interacts with the viruses to create an image, using magnetic force resonance microscopy.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100426120744-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Graduate student Ye Tao adjusts the magnetic force resonance microscope that he and Assistant Professor Christian Degen are building at MIT.
 

 
]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100426121931-4.jpg</fullURL>
<imageCredits><![CDATA[Image: Christian Degen]]></imageCredits>
<imageCaption><![CDATA[An electron micrograph showing the tip of the cantilever, with virus particles attached.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Magnetic resonance imaging, first developed in the early 1970s, has become a standard diagnostic tool for cancer, cardiovascular disease and neurological disorders, among others. MRI is ideally suited to medical imaging because it offers an unparalleled three-dimensional glimpse inside living tissue without damaging the tissue. However, its use in scientific studies has been limited because it can’t image anything smaller than several cubic micrometers.<br /><br />Now scientists are combining the 3-D capability of MRI with the precision of a technique called <a href="http://ocw.mit.edu/courses/materials-science-and-engineering/3-052-nanomechanics-of-materials-and-biomaterials-spring-2007/" target="_blank">atomic force microscopy</a>. This combination enables 3-D visualization of tiny specimens such as viruses, cells and potentially structures inside cells — a 100-million-fold improvement over MRI used in hospitals. <br /><br />Last year, Christian Degen, MIT assistant professor of chemistry, and colleagues at the IBM Almaden Research Center, where Degen worked as a postdoctoral associate before coming to MIT, used that strategy to build the first MRI device that can capture 3-D images of viruses. Last weekend, <a href="http://www.pnas.org/content/106/5/1313" target="_blank">their paper</a> reporting the ability to take an MRI image of a tobacco mosaic virus was awarded the <a href="http://www.pnas.org/site/misc/cozzarelliprize.shtml" target="_blank">2009 Cozzarelli Prize</a> by the National Academy of Sciences, for scientific excellence and originality in the engineering and applied sciences category. <br /><br />“It’s by far the most sensitive MRI imaging technique that has been demonstrated,” says Raffi Budakian, assistant professor of physics at the University of Illinois at Urbana-Champaign, who was not part of the research team. <br /><br />Using nanoscale MRI to reveal the 3-D shapes of biological molecules offers a significant improvement over X-ray crystallography, which was key to discovering the double-helix structure of DNA but is not well suited to proteins because they are difficult to crystallize, says Budakian. “There’s really no other technique that can go in molecule by molecule and determine the structure,” he says. Figuring out such structures could help scientists learn more about diseases caused by malformed proteins and identify better drug targets. <br /><br /><strong>Improving on MRI</strong><br /><br />Traditional MRI takes advantage of the very faint magnetic signals emitted by hydrogen nuclei in the sample being imaged. When a powerful magnetic field is applied to the tissue, the nuclei’s magnetic spins align, generating a signal strong enough for an antenna to detect. However, the magnetic spins are so weak that a very large number of atoms (usually more than a trillion) are needed to generate an image, and the best possible resolution is about three millionths of a meter (about half the diameter of a red blood cell).<br /><br />In 1991, theoretical physicist John Sidles first proposed the idea of combining MRI with atomic force microscopy to image tiny biological structures. IBM physicists built the first microscope based on that approach, dubbed magnetic resonance force microscopy (MRFM), in 1993.<br /><br />Since then, researchers including Degen and his IBM colleagues have improved the technique to the point where it can produce 3-D images with resolution as low as five to 10 nanometers, or billionths of a meter. (A human hair is about 80,000 nanometers thick.)<br /><br />With MRFM, the sample to be examined is attached to the end of a tiny silicon cantilever (about 100 millionths of a meter long and 100 billionths of a meter wide). As a magnetic iron cobalt tip moves close to the sample, the atoms’ nuclear spins become attracted to it and generate a small force on the cantilever. The spins are then repeatedly flipped, causing the cantilever to gently sway back and forth in a synchronous motion. That displacement is measured with a laser beam to create a series of 2-D images of the sample, which are combined to generate a 3-D image. <br /><br />MRFM resolution is nearly as good (within a factor of 10) of the resolution of electron microscopy, the most sensitive imaging technique that biologists use today. However, unlike electron microscopy, MRFM can image delicate samples like viruses and cells without damaging them. <br /><br /><strong>New targets</strong><br /><br />Degen, who became interested in pursuing new MRI techniques after seeing an electron microscope demonstration in college, says his work could help structural biologists discover new drug targets for viruses.<br /><br />“Usually if you want to find out how things work, you have to find the structure. Otherwise you don’t know how to design drugs,” he says. “You’re operating in a blind spot.”<br /><br />Degen and chemistry graduate student Ye Tao are now building an MRFM microscope in the basement of MIT’s Building 2. When completed, the microscope will be one of just a handful of its kind in the world. Most of the parts are in place and functioning, but Degen and Tao still need to obtain the refrigeration unit that will cool the system to just above absolute zero. The system must be cooled to 50 millikelvins to minimize thermal vibrations, which interfere with the cantilever’s magnet-induced displacement signal.<br /><br />Degen hopes to receive the refrigeration unit in late May or early June, but shipment could be delayed by an ongoing shortage of helium isotopes, which are required to achieve the necessary cooling. If all goes according to plan, the microscope could be generating images by the end of this year.<br /><br />Degen and two of his students are also pursuing another new approach to nanoscale MRI. This approach uses fluorescence instead of magnetism to image samples. Their new microscope replaces the magnetic tip with a diamond that has a defect in its crystal structure. The defect, known as a nitrogen-vacancy defect, functions as a sensor because its fluorescence intensity is altered by interactions with magnetic spins. This setup does not have to be cooled, so samples can be imaged at room temperature.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Thermoelectricity]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/explained-thermoelectricity-0427.html</link>
<story_id>15246</story_id>
<featured>0</featured>
<description><![CDATA[Turning temperature differences directly into electricity could be an efficient way of harnessing heat that is wasted in cars and power plants.]]></description>
<postDate>Tue, 27 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100426122513-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100426122513-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100426122513-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[Thermoelectricity is a two-way process. It can refer either to the way a temperature difference between one side of a material and the other can produce electricity, or to the reverse: the way applying an electric current through a material can create a temperature difference between its two sides, which can be used to heat or cool things without combustion or moving parts. It is a field in which MIT has been doing pioneering work for decades.<br /><br />The first part of the thermoelectric effect, the conversion of heat to electricity, was discovered in 1821 by the Estonian physicist Thomas Seebeck and was explored in more detail by French physicist Jean Peltier, and it is sometimes referred to as the Peltier-Seebeck effect. <br /><br />The reverse phenomenon, where heating or cooling can be produced by running an electric current through a material, was discovered in 1851 by William Thomson, also known as Lord Kelvin (for whom the absolute Kelvin temperature scale is named), and is called the Thomson effect. The effect is caused by charge carriers within the material (either electrons, or places where an electron is missing, known as “holes”) diffusing from the hotter side to the cooler side, similarly to the way gas expands when it is heated. The thermoelectric property of a material is measured in volts per Kelvin.<br /><br />These effects, which are generally quite inefficient, began to be developed into practical products, such as power generators for spacecraft, in the 1960s by researchers including Paul Gray, the electrical engineering professor who would later become MIT’s president. This work has been carried forward since the 1990s by Institute Professor Mildred Dresselhaus, Theodore Harman and his co-workers at MIT’s Lincoln Laboratory, and other MIT researchers, who worked on developing new materials based on the semiconductors used in the computer and electronics industries to convert temperature differences more efficiently into electricity, and to use the reverse effect to produce heating and cooling devices with no moving parts. <br /><br />The fundamental problem in creating efficient thermoelectric materials is that they need to be good at conducting electricity, but not at conducting thermal energy. That way, one side can get hot while the other gets cold, instead of the material quickly equalizing the temperature. But in most materials, electrical and thermal conductivity go hand in hand. New nano-engineered materials provide a way around that, making it possible to fine-tune the thermal and electrical properties of the material. Some MIT groups, including ones led by professors Gang Chen and Michael Strano, have been developing such materials.<br /><br />Such systems are produced for the heating and cooling of a variety of things, such as car seats, food and beverage carriers, and computer chips. Also under development by researchers including MIT’s Anantha Chandrakasan are systems that use the Peltier-Seebeck effect to harvest waste heat, for everything from electronic devices to cars and powerplants, in order to produce usable electricity and thus improve overall efficiency.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Mastering multicore]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/multicore-0426.html</link>
<story_id>15243</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers find a way to make complex computer simulations run more efficiently on chips with multiple processors.]]></description>
<postDate>Mon, 26 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100423165727-1.png</thumbURL>
<smallURL width='140' height='126'>http://web.mit.edu/newsoffice/images/article_images/w140/20100423165727-1.jpg</smallURL>
<fullURL width='368' height='331'>http://web.mit.edu/newsoffice/images/article_images/20100423165727-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[MIT researchers have developed software that makes computer simulations of physical systems run much more efficiently on so-called multicore chips. In experiments involving chips with 24 separate cores — or processors — simulations of fluid flows were at least 50 percent more efficient with the new software than they were with conventional software. And that figure should only increase with the number of cores.<br /><br />Complex computer models — such as atom-by-atom simulations of physical materials, or high-resolution models of weather systems — typically run on multiple computers working in parallel. A software management system splits the model into separate computational tasks and distributes them among the computers. In the last five years or so, as multicore chips have become more common, researchers have simply transferred the old management systems over to them. But John Williams, professor of information engineering in the Department of Civil and Environmental Engineering (CEE) and <a href="http://esd.mit.edu/Faculty_Pages/williams/williams.htm" target="_blank">Engineering Systems Division</a>, CEE postdoc David Holmes, and Peter Tilke, a scientific adviser at oilfield services company Schlumberger and a visiting scientist in the Earth Resources Lab, have developed a new management system that exploits the idiosyncrasies of multicore chips to improve performance.<br /><br />To get a sense of what it might mean to split a model into separate tasks, consider a two-dimensional simulation of a weather system over some geographical area — like the animated weather maps on the nightly news. The simulation considers factors like temperature, humidity and wind speed, as measured at different weather stations, and tries to calculate how they will have changed a few minutes later. Then it takes the updated factors and performs the same set of calculations again, gradually projecting its model out across hours and days.<br /><br />Changes to the factors in a given area depend on the factors measured nearby, but not on the factors measured far away. So the computational problem can, in fact, be split up according to geographic proximity, with the weather in different areas being assigned to different computers — or cores. The same holds true for simulations of many other physical phenomena. <br /><br /> 



 

<br /><br /> <strong>Video:</strong> <em>A computer model simulates the falling of a drop of water by calculating the forces that individual molecules exert on each other. The simulation can be broken into chunks, each representing a cluster of neighboring molecules, that are processed in parallel by different processing units, or “cores.” </em><br /><br /><strong>Smaller is better</strong><br /><br />When such simulations run on a cluster of computers, the cluster’s management system tries to minimize the communication between computers, which is much slower than communication within a given computer. To do this, it splits the model into the largest chunks it can — in the case of the weather simulation, the largest geographical regions — so that it has to send them to the individual computers only once. That, however, requires it to guess in advance how long each chunk will take to execute. If it guesses wrong, the entire cluster has to wait for the slowest machine to finish its computation before moving on to the next part of the simulation. <br /><br />In a multicore chip, however, communication between cores, and between cores and memory, is much more efficient. So the MIT researchers’ system can break a simulation into much smaller chunks, which it loads into a queue. When a core finishes a calculation, it simply receives the next chunk in the queue. That also saves the system from having to estimate how long each chunk will take to execute. If one chunk takes an unexpectedly long time, it doesn’t matter: The other cores can keep working their way through the queue.<br /><br />Perhaps more important, smaller chunks means that the system is better able to handle the problem of boundaries. To return to the example of the weather simulation, factors measured along the edges of a chunk will affect factors in the adjacent chunks. In a cluster of computers, that means that computers working on adjacent chunks still have to use their low-bandwidth connections to communicate with each other about what’s happening at the boundaries.<br /><br />Multicore chips, however, have a memory bank called a cache, which is relatively small but can be accessed very efficiently. The MIT researchers’ management system can split a simulation into chunks that are so small that not only do they themselves fit in the cache, but so does information about the adjacent chunks. So a core working on one chunk can rapidly update factors along the boundaries of adjacent chunks.<br /><br /><strong>E pluribus unum</strong><br /><br />In theory, a single machine with 24 separate cores should be able to perform a simulation 24 times as rapidly as a machine with only one core. In the February issue of <em>Computer Physics Communications</em>, the MIT researchers report that, in their experiments, a 24-core machine using the existing management system was 14 times as fast as a single-core machine; but with their new management system, the same machine was about 22 times as fast. And, Williams says, the new system’s performance advantage compounds with the number of cores, “like compound interest over time.”<br /><br />Geoffrey Fox, professor of informatics at Indiana University, says that the MIT researchers’ system is “clever and elegant,” but he has doubts about its broad usefulness. The problems of greatest interest to many scientists and engineers, he says, are so large that they will still require clusters of computers, where the MIT researchers’ system offers scant advantages. “State-of-the-art problems will not run on single machines,” Fox says.<br /><br />But Holmes points out that the model that he and his colleagues used in their experiments was a simulation of fluid flow through an oilfield, which is of immediate interest to Schlumberger, which helped fund the research. “We’re running problems with 50, 60 million particles,” Holmes says, “which is on the order of 20, 30 gigabytes.” Holmes also points out that 24-core computers “will not remain the state of the art for long.” Manufacturers have already announced lines of 128-core computers, and that could just be the tip of the iceberg.<br /><br />Williams adds that, even for problems that still require clusters of computers, the new system would allow the individual machines within the clusters to operate more efficiently. “Cross-machine communication is one or two orders of magnitude slower than on-machine communication,” Williams says, “so it makes sense to keep cross-machine communication to a minimum, which is what our solution allows.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Liquid-solid interactions, as never before seen]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/solid-liquid-0426.html</link>
<story_id>15241</story_id>
<featured>0</featured>
<description><![CDATA[New technique improves researchers’ ability to measure a key property of material surfaces.]]></description>
<postDate>Mon, 26 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100423172508-0.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100423172508-0.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100423172508-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[An Atomic Force Microscope, which is used to capture high-resolution images of liquid-solid interfaces.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100423122636-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Kislon Voitchovsky, left, and Francesco Stellacci watch the monitor showing observations being made with the Atomic Force Microscope.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='588'>http://web.mit.edu/newsoffice/images/article_images/20100423174044-3.jpg</fullURL>
<imageCredits><![CDATA[Micrograph courtesy of Francesco Stellacci and Kislon Voitchovsky]]></imageCredits>
<imageCaption><![CDATA[Images taken through the Atomic Force Microscope using the MIT team’s new technique can show details of individual atoms and molecules at the interface between a liquid and a solid surface.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Wettability — the degree to which a liquid either spreads out over a surface or forms into droplets — is crucial to a wide variety of processes. It influences, for example, how easily a car’s windshield fogs up, and also affects the functioning of advanced batteries and fuel-cell systems. <br /><br />Until now, the only way to quantify this important characteristic of a material’s surface has been to measure the shapes of the droplets that form on it, and this method has very limited resolution. But a team of MIT researchers has found a way to obtain images that improves the resolution of such measurements by a factor of 10,000 or more, allowing for unprecedented precision in determining the details of the interactions between liquids and solid surfaces. In addition, the new method can be used to study curved, textured or complex solid surfaces, something that could not be done previously.<br /><br />“This is something that was unthinkable before,” says Francesco Stellacci, the Paul M. Cook Career Development Associate Professor of Materials Science and Engineering at MIT, leader of the team that developed the new method. “It allows us to make a map of the wetting,” that is, a detailed view of exactly how the liquid interacts with the surface down to the level of individual molecules or atoms, as opposed to just the average interaction of the whole droplet.<br /><br />The new method is described in a paper appearing on April 25 in the journal <em>Nature Nanotechnology</em>. The lead author is postdoctoral fellow Kislon Voïtchovsky, and the paper is coauthored by Stellacci and others at MIT, in England, and in Italy. Stellacci explains that the ability to get such detailed images is important for the study of such processes as catalysis, corrosion and the internal functioning of batteries and fuel cells, and many biological processes such as interactions between proteins.<br /><br />For example, Voïtchovsky says, in biological research, “you may have a very inhomogeneous sample, with all sorts of reactions going on all over the place. Now we can identify certain specific areas that trigger a reaction.”<br /><br />The method, developed with support from the Swiss National Science Foundation and the Packard Foundation, works by changing the programming that controls an <a href="http://ocw.mit.edu/courses/materials-science-and-engineering/3-052-nanomechanics-of-materials-and-biomaterials-spring-2007/" target="_blank">Atomic Force Microscope (AFM)</a>. This device uses a sharp point mounted on a vibrating cantilever, which scans the surface of a sample and reacts to topology and the properties of the sample to provide highly detailed images. Stellacci and his team have varied a key imaging parameter: They cause the point to vibrate only a few nanometers (as opposed to tens to hundred of nanometers, which is typical).<br /><br />“By doing so, you actually improve the resolution of the AFM,” Stellacci explains. The resulting resolution, fine enough to map the positions of individual atoms or molecules, is “unmatched before with commercial instruments,” he says. Such resolution had been achievable before with very expensive specialized AFMs, of which only a few exist in the world, but can now be equaled by the much more common commercial models, of which there are thousands. Stellacci and his colleagues think the improved resolution results from the way the vibrating tip causes the water to repeatedly push against the surface and dissipate its energy there, but this explanation remains to be tested and confirmed by other researchers.<br /><br />With their demonstration of both a 10,000-fold improvement in resolution for the specific function of measuring the wetting of surfaces and a 20-fold improvement in overall resolution of the lower-cost AFM, Stellacci says it’s not clear which of these applications will end up having more impact.<br /><br />Arvind Raman, a professor and university faculty scholar in mechanical engineering at Purdue University, agrees that these advances have significant potential. The method demonstrated by this team, which Raman was not involved in, “can routinely achieve atomic resolution on hard surfaces even with commercial AFM systems, and it provides great physical insight into the optimum conditions under which this can be achieved, both of which are very significant achievements,” he says. “I really think many in the AFM field will jump on this and try to use the technique.”<br /><br />Raman adds that while the team’s interpretation of why the method works as it does offers “one possible mechanism behind the image formation, other plausible mechanisms also exist and will need to be studied in the future to confirm the finding.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Esther Duflo wins Clark medal]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/duflo-clark-0423.html</link>
<story_id>15242</story_id>
<featured>0</featured>
<description><![CDATA[MIT’s influential poverty researcher heralded as best economist under age 40.]]></description>
<postDate>Fri, 23 Apr 2010 17:00:17 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100423130725-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100423130725-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100423130725-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Esther Duflo, the Abdul Latif Jameel Professor of Poverty Alleviation and Development Economics at MIT, was named the winner of the 2010 John Bates Clark medal.]]></imageCaption>
</image>
<body><![CDATA[MIT economist Esther Duflo PhD ‘99, whose influential research has prompted new ways of fighting poverty around the globe, was named winner today of the John Bates Clark medal. Duflo is the second woman to receive the award, which ranks below only the Nobel Prize in prestige within the economics profession and is considered a reliable indicator of future Nobel consideration (about 40 percent of past recipients have won a Nobel).<br /><br />Duflo, a 37-year-old native of France, is the Abdul Latif Jameel Professor of Poverty Alleviation and Development Economics at MIT and a director of MIT’s Abdul Latif Jameel Poverty Action Lab (J-PAL). Her work uses randomized field experiments to identify highly specific programs that can alleviate poverty, ranging from low-cost medical treatments to innovative education programs.<br /><br />Duflo, who officially found out about the medal via a phone call earlier today, says she regards the medal as “one for the team,” meaning the many researchers who have contributed to the renewal of development economics. “This is a great honor,” Duflo told MIT News. “Not only for me, but my colleagues and MIT. Development economics has changed radically over the last 10 years, and this is recognition of the work many people are doing.” <br /><br />The American Economic Association, which gives the Clark medal to the top economist under age 40, said Duflo had distinguished herself through “definitive contributions” in the field of development economics. “Through her research, mentoring of young scholars, and role in helping to direct the Abdul Latif Jameel Poverty Action Lab at MIT, she has played a major role in setting a new agenda for the field of development economics, one that focuses on microeconomic issues and relies heavily on large-scale field experiments,” the association said in a statement. <br /><br /><strong>Fighting poverty</strong><br /><br />In 2003, Duflo co-founded J-PAL along with one of her mentors and frequent collaborators, Abhijit Banerjee, MIT’s Ford International Professor of Economics, as well as economist Sendhil Mullainathan, now of Harvard. While Duflo’s own work has often focused on fieldwork in India and Kenya, J-PAL supports research in dozens of countries, and aims to work with both governments and nongovernmental organizations to implement anti-poverty programs. <br /><br />“The field has exploded over the last few years,” says Duflo. Her receipt of the Clark medal, she adds, “is a sign that the field is so alive. Many more young people are now working in development economics, and hopefully that will continue.” <br /><br />In one notable study, Duflo, together with Banerjee and J-PAL’s executive director, Rachel Glennerster, found that the rate at which families in northern India will immunize their children jumps from about 5 percent to nearly 40 percent when parents are offered a small bag of lentils as an incentive. Duflo, Harvard economist Michael Kremer, and economist Jonathan Robinson of the University of California, Santa Cruz, have run repeated experiments in Kenya that help farmers use fertilizer in a more efficient fashion. <br /><br />Much of Duflo’s work has analyzed educational practices. After an experiment involving more than 120 schools in Kenya, Duflo, Kremer, and Pascaline Dupas of UCLA concluded that dividing classes into groups based on student performance can help both high-achieving students (because they benefit from being around their strongest peers) and low-achieving students (because they can be taught at a level more comprehensible to them). In India, Duflo, Stephen Ryan of MIT and Rema Hanna of Harvard discovered that instructor attendance at rural, one-teacher schools improved notably when verified by date-stamped cameras and linked to salary; student performance improved as a result. <br /><br />While these research projects typically take place on small scales at first, J-PAL works to broaden the scope of successful experiments. After Kremer and economist Edward Miguel of the University of California at Berkeley showed that giving children medicine to free them of intestinal worms markedly helps school attendance, J-PAL helped start Deworm the World, a nonprofit organization that helped the Kenyan government treat 3.6 million children in 2009.<br /><br />“We are extremely happy for Esther and MIT,” said Ricardo Caballero, chair of the Department of Economics and the Ford International Professor of Economics, Macroeconomics and International Finance. “She has built one of the most successful academic careers I can recall in recent times while making a huge difference for the poor around the world. This award is the latest recognition to her superb work but surely not the last one. What a collection she is putting together.”<br /><br /><strong>A series of honors</strong><br /><br />The Clark medal is one of several prizes Duflo has been awarded recently. In 2009, she received a MacArthur Fellowship; was the first recipient of the Calvó Armengol International Prize from the Barcelona Graduate School of Economics; became a member of the American Academy of Arts and Sciences; and delivered a lecture series at the College de France in Paris, having been named that institution's first holder of its "Knowledge Against Poverty" chair. J-PAL claimed a significant new international prize in January 2009, the BBVA Foundation Frontiers of Knowledge Award in the category of Development Cooperation. <br /><br />MIT economist Paul Samuelson was given the first Clark medal, in 1947, while MIT graduate Emmanuel Saez PhD ’99 was awarded last year’s prize. The most recent MIT faculty member to win the medal, before Duflo, was Daron Acemoglu, now MIT’s Charles P. Kindleberger Professor of Applied Economics, in 2005. Prior to 2010, the Clark medal had only been awarded in odd-numbered years; now it is given annually. <br /><br />Past MIT faculty members who have won the Clark medal include Samuelson, Robert Solow (who won it in 1961), Jerry Hausman (1985), Paul Krugman (1991) and Acemoglu. MIT alumni who have won the award include Lawrence Klein PhD ’44 (1959), Joseph Stiglitz PhD ’67 (1979), Lawrence H. Summers ’75 (1993), Steven Levitt PhD ’94 (2003) and Saez. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: David Jones on heart problems]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/3q-jones-0423.html</link>
<story_id>15237</story_id>
<featured>0</featured>
<description><![CDATA[Physician and historian of medicine explains why bypass surgery is popular even though it fails to help many patients.]]></description>
<postDate>Fri, 23 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100422175053-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100422175053-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100422175053-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[<em>With a universal health-care law set to cover all United States citizens starting in 2013, many experts are now wondering how medical costs can be contained. David Jones, an MIT associate professor of the history and culture of science and technology, has a distinctive perspective on the subject. In addition to his PhD in the history of science, Jones received his MD from Harvard in 2001 and worked as a doctor before coming to MIT. He is currently writing a book about the history of cardiac procedures that explores, among other things, the rise in popularity of those interventions despite clinical evidence that their effectiveness is limited. MIT News asked him about his ongoing research about heart medicine. </em><br /><br /><strong> Q.</strong> Is cardiac bypass surgery an over-used procedure in the United States — and if so, what is the evidence for this? <br /><br /><strong> A.</strong> No surgical operation has been studied more carefully than coronary artery bypass grafting. Research has clearly identified particular groups of patients who benefit from the procedure, in terms of both relief of symptoms and prolongation of life. However, there is dramatic variation in the rate at which coronary artery bypass grafting is performed from place to place within the United States, despite a relatively consistent prevalence of the disease. This variation is even more striking internationally: Surgeons in Canada and the United Kingdom perform coronary artery bypass grafting at much lower rates than their colleagues in the United States, again without differences in the frequency of disease or evidence that Americans live longer or better lives as a result. The problem may be even larger with a related technique, coronary angioplasty, in which cardiologists use balloon-tipped catheters to open obstructed coronary arteries. Although experts disagree about the numbers, somewhere between 20 percent and 80 percent of angioplasties are done in patients who will receive no survival benefit. These disconnects raise many questions about how resources should be allocated and about how these decisions should be made. <br /><br /><strong> Q.</strong> As a historian, you've been conducting original research about the rise in popularity of bypass surgery. When did the procedure take hold as a standard treatment, and why has it remained so common? <br /><br /><strong> A.</strong> Surgeons first described the technique of coronary artery bypass grafting in 1968. It quickly became a popular treatment: By 1977, 100,000 were performed each year. The procedure peaked in the mid-1990s, at over 600,000 operations each year. Its early popularity was based on its physiological rationale. Doctors and patients believed that coronary artery disease was a problem of supply and demand: As atherosclerotic plaques grow, they choke off the flow of blood and oxygen to cardiac muscle. Bypass surgery seemingly made perfect sense: It provided a way of getting blood past those obstructions. Surgeons could use post-operative X-ray techniques to show that they had increased blood flow to the heart. This disease model — and this visual evidence — appeared so logical that surgeons were immediately convinced that bypass surgery must be effective. Skeptics, however, wanted to assess the surgery by using randomized clinical trials, the usual gold standard for determining the efficacy of medical treatments. As those trials have been done, they have usually shown that the surgery provides a survival benefit only to a small subset of patients.  <br /><br /> This has triggered an ongoing debate about the role of visual and statistical standards of knowledge in medical practice. Imagine a common scenario: A cardiologist performs angiography revealing that a patient has an 80 percent obstruction of a major coronary artery. Randomized controlled trials suggest that this patient will not obtain a survival benefit from either angioplasty or bypass surgery. But it is extraordinarily difficult for either a patients or a doctor to do nothing, knowing that the plaque is there and might someday cause a problem. One cardiologist has called this the "oculostenotic reflex": If you see a stenosing lesion [a narrowing] in a coronary artery, you feel obligated to intervene. <br /><br /><strong> Q.</strong> Apart from these diagnostic issues, how do the financial incentives of surgery affect the frequency of bypass procedures? <br /><br /><strong>A.</strong> In the early years of bypass surgery, physicians had significant conflicts of interest. Some, paid on a fee-for-service basis, made millions of dollars each year. Competition from angioplasty, however, changed the dynamic, and now interventional cardiologists [who perform angioplasty] have a higher average salary than cardiac surgeons. Reformers hope they can contain these procedures by putting physicians on salary, but this will not solve the problem: As long as surgeons and cardiologists bring substantial revenue streams into hospitals, they will continue to have high salaries and considerable institutional power. Such conflicts of interest are less direct, but still very present. At many medical centers, angiography, angioplasty, and bypass surgery provide one-third of the revenue. Lowering reimbursements, through reform of Medicare or health insurance, could have a major impact on medical practice, but would have other major consequences, since medical centers have become so dependent on the revenue from these procedures. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT to file amicus brief in intellectual-property case]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/amicus-brief.html</link>
<story_id>15233</story_id>
<featured>0</featured>
<description><![CDATA[Institute will support Stanford in asking U.S. Supreme Court to review dispute that could undermine technology-transfer system.]]></description>
<postDate>Thu, 22 Apr 2010 20:58:34 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100422170237-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100422170237-1.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100422170237-1.jpg</fullURL>
<imageCaption><![CDATA[The U.S. Supreme Court]]></imageCaption>
</image>
<body><![CDATA[MIT next week will urge the U.S. Supreme Court to review an intellectual-property case that the Institute says could have an adverse impact on a well-established technology-transfer system that produces extraordinary benefits to the nation and to the world.<br /><br />MIT's Office of the General Counsel will file an <em>amicus curiae</em>, or "friend of the court," brief with the nation's top court on Monday, April 26, in the case, <em>Stanford University v. Roche Molecular Systems, Inc., et al</em>. MIT argues that the case, which deals with a patent dispute, could jeopardize the important relationship between the federal government and research universities - a relationship that is widely credited with fostering America's technological and economic leadership over the last half-century.<br /><br />Senior administrators said MIT, which is not a party to the case, is taking the unusual step of getting involved because the issues raised in the case go to the heart of the Institute's culture and success.<br /><br />"MIT is an undisputed leader when it comes to transferring research breakthroughs into products and services that benefit the world, and it is therefore fitting that the Institute add its voice in this important case," said Lita Nelsen, director of MIT's Technology License Office.<br /><br />The brief argues that the case threatens to undermine the Bayh-Dole Act, a federal law that has played a central role in America's system of innovation. Enacted in 1980 as an amendment to the Patent Act, Bayh-Dole expanded and accelerated the transformation of ideas in the lab into the products, jobs and revenues of commercial enterprise.<br /><br />Under Bayh-Dole, universities are allowed to retain title to patents that result from innovations developed with federal funding. The schools may file patents on the inventions they own, and may grant exclusive rights to private companies.<br /><br />The Economist has referred to Bayh-Dole as "perhaps the most inspired piece of legislation to be enacted in America over the past half-century." But the success of Bayh-Dole is threatened by a federal appellate court's recent ruling in <em>Stanford v. Roche</em>, MIT's brief argues. <br /><br />In that case, Stanford University sued Roche in 2005 for patent infringement. A federal district court denied Roche's claim that it owned the intellectual property in question, but the appeals court disagreed, saying that Stanford lacked complete ownership of the patents due to the specific wording of an agreement that Stanford requires all of its researchers to sign.<br /><br />The MIT brief says that the appellate court's decision and its formalistic reading of the Stanford agreement were wrong because the court failed to consider the "clear sequence of ownership rights" defined by Bayh-Dole. "Had the court considered the import of the Bayh-Dole Act to the federal question of patent assignments, the court would have found that the chain of title led to Stanford and stopped there," the brief says.<br /><br />The brief says that if the appeals court ruling stands, it could divert the ownership of patents away from universities and the federal government. The brief asks that the Supreme Court agree to review the case; at a minimum, the brief says, the Supreme Court should ask the federal government to weigh in on the matter.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[In the search for Earthlike exoplanets, GJ 436b has much to tell us]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/exoneptunes-0422.html</link>
<story_id>15225</story_id>
<featured>0</featured>
<description><![CDATA[First detailed analysis of the atmosphere of a Neptune-sized planet reveals surprisingly low methane levels, presents "new territory" for researching planets outside our solar system]]></description>
<postDate>Thu, 22 Apr 2010 04:01:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100421184129-1.png</thumbURL>
<smallURL width='140' height='108'>http://web.mit.edu/newsoffice/images/article_images/w140/20100421184129-1.jpg</smallURL>
<fullURL width='368' height='284'>http://web.mit.edu/newsoffice/images/article_images/20100421184129-1.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA/JPL-Caltech]]></imageCredits>
<imageCaption><![CDATA[An unusual, methane-free world is partially eclipsed by its star in this artist's concept. NASA's Spitzer Space Telescope has found evidence that a hot, Neptune-sized planet orbiting a star beyond our sun lacks methane — an ingredient common to many planets in our own solar system. ]]></imageCaption>
</image>
<body><![CDATA[Although we have yet to find an exoplanet, or planet outside our solar system, that is small and cool enough to resemble Earth and possibly host life, planetary scientists say it could happen within the next decade. They are preparing for that possibility by learning as much as they can about the atmospheres of progressively smaller exoplanets that can be thought of as stepping stones on the path to discovering — and making sense of — an Earth twin.<br /><br />Recently, a team of researchers, including Sara Seager, the Ellen Swallow Richards Associate Professor of Planetary Science in the Department of Earth, Atmospheric and Planetary Sciences, and postdoctoral researcher Nikku Madhusudhan, studied the atmosphere of GJ 436b, an exoplanet located about 30 light years away that is similar in size to Neptune, which is less than four times the diameter of Earth. Although they are large planets with hydrogen-rich atmospheres, Seager describes these "exo-Neptunes" as important practice cases for preparing to make sense of Earthlike exoplanets.<br /><br />In a paper published Thursday in <em>Nature</em>, Seager, Madhusudhan and colleagues from the <a href="http://planets.ucf.edu/" target="_blank">University of Central Florida</a>, Columbia University and NASA report that the atmosphere of GJ 436b does not contain the high levels of methane that are expected for a planet with a temperature of about 800 Kelvin (about 1000 degrees Fahrenheit). Instead of an atmosphere with plenty of methane and very little carbon monoxide, the researchers detected the opposite. While it's not yet clear why this is the case, the result of the first atmospheric analysis of an exo-Neptune suggests that scientists have to be more flexible in their theories about the molecular makeup of the atmospheres of smaller and cooler planets.<br /><br />According to Madhusudhan, who led the interpretation of data about GJ 436b collected by NASA's Spitzer Space Telescope, the results "indicate that some form of extreme non-equilibrium chemistry must be taking place" in the planet's atmosphere. <br /><br />Equilibrium chemistry refers to the formula that tells you which molecules should appear in the atmosphere of a planetary body if you know its temperature and pressure. Disequilibrium occurs when the type and ratios of these molecules deviate from this calculation. Based on its temperature and pressure, Earth is not supposed to have oxygen in its atmosphere, but because life and plants pump oxygen into the atmosphere, it exhibits disequilibrium chemistry.<br /><br />Until now, scientists had assumed that the atmospheres of exo-Neptunes would exhibit equilibrium chemistry because detailed chemical modeling of exoplanetary atmospheres had not considered other possibilities. While it's not clear if disequilibrium chemistry can explain the chemical composition of GJ 436b's atmosphere, the authors believe that their work, which was funded by the National Science Foundation and NASA, will prompt a new generation of sophisticated computer models for analyzing smaller exoplanets that must now account for disequilibrium. <br /><br />"Whatever the explanation ends up being, the MIT team's calculation demonstrates that it's not compatible with the standard approach usually used for exoplanets," says Joe Harrington, a planetary sciences professor at UCF who worked with lead author and graduate student Kevin Stevenson. "We're in new territory."<br /><br /><strong>Flux analysis</strong><br /><br />Scientists can figure out an exoplanet's radius and mass by measuring the changes in the light as an exoplanet crosses in front of, or transits, its parent star. They can also determine the temperature and atmospheric composition of an exoplanet by collecting data when it passes behind, or is eclipsed by, its star. <br /><br />In 2008, Harrington and Stevenson used the Spitzer telescope to observe six eclipses of GJ 436b. The telescope measures different wavelengths of infrared light, including the flux, or amount of heat given off by a planet's atmosphere as it radiates light from its parent star. The flux values can then be used to figure out what molecules make up a planet's atmosphere. <br /><br />After averaging the flux values measured by the telescope during the eclipses, Madhusudhan developed a sophisticated computer program to determine the molecular compositions of the planet's atmosphere based on these values. The program combines certain variables, such as the planet's temperature, with different amounts of the most stable and prominent molecules that exist in planetary atmospheres, which are methane, carbon dioxide, carbon monoxide, water vapor and ammonia, into one formula that produces flux values. The program was designed to analyze millions of combinations of these variables and to keep track of which combinations most closely match the flux values reported by Harrington's group. Statistical analysis of these values helped Madhusudhan determine the most likely composition of the atmosphere.<br /><br />Based on equilibrium chemistry calculations, the researchers predicted that for a planet with a temperature of about 800K, methane would be the most dominant carbon-bearing molecule in its atmosphere. They also expected there would be a lot of water vapor and very little carbon monoxide. But their analysis indicates that the planet's atmosphere has a lot of carbon monoxide and very low levels of methane. <br /><br />The researchers suggest that the low methane levels might result from vertical mixing, or the process of carbon monoxide being transferred to higher parts of the atmosphere at a rate that is faster than it can be converted into methane. Another possibility is that the temperature on GJ 436b causes methane to be converted into other compounds.<br /><br />Adam Showman, a planetary scientist at the University of Arizona, says the "provocative result" raises questions about the evolution of this planet, as well as the possibility that its atmosphere might represent an entirely new class of atmospheres that has never been explored. This could mean that the planet is not necessarily a good analog for Neptune, although Showman says much more data is needed to make that conclusion.<br /><br />Seager agrees that there's probably a lot of "unknown chemistry" operating on the planet, and that the challenge is to understand the range of that chemistry and figure out what it means for GJ 436b and other planets. "We have our work cut out for us," she says. Although the team plans to collect more data to help refine their theories, they are limited by current telescope technology and eagerly await the 2014 launch of the James Webb Space Telescope, which will be able to obtain much higher quality data of exoplanet atmospheres, including those of "super Earths," or planets that are as small as two times Earth's radius. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Faculty approve flexible engineering degree for undergraduates]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/engineering-ba-0422.html</link>
<story_id>15224</story_id>
<featured>0</featured>
<description><![CDATA[Approval of AeroAstro program adds a new dimension to engineering education.]]></description>
<postDate>Thu, 22 Apr 2010 04:00:05 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100421183056-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100421183056-1.jpg</smallURL>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100421183056-1.jpg</fullURL>
</image>
<body><![CDATA[Institute faculty voted yesterday to approve a flexible degree to be offered by the Department of Aeronautics and Astronautics to undergraduates who are interested in a more multi- and interdisciplinary engineering curriculum.<br /><br />The proposal will create a new degree within AeroAstro — 16-ENG — and graduates of the program will earn an "SB in Engineering," as recommended by the department. The new option in AeroAstro is part of a broad, schoolwide effort through which any of the School of Engineering's academic departments will be able to offer their own flexible "ENG" degree program. Undergraduate students who pursue these flexible majors will receive a more interdisciplinary engineering education, but one that still features the rigor and technical depth of the school's traditional engineering degrees. <br /><br />The flexible degree program addresses students' interest in tackling major global problems, such as energy, transportation, climate change and poverty, according to Subra Suresh, dean of the School of Engineering and Vannevar Bush Professor of Engineering. "This program will also allow students to pursue topics that are inherently interdisciplinary — like robotics, computational engineering or transportation — in greater depth," Suresh said. In addition, the option is intended to lower perceived barriers between engineering and other fields that can be pursued at the postgraduate level. "Studying engineering at MIT can be a gateway to many things," he added. "MIT students are interested not only in a disciplinary engineering degree, but also in addressing broad and complex problems that affect the world, and we can help them by making an engineering degree more appealing and more suited to this wide range of application — while preserving depth and rigor that characterize an MIT education."<br /><br />AeroAstro's Course 16-ENG was in large measure inspired by Mechanical Engineering's Course 2-A, a customizable degree program that has been offered by MechE since 1934 and gained ABET accreditation in 2002. Both degrees are structured around a core set of subjects offered by the department. Students then complement that core with a broad concentration area comprised of different subjects drawn from across the Institute. For example, an AeroAstro student who concentrates in energy could choose to take courses from an exhaustive list of subjects based in MechE, the Sloan School of Management, the School of Science or the School of Architecture. Among AeroAstro's suggested 16-ENG concentrations are energy, robotics and control, environment, engineering management, space exploration and computational engineering — with others being developed.<br /><br />Dean for Undergraduate Education Daniel Hastings said that the new degree program is an explicit recognition by the Institute of the national need for engineering education that goes beyond narrow disciplines: "For energy, climate change and international development, these issues aren't uniquely about engineering. They also involve politics, economics, systems thinking — you name it. Students need to know about all of these areas to make progress."<br /><br />Not only are MIT students more interested in such cross-disciplinary pursuits, but MIT research as a whole is also much more interdisciplinary than it was 30 years ago. "This culture of multi-disciplinary research is one of MIT's great strengths, and the flexible degree programs are simply another expansion of this culture into education," said Cynthia Barnhart, associate dean for academic affairs in the School of Engineering, and professor of Civil and Environmental Engineering and Engineering Systems.<br /> <br /><strong>Developing the degree</strong><br /><br />The AeroAstro proposal was developed after Suresh became dean in July 2007 and formed several strategic planning committees to consider the direction of the school. One committee recommended developing flexible engineering degrees for students who wanted a multidisciplinary approach to their education without having to take additional classes on top of their already demanding course load.<br /><br />Suresh asked Ian Waitz, head of AeroAstro and the Jerome C. Hunsaker Professor, to lead a separate committee of six engineering department heads to explore this possibility. That committee unanimously proposed a schoolwide flexible degree program into which individual departments can choose to opt. "You have to have that grassroots-based interest in serving departmental and students' needs when developing curricula at MIT," Waitz said. "For a field like aerospace, that has historically been multidisciplinary, this degree makes sense," he added.  This degree program, which will be offered in parallel with traditional degrees, was approved by all the appropriate Institute committees before it was presented at the faculty meeting.<br /><br />AeroAstro was interested in developing a flexible degree as soon as possible so that sophomores could enroll in it this fall. Working closely with colleagues from Mechanical Engineering — Mary Boyce, head of MechE, John Lienhard, department head for education, and Peko Hosoi, an associate professor in MechE — Waitz and David Darmofal, an associate professor in AeroAstro and associate department head, developed their flexible degree along the same lines as Course 2-A. The customizable degree offered by MechE has seen dramatic growth in enrollment in recent years. About 45 percent of MechE's incoming sophomores chose the degree option this year, according to Boyce, who said that this growth does not detract from enrollment in the department's other degree options. <br /><br />Another reason AeroAstro moved so swiftly to get the degree approved was to make sure that enough data would be available to support accreditation by ABET in 2013. Waitz stressed that this accreditation is important for flexible engineering degrees at MIT in order to validate the technical rigor that is consistent with the school's other accredited degrees. "If anything, we think the degree will be even more challenging and rigorous," he said, adding that his department recognizes that increased mentoring will be required now that students have more choices available to them.<br /><br />From the school's perspective, the program offers an opportunity to coordinate subject offerings across departments and schools, which will help to identify holes and eliminate redundancies in the curriculum. <br /><br />Suresh has established a standing committee with members from each department to enable curriculum coordination for the new degree program, which will be closely monitored with careful and aggressive evaluation after three years.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Bill Gates on MIT]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/3q-gates-0422.html</link>
<story_id>15227</story_id>
<featured>0</featured>
<description><![CDATA[After speaking about the importance of giving back, the philanthropist and Microsoft co-founder spoke to MIT News about innovation and learning at MIT]]></description>
<postDate>Thu, 22 Apr 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100421232128-1.png</thumbURL>
<smallURL width='140' height='92'>http://web.mit.edu/newsoffice/images/article_images/w140/20100421232128-1.png</smallURL>
<fullURL width='368' height='243'>http://web.mit.edu/newsoffice/images/article_images/20100421232128-1.png</fullURL>
<imageCredits><![CDATA[Photo: Justin Knight]]></imageCredits>
<imageCaption><![CDATA[Bill Gates meets with MIT faculty prior to his talk in Kresge Auditorium to discuss issues related to global poverty, including how MIT's OpenCourseWare (OCW) can be shared more effectively with the world.]]></imageCaption>
</image>
<body><![CDATA[<em>Bill Gates, in his role as co-chair of the Bill &amp; Melinda Gates Foundation, visited MIT on Wednesday as part of a three-day tour of five universities across the country that was aimed at inspiring the brightest minds to tackle the world's biggest problems. Before speaking at Kresge Auditorium and taking questions from students in the audience, he met with faculty to discuss issues related to global poverty, including how MIT's OpenCourseWare (OCW) can be shared more effectively with the world. Gates also met with students who presented their research related to global health and poverty, including malaria diagnostic tests in Africa and an open-source, cell-phone based telemedicine system known as Sana that extends medical diagnoses in remote and conflict-ridden regions. After his talk in Kresge, Gates sat down with MIT News to share what he learned during his visit.</em><br /><br /><strong>Q.</strong> Was there an idea or two in particular that you learned about here at MIT this morning that stuck with you or that perhaps was something you hadn't seen so far on your college tour?<br /><br /><strong>A.</strong> Yeah, there were a lot of good ones. The malaria thing, it was great to get that update. That's work that our Foundation is involved with. The cell phone stuff — it was good to see the progress, but that's not yet actually saving lives, and actually mapping it to practice in the field, it'll be interesting to see how that goes. I was glad to see that people are pushing, because I think that eventually, something will be found there, but that's very tricky. So, there was a very good dialogue. I talked a lot with the OpenCourseWare people about what the opportunities are to work together to take that to a new level. It's great the way it is, but why can't we have 20 times the current level of usage? Victor Zue had some ideas about applying technology to it, and we talked about how we draw other universities into that.<br /><br /><strong>Q.</strong> How much time do you personally spend simply learning? How much time do you spend watching videos of MIT lectures?<br /><br /><strong>A.</strong> Of the 33 courses that are available completely in video, there are 11 that I've watched most of. Three of those are Walter Lewin's physics courses that are absolutely superb. One is the Don Sadoway course, and there's an Eric Lander biology course. As I scanned the site last night just to see what was new, I saw a couple that I now know are up there that I've got to go and take — one on thermodynamics, one on structures — that I'm looking forward to. There's a lot of universities doing online courses, but I've watched more MIT ones than any others. <br /><br /><strong>Q.</strong> Does the work of your foundation resonate with college students? Are students as a whole as aware of the world's problems as you think they should be?<br /><br /><strong>A. </strong>Well, no, not as they should be, but the situation is dramatically better in that respect than it was when I went to college. There was no course on poverty, and MIT started one last year that sounds like that's going to be a great thing, and it could get significant attendance. So, no, there's a lot more that can be done. But things like OpenCourseWare, D-Lab, the group that brings students together to talk about poverty — even if all the universities were doing as much as MIT, that would be an improvement. But the idea was to start a dialog here about how MIT can even move up from the level that it's at today.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Bill Gates visits MIT]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/gates-visit-0422.html</link>
<story_id>15226</story_id>
<featured>0</featured>
<description><![CDATA[Calls for a spirit of service]]></description>
<postDate>Thu, 22 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100421230329-1.png</thumbURL>
<smallURL width='140' height='92'>http://web.mit.edu/newsoffice/images/article_images/w140/20100421230329-1.png</smallURL>
<fullURL width='368' height='243'>http://web.mit.edu/newsoffice/images/article_images/20100421230329-1.png</fullURL>
<imageCredits><![CDATA[Photo: Justin Knight]]></imageCredits>
<imageCaption><![CDATA[Philanthropist and Microsoft co-founder Bill Gates delivers his lecture at Kresge Auditorium Wednesday as part of his Campus Tour.]]></imageCaption>
</image>
<body><![CDATA[Bill Gates, philanthropist and retired co-founder of Microsoft Corp., urged MIT students on Wednesday to focus their talent and energy on tackling the world's biggest challenges, including global health, poverty and education.<br /><br />Gates' visit was part of a three-day tour of five universities that was intended to inspire students and faculty to focus on issues of inequity. Although he had completed similar college visits while at Microsoft, this trip marked Gates' first college tour since he assumed full-time duties at the Bill &amp; Melinda Gates Foundation nearly two years ago. With a $33.5 billion endowment, the Seattle-based foundation works to help all people lead healthy, productive lives.<br /><br />Titled "Giving Back: Finding the Best Way to Make A Difference," Gates's 30-minute presentation followed a private roundtable discussion with several faculty members about their work to fight global poverty. During that discussion, he also talked about how to expand MIT's OpenCourseWare (OCW), which is a free online portal that provides access to about 1,900 MIT courses, and which Gates uses frequently. He then met with a handful of students to hear about their research in global health, poverty and education.<br /><br />Gates was eager to learn during his visit, opening his presentation with a two-part question. "Are the brightest minds working on the most important problems?" he asked a packed Kresge Auditorium. "And to the degree that they're not, how could we increase that, which I think could make a huge difference?"<br /><br />Those questions were inspired by a weekend Gates recently spent with several friends who were eager to talk about two topics they found exciting: the NCAA March Madness basketball tournament and financial markets. That got Gates thinking about how he could shift some of the focus of the brightest minds from these popular topics toward solving problems that plague poor countries, such as health, food, sanitation and governance, as well as more global issues like education and energy. Even in areas of scientific innovation, a lot of the focus remains geared toward the perceived needs of the rich, such as baldness drugs, Gates said.<br /><br /><strong>Tackling vaccines and education</strong><br /><br />Gates showed only one graph during his talk: his "favorite chart in the world," which describes the decline in death rates of children below age five since 1960, when about 20 million children younger than five died. In 2009, that figured had dropped to about 8.8 million, a decline that Gates attributed largely to vaccinations. Despite the immense health benefit, however, less than one percent of medical spending goes toward funding vaccines, according to Gates.<br /><br />In addition to global health, Gates talked about fledgling education systems, particularly in the U.S., where he said it would take some type of dramatic breakthrough to make it possible to make a great education available to all who want one, especially as states continue to cut education funding. He praised MIT for being at the forefront of improving access to education through technology, thanks to the OCW collection of online courses. He said he is a "super happy user" of the collection and has watched 11 of the 33 available video courses to date. <br /><br />But other than improving access and developing a unified system to measure teacher effectiveness, Gates admitted that figuring out how to tackle education remains unclear, partly because the area has received very modest funding, and partly because of bureaucratic resistance to changing the status quo.<br /><br />Still, Gates remains optimistic. "There are reasons to believe we can make progress, but that rate of progress will be somewhat proportional to how we draw people in," he said, sharing his dream that within a few years, he will engage in conversation with bright minds who are eager to discuss teaching.<br /><br />"Yes, we might delay the invention of a new financial product by a few years, or we might even delay that baldness drug by a few years, but if it helps on the important problems, I think it's a good thing," he said.<br /><br /><strong>On hamburgers and giving back</strong><br /><br />Gates answered several questions from students in the audience, including his views on the role of nuclear power, which he believes needs research funding and innovation. In response to a question about how it feels to be the richest person in the world, Gates said he still hasn't "found any burgers at any price that are better than McDonald's," and that after a person attains a certain net worth — a few million dollars — adding personal wealth doesn't mean much. Success becomes, at that point, "all about how you're going to give it back." <br /><br />That message struck many of the students who attended the talk, including freshman Jason Elizalde, who was impressed with how easily Gates interacted with students before the event. "He was cracking jokes and just seems so down to Earth and like all that money hasn't gotten to him," he said.<br /><br />Nada Hashmi, a PhD candidate at MIT's Sloan School of Management, was grateful for the feedback Gates gave her when she met with him before the event to discuss her business idea for developing a mobile healthcare system that would send vans to remote villages in Saudi Arabia and other areas in the Middle East and Asia to distribute free healthcare. "He forced me to think of the big picture and challenged me to tackle health issues at a global level," Hashmi said. "It was great to see how people who have made it big concern themselves with giving back. It is something we should all aspire to as we go through our lives."<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The long road for Haiti]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/haiti-reconstruction-0421.html</link>
<story_id>15220</story_id>
<featured>0</featured>
<description><![CDATA[Haitian leaders speak at MIT about reconstruction]]></description>
<postDate>Wed, 21 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100420151612-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100420151612-1.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100420151612-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: U.N. development programme]]></imageCredits>
<imageCaption><![CDATA[The devastation of Port au Prince is seen from the sky after the January earthquake.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100420165752-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Frantz Verella, left, Haiti's former Minister of Public Works, Transport and Communication; and Patrick Delatour, right, Chairman of the Presidential Commission for Reconstruction and the Minister of Tourism in Haiti, spoke to the MIT community about rebuilding the devastated nation.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[MIT can help rebuild Haiti by sharing its expertise in engineering and urban planning, providing technical training in areas like database management and wireless technology and building relationships with universities in Haiti, Frantz Verella, Haiti's former minister of Public Works, Transport and Communications, told the MIT community at a presentation on Friday. <br /><br /> Joined by Patrick Delatour, Chairman of the Presidential Commission for Reconstruction and the Minister of Tourism, Verella spoke about actions taken by the Haitian government since January's devastating earthquake, as well as the government's vision for reconstruction over the next 20 years. Titled "Haiti: the Plan for Reconstruction," the presentation was the first of several forums to be sponsored by MIT's Committee on Race and Diversity over the next year.  <br /><br /> "With issues of race and diversity, there is no better topic to begin with," said J. Phillip Thompson, co-chair of the committee and professor in the Department of Urban Studies and Planning, explaining why the committee's first forum was focused on Haiti, where an estimated 300,000 people have died, and where 300,000 have been wounded and 1.3 million left homeless as a result of the earthquake. <br /><br /> Delatour said that although Port-au-Prince will remain the capital city, the reconstruction effort calls for decentralizing Haiti by populating smaller regional areas that will be connected by a national transportation network. The goal, Delatour explained, is for Haiti to emerge by 2030 "with an economy that is modern, dynamic and competitive." One way of doing that is to make tourism a top priority for economic recovery, he said. In the short term, the government plans to rebuild devastated zones and decongest settlement areas, and in the long term, it plans to provide resources like hospitals, schools and post offices in regional areas outside Port-au-Prince.  <br /><br /> Verella, an economist, engineer and former dean of the Faculty of Science at the University of Haiti, explained that Haiti's extreme vulnerability to natural disaster was compounded by a weak, corrupt state. He said that rebuilding efforts will focus specifically on areas that are less prone to natural disaster. <br /><br /> "We cannot in Haiti afford to go back to the status quo," said Verella, joining Delatour in stressing that for Haiti to truly rebuild itself, the reconstruction cannot be led by outsiders or NGOs, but must be carried out by a stronger, better Haitian government that is guided foremost by the interests of its citizens. Still, he welcomed outside guidance, noting that MIT could play a major role by establishing a center for excellence in infrastructure and urban planning. <br /><br /> Co-sponsored by MIT's Haiti Coordinating Committee, the presentation was part of MIT's ongoing effort to provide short- and long-term relief to the devastated country. As MIT News reported in March, two Media Lab students, Greg Elliott and Aaron Zinman, have developed Konbit, a free interactive communication platform that helps communities rebuild after a crisis by indexing the skills of local residents so that NGOs can quickly find and employ them. Konbit allows Haitians, their diaspora and the international community to report their skills by phone, text message or web.  <br /><br /> Since March, the beta version of the site has gone live, and Elliott and Zinman have partnered with telecommunications provider Digicel, have an agreement with Google, and are in talks with the U.N. Development Program to continue developing the social infrastructure for platform. They are currently searching for 100 volunteers to test Konbit, which is designed so that messages in native Creole can be translated by volunteer Haitians and then transcribed into a searchable database for NGOs. After the volunteer phase, Elliott and Zinman will conduct a short ethnographic study of Konbit involving Haitian students to examine interaction patterns and user experience before deploying the final system. <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: Gustavo Dudamel makes whirlwind visit to MIT]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/ss-dudamel-0421.html</link>
<story_id>15219</story_id>
<featured>0</featured>
<description><![CDATA[Conducts, hosts rehearsal as part of McDermott Award weekend]]></description>
<postDate>Wed, 21 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100420113451-9.png</thumbURL>
<smallURL width='140' height='78'>http://web.mit.edu/newsoffice/images/article_images/w140/20100420113451-9.jpg</smallURL>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113451-9.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Freshman violinist Shelby Heinecke concentrates as Dudamel conducts the students in the MIT Symphony Orchestra.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420112859-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[McDermott Award winner Gustavo Dudamel meets MIT President Susan Hockfeld before the April 16 award presentation. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420112900-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Gustavo Dudamel with Council for the Arts member Eran Egozy, MIT '95, co-founder and chief technical officer of Harmonix Music Systems, creators of the original Guitar Hero, Rock Band, Rock Band 2, and The Beatles Rock Band.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100421103222-12.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Dudamel with Professor of Music and Media Tod Machover, demonstrating a robot and musical software created in the Media Lab.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420112900-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Council for the Arts Chair Brit d'Arbeloff MIT '61 presents the 2010 McDermott Award in the Arts to Gustavo Dudamel, as MIT President Susan Hockfield looks on.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420112900-4.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[When the maestro speaks, students listen.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113119-5.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[An upbeat from Gustavo Dudamel inspires the MIT Symphony Orchestra in a public rehearsal on April 16.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113120-6.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Projectors helped the overflow Kresge Audience see the students respond to McDermott Award winner Gustavo Dudamel's direction.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113121-7.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[A rear projection gives the Kresge Audience a close up view as McDermott Award winner Gustavo Dudamel works with the MIT Symphony Orchestra.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113122-8.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[The French horn section (Vanessa Gardner, Peter James, Jared Krueger and others) responds to Dudamel's direction.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113451-10.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[An energetic Gustavo Dudamel leads the MIT Symphony Orchestra.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113452-11.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[The capacity crowd in Kresge Auditorium shows their appreciation for Dudamel and the students of MITSO.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='207'>http://web.mit.edu/newsoffice/images/article_images/20100420113452-12.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Gustavo Dudamel surrounded by exuberant members of MITSO following the Friday night rehearsal]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<br /><br />Venezuelan composer Gustavo Dudamel — conductor of Simón Bolívar Youth Orchestra in Caracas, the Gothenburg Symphony in Sweden, the Los Angeles Philharmonic and one of the most sought-after conductors worldwide came to MIT April 16-17 to accept the $75,000 Eugene McDermott Award in the Arts presented by the Council for the Arts at MIT. In accepting the award, Dudamel took part in a whirlwind of activities on campus. <br /><br /> While at MIT, Dudamel led the MIT Symphony Orchestra in a public rehearsal of Mozart's Symphony No. 38 ("Prague") and Rimsky-Korsakov's "Capriccio Espagnol," and he joined MIT professors John Harbison and Tod Machover in a panel discussion moderated by PBS journalist Maria Hinojosa on music as a social and educational project highlighting El Sistema and Youth Orchestra of Los Angeles. <br /><br /> "Dudamel gave the orchestra an insightful, honest and helpful dose of what it's like to work with a world-class conductor," Keith Powers reported in <em>The</em><em> Boston Herald</em>. "His suggestions and musical insights ranged from the quality of sostenuto to how to eat a hamburger, and every word of it got soaked up. The players were well prepared and certainly capable, and by the end of the session were playing with a quite sophisticated understanding of the potential in seemingly simple Mozartean phrases." <br /><br /> Graduate student Aravind Ratnam blogged about the rehearsal, writing, "Today's MIT Symphony Orchestra rehearsal conducted by Gustavo Dudamel was nothing short of spectacular ... Gustavo's face is euphoric with the reflection of the sun coming up. It is spring and you can almost smell the flowers ... I was on cloud nine throughout. It is not very often that I am moved to tears." <br /><br />]]></body>
</item>
<item>
<title><![CDATA[Guerrilla reporting in ‘difficult places’]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/cfcm-guerrilla-0420.html</link>
<story_id>15213</story_id>
<featured>0</featured>
<description><![CDATA[Activists describe their experiences using new technology to build free media networks in countries with scant resources or oppressive regimes.]]></description>
<postDate>Tue, 20 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100416172011-1.png</thumbURL>
<smallURL width='140' height='125'>http://web.mit.edu/newsoffice/images/article_images/w140/20100416172011-1.jpg</smallURL>
<fullURL width='368' height='329'>http://web.mit.edu/newsoffice/images/article_images/20100416172011-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='240'>http://web.mit.edu/newsoffice/images/article_images/20100418120301-2.jpg</fullURL>
<imageCaption><![CDATA[The event included a series of interviews, using the Web-based videoconferencing software Skype, with activists around the world.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In mid-2009, when thousands of Iranians took to the streets amid allegations of fraud in the June presidential election, the images of the protests that reached the West were almost exclusively those captured by ordinary citizens on cell phones and digital cameras and disseminated over the Internet, circumventing the government’s information clampdown. So Iran was a fitting place to begin the virtual tour of the world that took place last Thursday evening at MIT’s Stata Center, under the auspices of MIT’s Center for Future Civic Media and the title “Civics in Difficult Places.”<br /><br />Hosted by Ethan Zuckerman, a fellow at both Harvard’s Berkman Center for Internet and Society and the Center for Future Civic Media, the two-hour event was a series of interviews, using the Web-based videoconferencing software Skype, with activists around the world who are helping create grassroots media networks in countries with hostile political environments or scant resources.<br /><br />From the diverse experiences of the participants, a few common themes emerged. One was the sometimes-surprising resilience of new-media networks. <a href="http://www.caribbeanfreeradio.com/" target="_blank">Georgia Popplewell</a>, a blogger and podcaster based in Trinidad and Tobago who works with Zuckerman’s organization <a href="http://globalvoicesonline.org/" target="_blank">Global Voices</a>, said that while the recent earthquake in Haiti knocked out almost all of the country’s radio stations, Haitians remained able to communicate with each other and with the outside world — and prominent radio personalities could still reach their audiences — over Twitter and other social-media services. Ruthie Ackerman, who created the blog <a href="http://ceasefireliberia.com/" target="_blank">Ceasefire Liberia</a> as a bridge between Liberians in Liberia and those in the diaspora — and particularly the large Liberian community in Staten Island — assumed that Liberians in the U.S., with their vastly superior Internet connections, would post updates to the blog more often than their counterparts in Liberia, where Internet service is “virtually nonexistent” outside the capital, Monrovia, and even there, it’s expensive and slow. But the reverse proved to be true. “I think that the reason why is that there’s this sense of urgency in Liberia,” Ackerman said, “that they really want to get their stories out there, and that, if they don’t tell their stories, they won’t be told.” And Cameran Ashraf of <a href="http://www.accessnow.org/" target="_blank">AccessNow</a>, an organization born of the Iranian protests that, among other things, helps Iranians read and contribute to dissident websites without leaving any digital tracks, pointed out that even as the Iranian government shut down news outlets, arrested journalists and tried to filter Facebook and Twitter, it stopped short of cutting off Iranians’ Internet access entirely. Ashraf believes that, in a country like Iran, where 30 percent of people have Internet access, local economies are already so dependent on the Web that shutting it down would have been “adding fuel to the fire of an already tense situation.”<br /><br />Another recurring theme was the ingenuity with which people in “difficult places” use new technologies. <a href="http://www.humayusuf.com.pk/" target="_blank">Huma Yusuf</a>, a reporter for Pakistan’s largest English-language newspaper <em><a href="http://www.dawn.com" target="_blank">Dawn</a></em>, is involved in a project to set up local radio stations along the Pakistan-Afghanistan border, where literacy rates are so low that new-media networks would have limited reach. Cell-phone calls in Pakistan are expensive, Yusuf said, so reporters for the radio stations have established a code where multiple hangups convey different information — such as an imminent encounter with the military that could make communication difficult for a few hours. And Brenda Burrell and Bev Clark, who provide an <a href="http://kubatana.net/" target="_blank">uncensored outlet</a> for news in Zimbabwe, discussed their new project, <a href="http://www.freedomfone.org/" target="_blank">Freedom Fone</a>. Like Yusuf, they’re trying to create community radio stations; unlike Yusuf, they haven’t been granted access to radio spectrum. But cell-phone plans in Zimbabwe are much more affordable than they are in Pakistan. So Burrell and Clark have created a virtual radio station that allows the user to select and listen to programs using a cell phone’s ordinary voice connection. “Local people adapt technology and make use of it and hack it and play with it in ways that are completely unexpected,” Yusuf said, “and it’s really important to start from the bottom up.”<br /><br />The final conversation of the evening was with Lova Rakotomalala of Madagascar, whose organization, <a href="http://www.foko-madagascar.org/foko/" target="_blank">Foko Club</a>, has a short but extraordinary history. Foko Club began as a way for Malagasy high-school students to learn English, and one of their projects was an English-language blog. But that quickly evolved into a site where the students reported on their local communities using audio and video as well as text. In early 2009, when protests against the government led to deadly clashes with police and, ultimately, the ouster of the country’s president, the teenage members of Foko Club were there with their cameras, despite Rakotomalala’s attempts to dissuade them. Adults and even some experienced journalists joined the club, which rapidly became the most reliable source of information about the unfolding events, both inside and outside the country.<br /><br />At the end of the conversation, Zuckerman asked Rakotomalala to say goodbye in Malagasy, and Rakotomalala obliged, unleashing an effusion of liquid syllables that lasted four seconds. “So one of the things I’ve been learning about Malagasy,” said Zuckerman, “is that it has more syllables than I could ever possibly imagine.”<br /><br />“Even I am filled with respect,” said the Center for Future Civic Media’s director, Chris Csikszentmihályi.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[An altered state]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/irregular-forces-0419.html</link>
<story_id>15198</story_id>
<featured>0</featured>
<description><![CDATA[As Mexico’s violence persists, an MIT scholar says a solution must address the  economic dislocations tied to the country’s rapid urbanization.]]></description>
<postDate>Mon, 19 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100505113635-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100505113635-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100505113635-0.jpg</fullURL>
<imageCaption><![CDATA[The city of Juarez, Mexico.]]></imageCaption>
</image>
<body><![CDATA[Mexico’s president Felipe Calderon has made a military campaign against the country’s ascendant drug-trafficking gangs the centerpiece of his presidency. After thousands of fatalities, many of them due to retaliatory strikes by the cartels — including the murders of United States consulate workers last month — the battle remains unresolved.<br /><br />“I don’t see the drug traffickers giving up, and I don’t see the Mexican government winning this struggle with violence,” says Diane Davis, a professor of political sociology in MIT’s Department of Urban Studies and Planning, and a leading scholar of society and politics in Mexico. <br /><br />Lately, Davis  and other similarly minded experts have been making an increasingly public case that the Mexican government will not find long-term success focusing on a conventional military-style battle, as if it were fighting conventional insurgents from Latin America’s past, such as Colombia’s rural rebels. That is because, Davis argues in a pair of recent scholarly articles, Mexico’s cartels constitute “irregular armed forces” — well-organized, flexible urban gangs that make money smuggling drugs and other goods — buttressed by Mexico’s socioeconomic problems. <br /><br />The cartels, Davis contends, are different from rebel groups: They aim not to remove the whole government, but instead to usurp some of its functions. “Their activities are not always motivated by anti-government ideals or regime change,” Davis states in a new article, “Irregular Armed Forces, Shifting Patterns of Commitment, and Fragmented Sovereignty in the Developing World,” published in the March issue of <em>Theory and Society</em>. Rather, the cartels use violence to protect their “clandestine networks of capital accumulation.” <br /><br />By consensus of analysts, Mexican cartels now operate the most lucrative drug-smuggling routes into the Unites States, which had been under the control of Colombian groups in the 1980s and 1990s. The Mexican cartels now bring in from $10 billion to $25 billion annually from smuggling, according to a report in <em>The Wall Street Journal</em> this month. <br /><br />In classic sociological theory, the state has a monopoly on the legitimate use of violence through military and police forces. But in Mexico, drug cartels challenge that monopoly, and more brazenly than ever. “Think of the armed forces linked to drug lords as a form of a ‘privatized army,’” says Davis. <br /><br />The smuggling revenues of the cartels also give them staying power. “In cities, these groups have economic networks to supply the money to fight,” Says Davis. The resulting violence has reached levels that “resemble body counts from civil war battles,” as Davis writes. In the first half of 2009, Mexico saw an average of 127 cartel-related killings per week.<br /><strong><br />Narco-gang offer: ‘good salary, food, and help for families’</strong><br /><br />In Davis’ view, the massive urbanization of Mexico — more than 70 percent of Mexicans live in cities now, compared to about half in 1960 — has created social dislocations the cartels exploit. About 70 percent of Mexico’s urban labor force works in the “informal sector” of the economy, in transient jobs for small firms or in the world of black-market commerce. “As a result,” Davis writes, “much informal employment is physically and socially situated within an illicit commercial world” beyond the reach of the state, where violence occurs as gangs try to control drug supply routes. Moreover, informal workers are often not eligible for government safety-net benefits, making them even less invested in the success of the state.<br /><br />Thus the cartels have been startlingly open about recruiting new members. In the city of Laredo in 2008, Davis notes, a drug gang called the <em>Zetas</em> hung a banner on a downtown bridge asking for “military recruits and ex-military men … seeking a good salary, food, and help for their families,” with a phone number posted for prospective enlistees. Other gangs were founded outside Mexico, but have spread inside the country: one such group, <em>los Maras</em>, was founded by “city-based youth who turned to criminal activity because of the lack of employment alternatives in the large metropolitan areas of California, Mexico, and Central America,” Davis writes in another article, “Non-State Armed Actors, New Imagined Communities, and Shifting Patterns of Sovereignty and Insecurity in the Modern World,” published in the August 2009 issue of <em>Contemporary Security Policy</em>. (In February, the article received the Bernard Brodie Prize, awarded to the best article appearing in <em>Contemporary Security Policy</em> each year.) <br /><br />Mexico’s drug wars thus involve physically dispersed, evolving organizations that could be viewed more as self-sustaining networks than anti-state insurgents. “I think we still need to develop our analysis to better understand what’s going on in Mexico, and that’s why Davis’ contribution is so relevant,” states Fabio Armao, a professor of international relations at the University of Turin who studies organized crime. <br /><br />A real long-term solution to Mexico’s problems, Davis notes, will require economic development, to lessen the influence of the drug trade and reduce the appeal of gang recruitment efforts. Such long-term economic changes have happened before: As <em>The Wall Street Journal</em> report stated, the cocaine industry has shrunk from an estimated 6 percent of Colombia economy in 1987 to under 1 percent today. Yet as <em>The New York Times</em> noted in March, of the more than $1 billion the United States has provided Mexico during the last two administrations, much has been for military and police equipment and training.<br /><br />As Davis sees it, the Mexican government may be better equipped in the short run, but it still lacks a long-term perspective. And of the outlook in Washington, Davis says, “Attention is finally turning to Latin America, but the economic and policy contours have yet to be seen.” Among other things, Davis believes, the United States must work to limit its demand for drugs smuggled through Mexico. “I think some responsibility has to be accepted, and some re-thinking about trade and other incentives to get at the root of these transnational smuggling networks has got to be on the agenda. It is time for a paradigm shift.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Robotic therapy helps stroke patients regain function]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/stroke-therapy-0419.html</link>
<story_id>15204</story_id>
<featured>0</featured>
<description><![CDATA[MIT robots can deliver high-intensity interactive physical therapy.]]></description>
<postDate>Mon, 19 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100414154658-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100414154658-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100414154658-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Department of Veterans Affairs]]></imageCredits>
<imageCaption><![CDATA[A patient uses a robotic therapy device invented at MIT. The robotic joystick guides the patient’s arm as he tries to move the robot handle toward a moving or stationary target shown on the computer monitor. If the person starts moving in the wrong direction or does not move, the robotic arm gently nudges his arm in the right direction.]]></imageCaption>
</image>
<body><![CDATA[Stroke patients who received robot-assisted therapy were able to regain some ability to use their arms, even if the stroke had occurred years earlier, according to a study published April 16 in the online issue of <em>The New England Journal of Medicine</em>.<br /><br />The study, which examined the effectiveness of a class of robotic devices developed at MIT, found that in chronic stroke survivors, robot-assisted therapy led to modest improvements in upper-body motor function and quality of life six months after active therapy was completed; these improvements were significant when compared with a group of stroke patients who received the traditional treatment. Moreover, the robotic therapy — which involves a more intense regimen of activity than traditional stroke therapy — did not increase total health-care costs per stroke patient, and could make intensive therapy available to more people, say the researchers who led the study.<br /><br />The study results also challenge the notion that physical therapy only benefits stroke patients within the first six months after the stroke, says Albert Lo, a neurologist at the Providence VA Medical Center who led the study.<br /><br />“There are nearly six million stroke patients in the U.S. with chronic deficits,” says Lo. “We’ve shown that with the right therapy, they can see improvements in movement, everyday function and quality of life.”<br /><br /><strong>Mind and body</strong><br /><br />The study, conducted at four Veterans Affairs (VA) hospitals, found that patients who used the MIT robotic devices for 12 weeks experienced a small but significant gain in arm function. Another group of patients who received high-intensity therapy from a therapist, which matched the number and intensity of the robot movements, showed similar improvements. <br /><br />Hermano Igo Krebs, a principal research scientist in MIT’s Department of Mechanical Engineering who developed the MIT-Manus robot, has been working on robotic therapy since his graduate student years at MIT almost 20 years ago. In his early studies, he and his colleague, Professor Neville Hogan, found that it’s important for stroke patients to make a conscious effort during physical therapy. <br /><br />The MIT-Manus system, which Krebs started developing more than 20 years ago, is based on that principle. The patient grasps a robotic joystick that guides the patient’s arm, wrist or hand as he or she tries to make specific movements, helping the brain form new connections that will eventually help the patient relearn to move the limb on his or her own.<br /><br />In the New England Journal of Medicine study, researchers at VA hospitals in Baltimore, Seattle, West Haven, Conn., and Gainesville, Fla., compared the MIT-Manus system to a high-intensity rehab program delivered by a human therapist, which was designed specifically for this study. <br /><br />Each group included about 50 patients, who were also compared with a group of 28 stroke patients who received so-called “usual care” — general health care and three hours per week of traditional physical therapy for their stroke-damaged limb.<br /><br />Patients using the MIT-Manus system grasp a joystick-like handle connected to a computer monitor that displays tasks similar to those in simple video games. In a typical task, the subject attempts to move the robot handle toward a moving or stationary target shown on the computer monitor. If the person starts moving in the wrong direction or does not move, the robotic arm gently nudges his or her arm in the right direction. <br /><br />“The ability to be interactive is critical,” says Krebs. “We program the robot to only give assistance as needed.”<br /><br />Patients in the study received therapy three times a week for 12 weeks, and during each hour-long session, they made hundreds of repetitive motions with their arms. At the end of 12 weeks, tests revealed a small but statistically significant improvement in quality of life, and a modest improvement in arm function. When the subjects were tested again at 36 weeks, both the robot therapy group and intensive human-assisted therapy group showed improvement in arm movement and strength, everyday function and quality of life compared to the usual-care group.<br /><br />The high-intensity, interactive physical therapy offered to patients who did not receive robot-assisted therapy was developed specifically for comparison purposes for this study, and is not generally available. Furthermore, the physical demands on the therapist make it unlikely that it will ever be widely used. <br /><br />“If you can get a therapist to work at that pace with a patient, certainly the benefits are roughly the same, and we showed this benefit when we designed this intensive comparison group, but it’s not practical,” says Krebs. “Robotics and automation technology are ideal for this kind of highly repetitive tasks. We’re using robotic technology to create a tool for the therapist to afford this kind of high-intensity therapy while maintaining the therapist supervisory role, deciding what is right for a particular patient.”<br /><br />This particular study was designed to test the effects of only conventional therapy versus only robotic therapy, but Bruce Dobkin, a neurologist at the UCLA Stroke Center, says the best approach may end up being a combination of those two strategies. “If robotic therapy is going to be helpful, you need to find a more integrated way to use the robotic device,” he says. <br /><br /><strong>The value of robots</strong><br /><br />Another way to make robotic therapy more practical could be to lower the costs, says Dobkin, who was part of the data-safety monitoring committee that supervised the research. In the VA study, the robotic therapy cost an average of $9,977 per patient, and the intensive nonrobotic therapy cost $8,269 per patient. However, overall healthcare per-patient costs, including costs for those who received only usual care, were not very different over the total 36-week study period — $15,562 per patient for robot-assisted therapy, $15,605 for intensive nonrobotic therapy, and $14,343 for usual care.<br /><br />Krebs believes that once the robotic devices can be mass-produced, which he expects will occur within the next 10 years, the costs will drop. “What you have to do is make more of them, and that will drive down costs to a point where people can have them in their homes,” he says.<br /><br />Krebs is also encouraged by the fact that many of the patients in the study had either suffered multiple strokes or had experienced their strokes many years earlier, yet still showed improvement. “We put the bar very high,” he says. “If we worked with patients sooner after their first stroke, we may get even better results.” He is now working with doctors to plan such a study.<br /><br />Krebs and his collaborators are also studying whether the MIT-Manus could help patients with cerebral palsy, multiple sclerosis and spinal cord injury.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The deep roots of inequality]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/inequality-roots-0416.html</link>
<story_id>15209</story_id>
<featured>0</featured>
<description><![CDATA[MIT economics student’s study of Peru shows how practices from hundreds of years ago can influence prosperity today. ‘Pathbreaking,’ says a Harvard economist.]]></description>
<postDate>Fri, 16 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100415174021-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100415174021-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100415174021-1.jpg</fullURL>
<imageCaption><![CDATA[A farm in the Peruvian Andes. An MIT economics student has shown that there are deep historical roots of poverty and wealth among Peru's agrarian communities.]]></imageCaption>
</image>
<body><![CDATA[To an economist, a map of southern Peru has a peculiar appearance. If you draw a line forming a kind of jagged oval, outlining a chunk of the Andes running from northwest to southeast, you have enclosed a region of relative poverty lying among areas of greater wealth. Yet there is no readily apparent explanation for this disparity; the well-off districts do not appear to have more natural resources than the poor ones, for instance.<br /><br />But Melissa Dell, a PhD student in MIT’s Department of Economics, thinks this economic riddle has an answer: the invisible hand of history. More specifically, Dell believes a system of labor conscription that the region’s Spanish colonial rulers used from 1573 to 1812 accounts for the pattern of economic development in the region today. The areas where the Spainish forced locals to leave the land and work in mines — a practice called the <em>mita</em> — are precisely the poorest today, while the places where workers were left alone are now wealthier. <br /><br />“Historical institutions do matter,” says Dell. “We are still seeing these differences based on this forced labor practice that was abolished almost 200 years ago.” In a working paper produced in January, “<a href="http://econ-www.mit.edu/files/5241" target="_blank">The Persistent Effects of Peru’s Mining <em>Mita</em></a>,” Dell explains and quantifies the presence of this lingering effect. <br /><br />In 1545, the mines around the town of Potosi, high in the Andes, were discovered to contain more silver than any other place in the Spanish empire. To the northwest, mines near the town of Huancavelica contained the mercury necessary for refining silver ore. By 1573, the Spanish instituted a conscription system through which 200 communities in the region were required to provide one-seventh of their workers — mostly farmers by trade — for these mines. During any given year, 3 percent of all adult men in Peru were conscripted for duty in Potosi and Huancavelica. The Spanish abolished the system in 1812, as the mines became depleted. <br /><br />To examine the long-term effects of the <em>mita</em>, Dell identified one particular area within the larger mining region where towns subject to the <em>mita</em> have the same physical and social characteristics — they grow the same crops and have the same ethnic composition — as neighboring communities that were spared conscription. <br />In this way she created a sample group of <em>mita</em>-affected locales, and a kind of  control group of non-<em>mita</em> areas, to assess the <em>mita</em>’s effects. Looking at current economic data, Dell found that the areas once subject to conscription feature a level of household consumption today that is 25 percent below that of the neighboring, non-<em>mita</em> communities. Moreover, in old <em>mita</em> locales, 6 percent more of the children have stunted growth than in the other areas. In short, places subject to the <em>mita</em> from the late 16th century through the early 19th century are significantly poorer in the 21st century.<br /><br />Dell believes this poverty has persisted due to multiple, interconnected factors. After 1573, the areas unaffected by the <em>mita</em> often featured haciendas, large rural estates with many laborers, and stable property rights. In <em>mita</em> districts, by contrast, the Spanish rulers mostly just allowed small land tenants to exist, without substantial property rights. Over time, the larger landowners were better able to lobby for public goods, such as functioning roads leading to markets. This created a historical split that is still evident: The lack of similar public infrastructure in the <em>mita</em> areas means that even today, a greater portion of local residents are subsistence farmers with little access to markets and education. <br /><br />To arrive at her conclusions, Dell also spent a month in Peru conducting interviewing. “I talked to everyone from peasants to agrarian scientists, and they told stories very consistent with the data,” Dell says. “Peasants would say, ‘Well, other people grow corn just like us, but they’re richer because they can take their corn to the market, whereas the government didn’t build any roads for us. So we just produce enough to eat, and that’s it.’”<br /><br />The paper, under review at the journal Econometrica, is akin to the work of Dell’s graduate advisor, MIT economist Daron Acemoglu, who has extensively studied the relationship between economic development and legal institutions such as property rights. Dell believes her study complements our existing historical knowledge by showing the empirical effects of Spain’s colonial practices.  <br /><br />“I really think the value of this is to merge the history with current outcomes using statistical tools,” says Dell. “A historian could tell us more about the details of the <em>mita</em>, but they don’t necessarily have the quantitative tools to establish causality.”<br /><br />Other economists in the field are impressed by Dell’s findings. “I think it’s a path-breaking paper,” says James Robinson, a professor of government at Harvard. “This is the first paper that’s taken a specific institution of this kind, and shown real evidence of the long-running impact of colonialism in Latin America and Peru today.” For economists interested in economic development, Robinson adds, “There is a lot of scope for more studies like this.” <br /><br />Dell would also like the paper to usefully complicate ideas about growth in Latin America. Other economists have argued that the region has lagged economically because of its inequalities — because, for example, Latin America had structures like the hacienda system with a few large landowners, as opposed to the plethora of smaller, secure property owners in North America. But as Dell points out, because smaller landholders in Peru lacked solid property rights, the hacienda system, for all its inequalities, actually engendered greater public investment. Dell aims to find additional data sets that shed more light on the interaction of the state and private interests  in Latin America, to better understand the unique economic trajectory within many parts of the region.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Microsensors without microfabrication]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/accelerometer-0416.html</link>
<story_id>15210</story_id>
<featured>0</featured>
<description><![CDATA[By building a six-dimensional motion sensor from a tiny metal bead in a tiny hole, MIT researchers introduce a new class of microdevice.]]></description>
<postDate>Fri, 16 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100415175054-1.png</thumbURL>
<smallURL width='140' height='95'>http://web.mit.edu/newsoffice/images/article_images/w140/20100415175054-1.jpg</smallURL>
<fullURL width='368' height='250'>http://web.mit.edu/newsoffice/images/article_images/20100415175054-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Rehmi Post/Center for Bits and Atoms]]></imageCredits>
<imageCaption><![CDATA[A tiny metal bead suspended in an electric field is just visible in the middle of a hole drilled through a circuit board. The hole itself is only the diameter of the wires used to connect circuit elements on the circuit board.]]></imageCaption>
</image>
<body><![CDATA[Miniature motion sensors are everywhere these days, detecting the orientation of cell phones, deploying air bags in cars and measuring stresses in buildings and mechanical systems. But manufacturing the sensors’ tiny moving parts requires the same high-tech, billion-dollar facilities that churn out computer chips.<br /><br />Researchers at MIT’s Center for Bits and Atoms (CBA) have now built a motion sensor that consists of a tiny metal bead suspended in what the center’s director, Neil Gershenfeld, describes as “a hole drilled in a circuit board.” A fluctuating electric field holds the bead aloft, in a tight orbit, and disturbances of the orbit indicate the sensor’s direction of motion. Gershenfeld believes that the sensor opens the door to a new class of miniaturized devices that exploit the dynamics of simple physical systems instead of the mechanical interactions of precisely micromachined parts. Such “microdynamical” devices, Gershenfeld says, could enable cheaper, simpler, more responsive sensors for a range of applications, including the measurement of sound, pressure, fluid-flow and magnetic fields.<br /><br />The CBA researchers’ device can do the work of at least six different micromechanical sensors. It can measure linear motion in three dimensions, which would ordinarily require three accelerometers. But it can also gauge its orientation — whether it’s tipped sideways or forward, or it’s been rotated — which would usually require an additional three gyroscopes.<br /><br />A six-dimensional sensor would make the motion detection of handheld devices much more precise. The Wii game controller, for instance, wouldn’t need an infrared emitter mounted to the television, and the Apple iPhone would change its screen orientation more reliably. Rehmi Post, a visiting scientist at CBA who initiated the sensor project as a PhD student at MIT, points out that the three-axis accelerometer is the most expensive component of the Wii remote. He believes that ultimately, a six-dimensional microdynamical sensor could be manufactured for about a tenth as much.<br /><br />“If they can get all six degrees out of it, it would be huge,” says Michael Judy, a researcher at Analog Devices, the company that built the Wii’s accelerometers. “That’s the holy grail right now in the human interface to electronics.” Judy says that the application of motion sensing that has sparked the most interest is navigation in environments where GPS information is either unreliable or too imprecise. For instance, local spatial tracking would let hospital workers immediately determine each other’s locations, even on different floors of a large building.<br /><br />Gershenfeld suggests some other applications, too: scrolling through web pages, or viewing a 3-D virtual object from different angles, simply by moving a cell phone in space; or pens that can digitally record whatever’s written with them.<br /><br /><strong>Back in the saddle</strong><br /><br />In the <a href="http://apl.aip.org/applab/v96/i14/p143501_s1" target="_blank">most recent issue</a> of the journal <em>Applied Physics Letters</em>, Gershenfeld, Post and George Popescu, who worked on the project as a graduate student, describe how they built their microdynamical sensor. At its heart is a particle trap, a device commonly used in experimental physics. Physically, the trap is very simple: two metal plates on either side of a circuit board, with a hole about the diameter of an electrical wire drilled through them. But a computer circuit hooked up to the plates exerts precise control over the electric field they produce.<br /><br />The electric field, Gershenfeld explains, can be thought of as saddle-shaped: front to back, it curves upward at the ends, but side to side, it curves downward. The field fluctuates as if it were rotating, and a particle at its center is like a marble on a warped turntable. The marble starts to roll down one of the downward slopes, but the turntable revolves, and the marble finds itself rolling up an uphill slope instead. When it falls back down the slope, it repeats the whole process on the opposite side of the turntable, and so on.<br /><br />A particle in the trap is thus not perfectly still but rapidly oscillating as, in effect, it rolls back and forth between upward slopes. Each of the six types of motion detected by a complete set of accelerometers and gyros disturbs the particle in a distinctive way.<br /><br />“It’s great research,” says Judy. “It has a lot of possibilities. But it needs a lot of work.” He points out, for instance, that generating the electric field in the prototype sensor required voltages in the vicinity of 1,000 volts. Building up that kind of voltage in a handheld device isn’t impossible, but it can introduce power inefficiencies: “The higher the voltage, the more power you burn to get it,” Judy says. Post, however, observes that the lenses in cell phone cameras typically require about 100 volts and that existing technology can generate that type of voltage efficiently. And a commercial version of the sensor would probably use a smaller particle trap, he says: “The necessary voltage decreases as the diameter of the trap decreases.”<br /><br />Another unresolved question, however, is how to measure the particle’s oscillation. In their prototype, the CBA researchers used a miniature camera, and Gershenfeld says that incorporating an optical sensor into a practical, mass-producible device is an engineering challenge “on the order of the optics of a CD player.” In the meantime, however, the researchers are working on a version of the device in which the metal bead is mounted on a wire that can directly relay electrical information about its oscillation. The wire would restrict the particle’s motion in one dimension, but the sensor would be easier to manufacture, and it would still be useful in cars or other vehicles that tend not to suddenly launch into the air.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Engineering, science and management graduate programs continue to excel]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/usnews-rankings-0415.html</link>
<story_id>15206</story_id>
<featured>0</featured>
<description><![CDATA[MIT tops several disciplines in U.S. News & World Report annual rankings]]></description>
<postDate>Thu, 15 Apr 2010 04:02:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100414223944-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100414223944-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100414223944-1.jpg</fullURL>
</image>
<body><![CDATA[MIT continues to excel in its engineering, science and management graduate programs, according to the latest <em>U.S. News &amp; World Report</em> <a href="http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools" target="_blank">annual rankings</a>.<br /><br />MIT’s School of Engineering was again ranked number one in the magazine’s annual evaluation of U.S. graduate school programs, which hits newsstands next week. The School of Engineering has achieved the top score in the <em>U.S. News</em> rankings each year since the rankings were created in 1990.<br /><br />MIT also received top marks for the following specialties within its graduate engineering program: aeronautics and astronautics, chemical engineering, electrical engineering, materials engineering and mechanical engineering. <br /><br />The MIT Sloan School of Management jumped from fifth to third among the nation’s MBA programs, according to the magazine (Harvard and Stanford were tied for first). Sloan’s specialty programs in information systems, production/operations and supply chain/logistics were again ranked first.<br /><br />The magazine this year updated its rankings of doctoral programs in the sciences. The Institute’s graduate programs in chemistry, physics, mathematics and earth sciences were all rated number one. In biological sciences, MIT was tied for second with Harvard, with Stanford occupying the top position.<br /><br />MIT’s graduate program in computer science shared the top position with Carnegie Mellon, Stanford, and the University of California, Berkeley.<br /><br /><em>U.S. News</em> does not issue annual rankings for all doctoral programs but, instead, revisits them every few years. Last year, for example, <em>U.S. News</em> evaluated economics doctoral programs, and MIT tied for first place with Harvard, Princeton and the University of Chicago. <br /><br />Each year, <em>U.S. News</em> ranks professional-school programs in business, education, engineering, law and medicine. These rankings are based on two types of data: expert opinions about program quality, and statistical indicators that measure the quality of a school’s faculty, research, and students. Rankings of programs in the sciences, social sciences and humanities, meanwhile, are based solely on the ratings of academic experts.<br /><br />The magazine's annual ranking of U.S. undergraduate schools is due to be published in August. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The pull of artificial gravity]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/artificial-gravity-0415.html</link>
<story_id>15202</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers say a centrifuge on the International Space Station — hinted at in Obama’s NASA proposal — would be a boon for physiological research]]></description>
<postDate>Thu, 15 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100414133926-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100414133926-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100414133926-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: NASA]]></imageCredits>
<imageCaption><![CDATA[Astronaut and MIT alumnus Robert L. Satcher Jr. '86, PhD '93, STS-129 mission specialist, participates in a spacewalk in 2009.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='297'>http://web.mit.edu/newsoffice/images/article_images/20100414134117-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Justin Kaderka, a graduate student in the Department of Aeronautics and Astronautics, stands near the centrifuge located at MIT that is used to test how spinning affects the vestibular system, which influences our sense of spatial orientation.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Although President Obama’s vision for NASA’s future calls for canceling the Constellation program that was intended to send humans to the moon by 2020, his proposed budget for the agency still holds opportunities for future space research and urges NASA to develop the technology to enable human spaceflight to locations like Mars that are beyond low-Earth orbit. <br /><br />Several MIT researchers have been intrigued by one item in the proposal that they hope Obama will elaborate on during today’s policy speech at the Kennedy Space Center in Florida: extending operations of the International Space Station (ISS) past its planned retirement in 2016.<br /><br />According to Obama’s proposal, keeping the ISS operational would allow NASA and other nations’ space agencies to deploy new research facilities there, such as a centrifuge, which simulates the effects of gravity by spinning. Equipping the space station with a centrifuge would give scientists a valuable opportunity to test whether the device can offset some of the negative side effects of zero gravity, according to Justin Kaderka, a graduate student in MIT’s Department of Aeronautics and Astronautics, who has studied the effectiveness of artificial gravity. <br /><br />Without gravity, bone loses essential minerals, muscles atrophy and the cardiovascular system weakens. Since the 1960s, artificial gravity has been discussed as a way to keep astronauts healthy during long-duration trips to places like Mars, which would take three years. Creating artificial gravity in space can be achieved either by using a “long-range centrifuge” in which a large spacecraft rotates continuously or a “short-range centrifuge” in which a person is attached to a spinning structure for intermittent periods. In either scenario, the spinning motion creates an acceleration field that mimics the effects of gravity on the human body. <br /><br />For nearly two years, Kaderka has worked with Laurence Young, the Apollo Program Professor of Astronautics and a professor of health sciences and technology, to understand how artificial gravity compares to traditional countermeasures, such as resistance training and aerobic exercises done on treadmills and cycling machines. Although these methods are currently tested on the ISS, astronauts continue to experience weakening of the bone, muscle and cardiovascular systems upon their return.<br /><br />Artificial gravity could be an effective countermeasure to deconditioning of a variety of human physiological systems because it supplies the missing stimulus — gravity — before significant weakening can occur. “Artificial gravity is unique because it goes to the source of the problem, unlike other methods that are designed to counter the deconditioning of individual physiological systems,” says Kaderka, who reviewed the results of more than 75 experiments done over the past 40 years on ways to counter the physical losses caused by prolonged weightlessness.<br /> <br />In a thesis that will be published in May, Kaderka concludes from his review that artificial gravity works just as well as traditional countermeasures at treating the adverse effects of long-term weightlessness on the cardiovascular system, especially when combined with other aerobic exercises. As for artificial gravity’s effects on muscle and bone, however, Kaderka suggests that much more research is needed.<br /><br />William Paloski, a professor of space science at the University of Houston, values Kaderka‘s analysis for organizing a “hodgepodge of test objectives and findings,” noting that the work sets the stage for a “knowledge-driven coordinated approach” to future studies that could lead to successful gravity therapies for missions.<br /><br /><strong>The meta-analysis</strong><br /><br />Although Kaderka’s “meta-analysis” tries to provide a scientific basis for future experiments, he acknowledges that the data are limited because so few deconditioning studies involving artificial gravity have been done on Earth. These experiments are expensive because they require subjects to be bedridden for several weeks in order to mimic the deconditioning that astronauts experience in space. <br /><br />Specifically, Kaderka compared the results of nearly 35 artificial-gravity deconditioning studies (including some done on animals) to those from about 40 experiments that tested traditional countermeasures. Taking into consideration differences between studies, such as their duration, Kaderka drew comparisons between studies that had consistency across certain variables, such as which physiological systems were assessed. <br /><br />Based on data about subjects’ maximum rate of oxygen uptake and tolerance to cardiovascular stress, among other parameters, he concludes that artificial gravity is just as effective as other countermeasures in offsetting cardiovascular deconditioning. But because there have been so few bed-rest studies that examined the effects of artificial gravity on muscle or bone, Kaderka was unable to make the same conclusion about artificial gravity for those systems.<br /><br /><strong>Not if, but when</strong><br /><br />Kaderka and Young acknowledge that even if a centrifuge is put on the ISS, much more ground-based research on artificial gravity will be needed to determine certain unknown variables, such as the amount of time or at what speed a person needs to be spun on a centrifuge for its use to be effective. They say that having the ISS centrifuge could provide hard evidence that artificial gravity is a viable countermeasure, which might lead to NASA funding more Earth-based experiments.<br /><br />According to Young, who submitted a proposal through the Japanese Space Agency to deploy a centrifuge on the ISS months ago, it will take researchers years to quantify the effectiveness of artificial gravity, which is why NASA’s plans should include both Earth and space experiments as soon as possible so that the agency is prepared to use the countermeasure when it decides to conduct long-duration flights.<br /><br />“It’s not a question of if, but when, NASA makes that decision,” says Young, explaining that after NASA’s proposed robotic exploration of Mars and faraway asteroids, astronauts will need to travel to and explore those surfaces. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Why cancer drugs lose their power]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/drug-resistance-0415.html</link>
<story_id>15203</story_id>
<featured>0</featured>
<description><![CDATA[MIT biologists show how tumors can become resistant to the commonly used chemotherapy drug cisplatin.]]></description>
<postDate>Thu, 15 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100414150447-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100414150447-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100414150447-1.jpg</fullURL>
<imageCaption><![CDATA[A 3-D model of a cisplatin molecule.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='490'>http://web.mit.edu/newsoffice/images/article_images/20100414153225-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Kent Dayton Photography]]></imageCredits>
<imageCaption><![CDATA[Tyler Jacks, director of the David H. Koch Institute for Integrative Cancer Research at MIT.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[For 30 years, the chemotherapy drug cisplatin has been one of doctors’ first lines of defense against tumors, especially those of the lung, ovary and testes. While cisplatin is often effective when first given, it has a major drawback: Tumors can become resistant to the drug and start growing again.<br /><br />Now, MIT cancer biologists have shown how that resistance arises, a finding that could help researchers design new drugs that overcome cisplatin resistance. The team, led by Tyler Jacks, director of the David H. Koch Institute for Integrative Cancer Research at MIT, reports the results in the April 15 issue of the journal <em>Genes and Development</em>. <br /><br />Cisplatin and other platinum-based cancer drugs destroy tumor cells by binding to DNA strands, interfering with DNA replication. That activates the cell’s DNA repair mechanisms, but if the damage is too extensive to be repaired, the cell undergoes programmed suicide.  <br /><br />Eventually, cancer cells learn to fight back. The new study shows that tumor cells treated with cisplatin ramp up their DNA repair pathways, allowing them to evade cell death, says Trudy Oliver, a postdoctoral fellow in Jacks’ lab and lead author of the paper. <br /><br />Previous studies had suggested several possible mechanisms for resistance development, including enhancement of DNA repair pathways, detoxification of the drug, and changes in how the drug is imported into or exported out of the cell. However, those studies were done in cancer cells grown in the lab, not in living animals (in vivo).<br /><br />“Many mechanisms have been identified but it’s not clear what happens in vivo because the in vivo environment is so much more complicated than in cell lines,” says Oliver. <br /><br />Oliver and her colleagues set out to study cisplatin resistance in mice with a mutation in a gene called Kras, which leads the animals to develop lung cancer. About 30 percent of human lung cancer patients have mutations in Kras. Some of the mice also had defective versions of the tumor suppressor gene p53, which is mutated in about half of human lung cancers.<br /><br />The researchers found that cisplatin was effective against lung tumors in both sets of mice, though it was more potent in mice that still had functional p53. In those mice, tumors actually shrank, while the drug only slowed tumor growth in mice with defective p53. Those results are consistent with findings in human patients.<br /><br />After four doses of cisplatin, mice with normal p53 developed resistance to the drug, and tumors started growing faster. To figure out why, the researchers analyzed which genes were being transcribed more as resistance developed, and identified several that are involved in DNA repair pathways. <br /><br />One gene that particularly caught the researchers’ attention is PIDD (p53-induced protein with a death domain), which is turned on by p53 and has been implicated in programmed cell death, though its exact function is not known. When PIDD levels are artificially increased in human lung cancer cells, they become more resistant to cisplatin. Oliver is now studying tumors in which the PIDD gene has been knocked out, to see if its absence hinders drug resistance. <br /><br />It is likely that PIDD is just one of many genes, in many pathways, involved in the drug resistance process, says Oliver. “It’s not a simple phenomenon,” she says. <br /><br />The work was funded by the National Institutes of Health and the National Cancer Institute.<br /><br />This study represents an important step toward better understanding of how cisplatin resistance arises, says Thomas Helleday, professor of radiation oncology at the University of Oxford, who was not involved in the research. Helleday describes the work as a “landmark” paper that “has important implications in the design of new drugs, which should be targeted to stop the repair that is activated in cisplatin-resistant tumors.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Letter to the community on MIT’s financial condition]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/financial-update.html</link>
<story_id>15201</story_id>
<featured>0</featured>
<description><![CDATA[President Susan Hockfield credits community efforts, research funding for putting MIT on a ‘stable path.’]]></description>
<postDate>Wed, 14 Apr 2010 14:15:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100414102318-1.png</thumbURL>
<smallURL width='140' height='138'>http://web.mit.edu/newsoffice/images/article_images/w140/20100414102318-1.jpg</smallURL>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100414102318-1.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='366' height='488'>http://web.mit.edu/newsoffice/images/article_images/20100414102319-2.jpg</fullURL>
<imageCaption><![CDATA[MIT President Susan Hockfield ]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[To the members of the MIT community:<br /><br />I write to provide an update on MIT's financial condition. After 18 months of extraordinary upheavals in the global economy, I'm often asked, "How is MIT doing financially?" While normally we report on Institute finances only at the end of the fiscal year, in these unusual times I want to update you a few months in advance.  <br /><br />This letter also expresses my gratitude, because the comparative stability of our current finances stems directly from the concerted attention and actions of many members of the MIT community and from the great generosity of our alumni and friends. We all recognize the difficult and sometimes painful decisions required by the budget cuts taken for this and next year, but I hope that this interim report provides reassurance about the path forward. <br /><br />Of course, no one can accurately predict future economic directions, so we continue to prepare for a range of scenarios, from another downturn, to slow growth, to a more robust recovery. I am pleased to report, however, that many indicators we follow closely show MIT on a stable path, well buffered against further economic turbulence and, at the same time, well prepared to seize emerging opportunities.  <br /><br /><strong>MIT's budget returns to balance<br /><br /></strong>As I noted in previous letters, because the MIT community rose to the challenge of reducing expenditures for the current fiscal year and the next, we will have brought MIT's budget back into balance. Led by the efforts of the nearly 200-member <a href="http://web.mit.edu/instituteplanning/TaskForceFinalReport.pdf" target="_blank">Institute-wide Planning Task Force</a>, inspired by suggestions from the Idea Bank, and guided by the prudence and thoughtfulness of budget administrators across the Institute, we have turned this difficult period into an opportunity to improve our operations, to amplify the sources of our greatest strength and to design ways to use our resources more effectively. With this secure foundation, I can also report reassuring results from three key sources of income: research funding, philanthropic giving and endowment returns.<br /><br /><strong>Research volume has increased</strong><br /><br />MIT's success in attracting research funding provides an important measure of the quality, ambition and accomplishment of our faculty, staff and students. This year, MIT's research funding increased markedly, both from standard sources and through funds from the American Recovery and Reinvestment Act of 2009 (ARRA, the "Stimulus Bill"). Excluding ARRA projects, MIT's on-campus research funding rose 10.5% compared to this time last year, to $436 million; including ARRA projects, MIT's on-campus research funding rose 14%, to $451 million. Lincoln Laboratory's funding has also increased, bringing total MIT research funding to $986 million excluding ARRA funds and $1 billion including them. In such difficult times, this very healthy growth in research funding reflects MIT's consistently extraordinary research activities.<br /><br /><strong>MIT benefits from generous philanthropic giving<br /><br /></strong>Philanthropic giving often falters in the face of economic turmoil. However, our donors have steadfastly continued their remarkable generosity. Our cash gifts at the end of March reached an historic high for this point in the fiscal year, and, while pledges for future gifts started very slowly this year, we now lag last year's pace by only a few percentage points. In my visits to several MIT Alumni Clubs, I have heard directly from our graduates that even (and perhaps especially) in this uncertain time, they enthusiastically support MIT's mission of discovery, innovation, education and service.  Reflecting their enthusiasm, the Campaign for Students has reached $450 million of our $500 million goal, which sets us on track to meet or exceed the goal in time for the launch of the 150th anniversary celebration in January 2011. These gifts will offset some of the difficult reductions we have been forced to make. For example, for several programs new gifts for graduate fellowships will help compensate for reduced funds flowing from the endowment.<br /><strong><br />MIT's endowment has returned positive results this year</strong><br /><br />Although it is impossible to forecast precisely the end-of-year (June 30) endowment returns, I can report that MIT's endowment performance has improved from last year's. Last year, MIT experienced less severe investment losses than many of our peers, and the investment choices that moderated last year's losses will likely moderate this year's gains. As of the end of March, we estimate MIT's endowment performance in the high single digits.  <br /><br />Our budget plans for the next several years anticipate slow, steady financial improvement, and the current endowment performance aligns well with those plans. While the modest gains that we expect will not relieve the need to take the reductions now under way for this year and next, if the steady performance I have described here continues through to year's end, we will not require cuts beyond those we have budgeted. Going forward — and remembering how dire the global economic situation appeared just a year ago — our anticipated returns should offer a hard-earned sense of relief. <br /><br />To every member of the MIT community whose sustained effort, thoughtful input and patient forbearance have earned us the stability to move forward with confidence, I thank you.<br /><br />Most sincerely,<br />Susan Hockfield<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[In The World: Better wound treatment for all]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/itw-haiti-0414.html</link>
<story_id>15197</story_id>
<featured>0</featured>
<description><![CDATA[A streamlined version of 'negative-pressure' wound therapy is put to the test in Haiti — and could have 'enormous potential' across the developing world.]]></description>
<postDate>Wed, 14 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100413145044-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100413145044-1.jpg</smallURL>
<fullURL width='368' height='278'>http://web.mit.edu/newsoffice/images/article_images/20100413145044-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Melanie Gonick]]></imageCredits>
<imageCaption><![CDATA[Danielle Zurovcik SM '07 demonstrates how to use the negative pressure pump to seal an arm wound.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='278'>http://web.mit.edu/newsoffice/images/article_images/20100413145045-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[The simple pump creates negative pressure &#8212; shown here sealing a bandage on a leg wound &#8212; which speeds up the healing process.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Nobody knows precisely why it works, but doctors have known for decades that the healing process for open wounds can be greatly speeded up by applying negative pressure — that is, suction — under a bandage sealed tightly over the affected area. The speculation is that it helps by drawing bacteria and fluid away from the wound, keeping it cleaner.<br /><br />For patients, there is a benefit even beyond the speedier healing. Traditional dressings need to be removed and replaced — sometimes painfully — up to three times a day, but with the negative pressure system dressings can be left in place for a few days. But in the developing world, there's a problem: The systems are expensive, and they need to be plugged in or powered by batteries that last only a few hours. In many developing nations, a reliable source of electricity is rarely available.<br /><br />That's the problem that students in an MIT mechanical engineering class decided to tackle a few years ago. With the help of Dr. Robert Sheridan from Massachusetts General Hospital, the students developed a simple, inexpensive and lightweight version of the system that required no power supply and could be left in place for days. One of those students, Danielle Zurovcik SM '07, continued to work on the project and made it the subject of her master's thesis. She has continued to work on it on the side as she pursues her doctorate.<br /><br />The project was part of a mechanical design class taught by mechanical engineering professor Alexander Slocum in collaboration with local hospitals through a Boston-based organization called the Center for Integration of Medicine and Innovative Technology.<br /><br />Earlier this semester, Zurovcik, who had been making plans for field tests of the patent-pending device at a rural clinic in Rwanda this fall, was asked by the nonprofit healthcare organization Partners in Health to take part in earthquake relief efforts in Haiti. She traveled there with a supply of 50 of the current version of the plastic, molded pumps, which cost about $3 each. (The only portable versions on the market today cost $100 a day just to rent, and must have their batteries recharged after about six hours.) <br /><br />The device, a cylinder with accordion-like folds, is squeezed to create the suction, and then left in place, connected to the underside of the wound dressing by a thin plastic tube. At that point, it requires no further attention: "It holds its pressure for as long as there's not an air leak," Zurovcik explains. For that reason, a suitable dressing that can hold the seal is a crucial element of the system.<br /><br />The Haitian patients who were treated with the device were pleased with how it worked. While the team didn't have time to conduct long-term evaluations, Zurovcik says, "In the short term, we systematically evaluated the wounds, and were able to verify that negative-pressure therapy was being applied and the healing process was underway." <br /><br /><strong>'Enormous potential'</strong><br /><br />The trip to Haiti was led by Dr. Robert Riviello of the Division of Trauma, Burn and Surgical Critical Care at Brigham and Women's Hospital in Boston. Riviello estimates that between 50 million and 60 million people in low-income countries suffer from acute and chronic wounds, and a large number of them would benefit from negative-pressure wound therapy. He says the device "has the potential to be a great benefit to patients around the world" once a few technical hurdles are cleared. <br /><br />"Our biggest challenge at the moment is ensuring a reliably intact seal on human skin [that can be] easily applied," Riviello says. "If we can resolve this, then I think there is enormous potential."<br /><br />Zurovcik notes that an improved version of the device — one that maintains a more constant pressure and is smaller and so easier to conceal when being worn for days — has been developed and is being manufactured now.<br /><br />Zurovcik and her team designed the devices to be made in a sustainable way. They can be manufactured locally in many developing nations, using equipment that already exists there, she says. She is already in discussions with a plastic molding company in Rwanda, she says.<br /><br />She plans to go to Rwanda in the fall to test the new version of the device, which is small enough to carry in a pocket. "Their clinics are filled with wounds," she says, noting that the injuries are often severe because patients avoid going to clinics as long as they can. The clinics themselves "don't have power, don't have a lot of supplies. I'd like to be able to bring something simple, that patients would be able to care for on their own."<br /><br /><em><a href="http://web.mit.edu/newsoffice/topic/in-the-world.html">In The  World</a></em><em> is a column that explores the ways members of the MIT community are developing technology — from the appropriately simple to the cutting edge — to help meet the needs of communities around the planet, especially those in the developing world. If you have suggestions for future columns, please e-mail <a href="mailto:newsoffice@mit.edu" target="_blank">newsoffice@mit.edu</a>.</em><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Toward more efficient wireless power delivery]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/wireless-power-0409.html</link>
<story_id>15182</story_id>
<featured>0</featured>
<description><![CDATA[Latest research shows that efficiency improves when multiple devices are charged at once.]]></description>
<postDate>Tue, 13 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100412115240-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100412115240-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100412115240-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100412115623-2.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy MacArthur Foundation]]></imageCredits>
<imageCaption><![CDATA[Assistant professor of physics Marin Solja&#269;i&#263;]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In 2007, MIT researchers announced that they had discovered a novel way of transmitting electricity without the use of wires. Now, the researchers have demonstrated that the system’s efficiency at transmitting energy improves significantly when it is used to charge multiple devices at the same time.<br /><br />The new work, reported in a paper in <em>Applied Physics Letters</em> earlier this year, has also demonstrated a system much closer to one that could be used to power typical consumer electronic devices. In the original proof-of-concept in 2007, both the transmitter and receiver consisted of coils that were about two feet across. These two units were placed more than six feet apart and were used to light a 60-watt bulb — even with people sitting in between. But the new system uses a slightly larger transmitter, with receivers that are only about a foot across – moving closer to a size that could eventually be built into a PC or a television set. The transmitting coil could be built into a wall or ceiling, the researchers say, and the transfer of power has been shown to work over distances comparable to the size of an ordinary room.<br /><br />André Kurs, a doctoral student in MIT’s Department of Physics and the lead author of the recent paper, says this reduction in size of the receiving coil is an ongoing process. With some more work on further reducing the coil’s diameter and thickness, “we could embed it in a portable device,” he says.<br /><br />The basic underlying principle for transmitting power wirelessly goes back more than a century to the work of Nikola Tesla and other pioneers of electricity, but the MIT team invented a way of making the process far more efficient and practical. <br /><br />The system works by creating a strong electromagnetic resonance between the sending and receiving coils — similar to the way a tuning fork can start vibrating when exposed to a sound of exactly the right frequency, or the way a radio antenna can be tuned to just the frequency of a single station out of the hundreds that are simultaneously broadcasting their signals. In this case, the magnetic resonance between the two coils is unaffected by objects in between the coils, and by the same token objects between the coils — including people — are not affected by the magnetic fields.<br /><br />The key to that advance — that is, the ability to transmit useful amounts of power using coils of a reasonable size — was found in 2005 by MIT assistant professor of physics Marin Solja?i?, who developed the idea along with Kurs, students Aristeidis Karalis SM ’03 ScD ’08 (now a postdoctoral researcher) and Robert Moffatt ’09, and physics professors Peter Fisher and John Joannopoulos.<br /><br />Although predicted by theory, the increase in efficiency when powering two devices at the same time had not been previously demonstrated in experiments. The team that carried out the recent work — Kurs, Moffat and Solja?i? — found that when powering two devices at once, which individually could achieve less than 20 percent efficiency in power transfer, the combined efficiency climbed to more than 30 percent. The two receiving coils resonate with each other as well as with the transmitting coil, and help to reinforce the strength of the magnetic field. Kurs says that the efficiency should continue to rise as more devices are added, climbing toward a theoretical limit of 100 percent. The research has been funded by the NSF, the Army Research Office, DARPA, and a grant from 3M.<br /><br />The amount of power transmitted in the latest experiment was on the order of 100 watts, but Kurs says that is only limited by the amplifier used for the transmitting coil, and can easily be increased. “It could be several hundred watts, or a kilowatt,” he says — enough to power several typical household devices at once, such as lamps, computers or television sets. “You could feed power to a medium-sized room, and power a dozen devices,” he says.<br /><br />The researchers set up a company in 2007, called WiTricity, to develop the invention and eventually bring it to market. Most of the Watertown-based company’s principals and board of advisors are MIT professors, students, or alumnae. The company originally estimated it would take several years to develop a commercial product, and have “been making good progress. I think it’s reasonably close,” says Kurs, who works at the company while completing his doctorate. No further breakthroughs are required, the researchers say, just continued engineering work to find the optimum design of the coils and the electrical control systems.<br /><br />In addition to working on reducing the size of the receiving coils, the researchers are also trying to improve the system for tuning the devices to achieve maximum efficiency. In the laboratory tests, they spent considerable time manually tuning each part of the system, but for a practical consumer product this process will have to be fully automated. “It does get a little harder to tune multiple devices,” Kurs says.<br /><br />A number of other companies have independently jumped on the bandwagon and begun to develop similar wireless power systems, including large companies like microchip maker Intel and electronics giant Sony. “Quite a few companies have reproduced the original results,” Kurs says.<br /><br />And Tesla, whom the researchers acknowledge in the footnotes to their papers, would no doubt be pleased by the progress. “He did have the notion,” Kurs says, “but in practice it’s a hard thing to make work. You need a good model of how your coupling varies with distance and how to minimize the losses in the system, and people didn’t have a good understanding of it at the time.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Drug discovery, Netflix style?]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/drug-development-0413.html</link>
<story_id>15194</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers apply ranking algorithms to pharmaceutical R&D.]]></description>
<postDate>Tue, 13 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100414084554-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100414084554-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100414084554-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100412162830-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[Shivani Agarwal, a postdoctoral associate in the Computer Science and Artificial Intelligence Laboratory]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[In the last 10 years, the growth of the Internet has made ranking algorithms one of the hottest topics in computer science. The most famous ranking algorithm is Google’s, which determines the order of search results, but close behind are the Netflix and Amazon algorithms that make recommendations on the basis of customers’ prior decisions. Now researchers at MIT and Harvard Medical School have shown that ranking algorithms could find an important application in a somewhat surprising field: drug development.<br /><br />Drug development typically begins with the identification of a “target” — a molecule involved in the biological processes underlying some disease. The next step is to try to find chemicals that either promote or suppress the molecule’s production. Scientists have assembled huge libraries — both virtual and physical — of chemical compounds that might be active against biological targets, and drug developers who have identified a target usually select a group of candidate drugs from those libraries.<br /><br />But the majority of drug candidates fail — they prove to be either toxic or ineffective — in clinical trials, sometimes after hundreds of millions of dollars have been spent on them. (For every new drug that gets approved by the U.S. Food and Drug Administration, pharmaceutical companies have spent about $1 billion on research and development.) So selecting a good group of candidates at the outset is critical. <br /><br />Drug companies have been using artificial-intelligence algorithms to help select drug candidates since the late 1990s. But in a paper <a href="http://pubs.acs.org/doi/pdf/10.1021/ci9003865" target="_blank">appearing in the next issue</a> of the American Chemical Society’s <em>Journal of Chemical Information and Modeling</em>, Shivani Agarwal, a postdoctoral associate in the Computer Science and Artificial Intelligence Laboratory, Deepak Dugar, a graduate student in chemical engineering, and the Harvard Medical School’s Shiladitya Sengupta showed that even a rudimentary ranking algorithm can predict drugs’ success more reliably than the algorithms currently in use.<br /><br />At a general level, the new algorithm and its predecessors work in the same way. First, they’re fed data about successful and unsuccessful drug candidates. Then they try out a large variety of mathematical functions, each of which produces a numerical score for each drug candidate. Finally, they select the function whose scores most accurately predict the candidates’ actual success and failure.<br /><br />The difference lies in how the algorithms measure accuracy of prediction. When older algorithms evaluate functions, they look at each score separately and ask whether it reflects the drug candidate’s success or failure. The MIT researchers’ algorithm, however, looks at scores in pairs, and asks whether the function got their order right.<br /><br />“The criterion we’re giving it is almost the simplest ranking criterion you could construct,” Agarwal says. Nonetheless, in experiments involving data on existing drugs, it consistently predicted the drugs’ success more reliably than the algorithms now in use. The improvements were relatively modest, but to Agarwal, they’re an indication that recent research on more sophisticated ranking algorithms holds real promise for drug discovery.<br /><br />“There’s a really very systematic improvement over previous methods, and that’s quite striking,” says Peter Bartlett, a professor of computer science and engineering at the University of California, Berkeley. “This is a very nice empirical demonstration that these methods are more effective than the standard methods.”<br /><br />Anton Hopfinger, a professor at the University of New Mexico College of Pharmacy, cautions that when computer systems rank drug candidates, “the key component is not too surprisingly the properties of the drug or molecule you use to train the system.” That is, the success of the system depends crucially on the mathematical descriptions of the drug candidates. Even the ideal algorithm is helpless if it’s acting on data uncorrelated with a molecule’s biological activity.<br /><br />But Agarwal is a computer scientist, not a biologist. So while the biologists continue to refine their descriptions of the chemical properties of biological molecules, Agarwal continues to refine her algorithms for ranking drug candidates. At the moment, she’s investigating algorithms that maximize the accuracy of the rankings at the top of a list, even at the expense of lower rankings, since drug developers are generally interested in only a handful of the most promising drug candidates.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[John Reed nominated to chair MIT Corporation]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/corporation-head-0412.html</link>
<story_id>15190</story_id>
<featured>0</featured>
<description><![CDATA[Pending his election in June, the retired chairman and CEO of Citigroup will succeed Dana Mead.]]></description>
<postDate>Mon, 12 Apr 2010 14:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100412084156-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100412084156-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100412084156-1.jpg</fullURL>
<imageCaption><![CDATA[John S. Reed, retired chairman and CEO of Citigroup and former chairman of the New York Stock Exchange, has been nominated to serve as the next chair of the MIT Corporation.]]></imageCaption>
</image>
<body><![CDATA[John S. Reed, retired chairman and CEO of Citigroup and former chairman of the New York Stock Exchange, has been nominated to serve as the next chair of the MIT Corporation, the Institute's governing body. Reed '61, SM '65, has been nominated by the Executive Committee and has agreed to serve, pending his election by the Corporation at its June 4 meeting. <br /><br />Reed would succeed Dana G. Mead PhD '67, who announced last fall that he would step down as Corporation chair at the end of June. <br /><br />"I am honored and delighted by the prospect of serving in this important role and working with President Susan Hockfield and the Institute's senior leadership," said Reed. "I have been close to MIT for many years, which has afforded me the opportunity to appreciate the vital role it plays in the nation and the world."<br /><br />"John Reed would bring tremendous strengths to the role of MIT Corporation chair," said President Susan Hockfield. "He understands and appreciates deeply the values of the Institute, and through his career he has demonstrated a degree of breadth, creativity, foresight and global scope that will be enormously useful to MIT in the years ahead. The Institute has long benefited from John's guidance, and I know it would benefit even more from his leadership of the Corporation."<br /><br />Hockfield praised Mead for providing exceptional leadership, saying, "He has served the Institute with extraordinary dedication and a profound concern for our mission and ideals. He has made immeasurable contributions to MIT's governance, and has been instrumental in developing the resources the Institute needs to bring together the world's very best minds to solve its most intractable problems."<br /><br />Much of the MIT Corporation's work is conducted through its various committees. The MIT Corporation chair is, by virtue of the position, a member of the Executive Committee and the Investment Management Company board. The MIT Corporation chair also presides over all Corporation meetings and chairs the Corporation Development Committee and Membership Committee. <br /><br />Reed, a life member of the Corporation, serves or has served on the visiting committees for the Engineering Systems Division, the MIT Sloan School of Management and the Office of Sponsored Research. He currently chairs the Mathematics Visiting Committee.<br /><br />Born in Chicago in 1939, Reed was raised in Argentina and Brazil, where his father was an executive with Armour and Co. He received joint SB and BA degrees from MIT and Washington &amp; Jefferson College before serving for two years as an officer in the U.S. Army Corps of Engineers. Reed earned his master's degree in management from the MIT Sloan School of Management in 1965 and joined Citibank shortly thereafter.<br /><br />Over the next 35 years, Reed became known for ushering in many banking innovations in the United States and for raising Citibank's profile in emerging Asian and Latin American markets. He became the bank's chairman and CEO in 1984.<br /><br />After the bank merged with the Travelers Co. in 1998, Reed served as chairman and co-CEO of the new company, Citigroup. He retired from Citigroup in 2000, and then served as chairman of the New York Stock Exchange from September 2003 until April 2005. During this time, he helped establish new governance rules as the NYSE became a public corporation. More recently, Reed has voiced support for reforms intended to safeguard America's financial system.<br /><br />Reed has broad experience in the governance of corporate and academic institutions. He held long tenures on the boards of Philip Morris, United Technologies and Monsanto. Reed is currently a director of MDRC, a nonprofit, nonpartisan social policy research organization. He was chairman and a trustee of the Center for Advanced Study in the Behavioral Sciences, and was a board member at the Russell Sage Foundation, which supports research in the social sciences, and at the Spencer Foundation, which funds research aimed at improving education. He is a fellow of both the American Academy of Arts and Sciences and of the American Philosophical Society, and is an overseer of the Boston Symphony Orchestra.<br /><br />In addition to his work with the MIT Corporation, Reed serves on the MIT Energy Initiative's external advisory board and the Center for International Studies advisory board.<br /><br />Reed and his wife, Cynthia, live in Duxbury, Mass., and have a townhome in Boston. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Viruses harnessed to split water]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/belcher-water-0412.html</link>
<story_id>15186</story_id>
<featured>0</featured>
<description><![CDATA[MIT team’s biologically based system taps the power of sunlight directly, with the aim of turning water into hydrogen fuel.]]></description>
<postDate>Mon, 12 Apr 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100409114603-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100409114603-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100409114603-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Angela Belcher, the Germeshausen Professor of Materials Science and Engineering and Biological Engineering, demonstrates a virus-templated catalyst solution used in harnessing energy from water.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100409114603-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Professor Angela Belcher and her team have created a virus-templated catalyst solution used to harness energy from water.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100412090938-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Dominick Reuter]]></imageCredits>
<imageCaption><![CDATA[Professor Angela Belcher and graduate students Yoon Sung Nam (blue lab coat) and Heechul Park (white lab coat).]]></imageCaption>
</image>
<image>
<fullURL width='368' height='418'>http://web.mit.edu/newsoffice/images/article_images/20100412094003-4.jpg</fullURL>
<imageCredits><![CDATA[Graphic courtesy of Angela Belcher]]></imageCredits>
<imageCaption><![CDATA[A computer visualization of the biologically-based system shows the virus itself (in yellow) with molecules of pigment (in pink) and of the metal catalyst (brown spheres) attached to its surface. The pigment and catalyst cause water molecules to split apart when they come in contact.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[A team of MIT researchers has found a novel way to mimic the process by which plants use the power of sunlight to split water and make chemical fuel to power their growth. In this case, the team used a modified virus as a kind of biological scaffold that can assemble the nanoscale components needed to split the hydrogen and oxygen atoms of a water molecule.<br /><br />Splitting water is one way to solve the basic problem of solar energy: It’s only available when the sun shines. By using sunlight to make hydrogen from water, the hydrogen can then be stored and used at any time to generate electricity using a fuel cell, or to make liquid fuels (or be used directly) for cars and trucks.<br /><br />Other researchers have made systems that use electricity, which can be provided by solar panels, to split water molecules, but the new biologically based system skips the intermediate steps and uses sunlight to power the reaction directly. The advance is described in a paper published on April 11 in <em>Nature Nanotechnology</em>. The Italian energy company Eni supported the research through the MIT Energy Initiative  (MITEI).<br /><br />The team, led by Angela Belcher, the Germeshausen Professor of Materials Science and Engineering and Biological Engineering, engineered a common, harmless bacterial virus called M13 so that it would attract and bind with molecules of a catalyst (the team used iridium oxide) and a biological pigment (zinc porphyrins). The viruses became wire-like devices that could very efficiently split the oxygen from water molecules.<br /><br />Over time, however, the virus-wires would clump together and lose their effectiveness, so the researchers added an extra step: encapsulating them in a microgel matrix, so they maintained their uniform arrangement and kept their stability and efficiency.<br /><br />While hydrogen obtained from water is the gas that would be used as a fuel, the splitting of oxygen from water is the more technically challenging “half-reaction” in the process, Belcher explains, so her team focused on this part. Plants and cyanobacteria (also called blue-green algae), she says, “have evolved highly organized photosynthetic systems for the efficient oxidation of water.” Other researchers have tried to use the photosynthetic parts of plants directly for harnessing sunlight, but these materials can have structural stability issues.<br /><br />Belcher decided that instead of borrowing plants’ components, she would borrow their methods. In plant cells, natural pigments are used to absorb sunlight, while catalysts then promote the water-splitting reaction. That’s the process Belcher and her team, including doctoral student Yoon Sung Nam, the lead author of the new paper, decided to imitate.<br /><br />In the team’s system, the viruses simply act as a kind of scaffolding, causing the pigments and catalysts to line up with the right kind of spacing to trigger the water-splitting reaction. The role of the pigments is “to act as an antenna to capture the light,” Belcher explains, “and then transfer the energy down the length of the virus, like a wire. The virus is a very efficient harvester of light, with these porphyrins attached. <br /><br />“We use components people have used before,” she adds, “but we use biology to organize them for us, so you get better efficiency.”<br /><br />Using the virus to make the system assemble itself improves the efficiency of the oxygen production fourfold, Nam says. The researchers hope to find a similar biologically based system to perform the other half of the process, the production of hydrogen. Currently, the hydrogen atoms from the water get split into their component protons and electrons; a second part of the system, now being developed, would combine these back into hydrogen atoms and molecules. The team is also working to find a more commonplace, less-expensive material for the catalyst, to replace the relatively rare and costly iridium used in this proof-of-concept study.<br /><br />Thomas Mallouk, the DuPont Professor of Materials Chemistry and Physics at Pennsylvania State University, who was not involved in this work, says, “This is an extremely clever piece of work that addresses one of the most difficult problems in artificial photosynthesis, namely, the nanoscale organization of the components in order to control electron transfer rates.”<br /><br />He adds: “There is a daunting combination of problems to be solved before this or any other artificial photosynthetic system could actually be useful for energy conversion.” To be cost-competitive with other approaches to solar power, he says, the system would need to be at least 10 times more efficient than natural photosynthesis, be able to repeat the reaction a billion times, and use less expensive materials. “This is unlikely to happen in the near future,” he says. “Nevertheless, the design idea illustrated in this paper could ultimately help with an important piece of the puzzle.”<br /><br />Belcher will not even speculate about how long it might take to develop this into a commercial product, but she says that within two years she expects to have a prototype device that can carry out the whole process of splitting water into oxygen and hydrogen, using a self-sustaining and durable system.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Weighing the cell]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/cell-growth-0412.html</link>
<story_id>15187</story_id>
<featured>0</featured>
<description><![CDATA[MIT biological engineers devise a way to measure, for the first time, how single cells accumulate mass.]]></description>
<postDate>Mon, 12 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100409125517-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100409125517-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100409125517-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[Scott Manalis]]></imageCaption>
</image>
<body><![CDATA[Using a sensor that weighs cells with unprecedented precision, MIT and Harvard researchers have for the first time measured the rate at which single cells accumulate mass — a feat that could shed light on how cells control their growth and why those controls fail in cancer cells.<br /><br />The research team, led by Scott Manalis, MIT associate professor of biological engineering, revealed that individual cells vary greatly in their growth rates, and also found evidence that cells grow exponentially (meaning they grow faster as they become larger).<br /><br />The new measurement system, reported in the April 11 edition of the journal <em>Nature Methods</em>, is the first technique that can measure cells’ mass as they grow over a period of time (in this case, ranging from five to 30 minutes). Previous methods for measuring cell growth rates have focused on volume or length measurements, and have not yet exhibited the necessary precision for revealing single-cell growth models.<br /><br />The new method should give researchers a way to unravel the relationship between cell growth and cell division — a relationship that has long been murky, says Marc Kirschner, professor of systems biology at Harvard Medical School. While biologists have a good idea of how the cell division cycle is controlled, “the problem of cell growth — how a cell regulates the amount of material it makes — is not well known at all,” says Kirschner, an author of the Nature Methods paper.<br /><br /><strong>Controlled growth</strong><br /><br />A longstanding question in studies of cell growth is whether growth is linear or exponential. Previous studies have yielded conflicting data. <br /><br />“Over the twofold size range experienced by most proliferating cells, linear and exponential growth curves differ by less than 10 percent, and so the measurement precision must be much less than this,” says Manalis, a member of MIT’s David H. Koch Institute for Integrative Cancer Research. <br /><br />The researchers studied four types of cells: two strains of bacteria (<em>E. coli</em> and <em>B. subtilis</em>), a strain of yeast and mammalian lymphoblasts (precursors to white blood cells). They showed that B. subtilis cells appear to grow exponentially, but they did not obtain conclusive evidence for <em>E. coli</em>. That’s because there is so much variation between individual cell growth rates in E. coli, even for cells of similar mass, says Francisco Delgado, a grad student in Manalis’ lab and co-lead author of the paper.  <br /><br />If cells do grow exponentially, it means there must be some kind of mechanism to control that growth, says Kirschner. Otherwise, when cells divide into two slightly different-sized daughter cells, as they often do, the larger cell in each generation would always grow faster than the smaller cell, leading to inconsistent cell sizes. Instead, cells generally even out in size, through a mechanism that biologists don’t yet understand.<br /><br /><strong>Going with the flow</strong><br /><br />The cell-mass sensor, which Manalis first demonstrated in 2007, consists of a fluid-filled microchannel etched in a tiny silicon slab that vibrates inside a vacuum. As cells flow through the channel, one at a time, their mass slightly alters the slab’s vibration frequency. The mass of the cell can be calculated from that change in frequency, with a resolution as low as a femtogram (10-15 grams) which is less than 0.01 percent of the weight of a lymphoblast cell in solution.<br /><br />Michel Godin, a former postdoctoral associate in Manalis’ lab and co-lead author of the paper, developed a way to trap a cell within the microchannel by precisely coordinating the flow direction. That enables the researchers to repeatedly pass a single cell through the channel every second or so, measuring it each time it moves through. <br /><br />The new system represents a significant advance over any existing cell measurement technique, says Fred Cross, a Rockefeller University professor who studies the yeast cell cycle. “Since it directly measures biomass (at least net biomass with density greater than water) by the truly remarkable expedient of effectively directly placing a single cell on a scale, it is not troubled by ambiguities and inaccuracies inevitably associated with previous, more indirect measurements,” Cross says.<br /><br />In their current studies, Manalis and his students are tagging proteins inside the cell with fluorescent molecules that reveal what stage of the cell cycle the cell is in, allowing them to correlate cell size with cell-cycle position and ultimately obtain a growth model for yeast and mammalian cells. They are also working on a way to add chemicals such as nutrients, antibiotics and cancer drugs to the fluid inside the microchannel so their effect on growth rates can be studied.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Friends, family gather to honor Paul Samuelson]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/samuelson-memorial-0412.html</link>
<story_id>15188</story_id>
<featured>0</featured>
<description><![CDATA[At memorial service, Nobel-winning economist’s ‘warmth, wit and humility’ are recalled.]]></description>
<postDate>Mon, 12 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100409114929-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100409114929-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100409114929-1.jpg</fullURL>
<imageCaption><![CDATA[Paul Samuelson]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='311'>http://web.mit.edu/newsoffice/images/article_images/20100412095026-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: L. Barry Hetherington]]></imageCredits>
<imageCaption><![CDATA[Speakers at the memorial service for Paul A. Samuelson, held at MIT’s Kresge Auditorium on Saturday, April 10:

(From left) James Poterba, Mitsui Professor of Economics at MIT; Ricardo Caballero, Ford International Professor of Economics at MIT and chairman of the Department of Economics; Helmut Weymar; Stanley Fischer, Governor of the Bank of Israel; MIT President Susan Hockfield; Robert Solow, MIT Institute Professor, Emeritus, and Professor of Economics, Emeritus; Paul Krugman, professor of economics and international affairs at Princeton University; Lawrence Summers, director of the White House’s National Economic Council; and William Samuelson, professor of business policy and law at Boston University.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Family members, friends and admirers gathered to pay tribute to Paul Samuelson during a memorial service on Saturday at MIT, where the legendary economist was recalled as a man of kindness, unyielding good humor and boundless intellectual curiosity.<br /><br />Several hundred people attended the service in honor of Samuelson, a singular figure in 20th-century economics whose work reshaped the subject into a highly rigorous, mathematical discipline, and whose energy and renown quickly helped build one of the world’s leading economics departments after his arrival at MIT in 1940.<br /><br />“He achieved that stature and transformed his field in a way that I think of as quintessentially MIT,” said MIT President Susan Hockfield at the service. “He entered a discipline that was largely descriptive and he used the tools of mathematics to make it rigorous, coherent, and predictive. In his commitment to both the research and teaching of economics, he embodied an MIT ideal.” <br /><br />Moreover, as Hockfield noted, “His warmth, wit and humility shaped the collaborative culture that became the department’s signature.”<br /><br />Samuelson died in December, at the age of 94. His PhD thesis, published as <em>Foundations of Economic Analysis</em>, became a transformative work, while his book <em>Economics</em>, first published in 1948, is the best-selling textbook of all time in the field. In 1947, Samuelson was awarded the first-ever John Bates Clark Medal by the American Economic Association, now given annually to the best economist under the age of 40; in 1970, he was awarded the second-ever Nobel Prize in economics. <br /><br /><strong>Robert Solow: ‘We all had an office next to Paul’</strong><br /><br />Samuelson’s great friend and colleague, Robert Solow, served as master of ceremonies for the event, and shared a variety of reflections. For nearly 60 years, Solow occupied an office next door to Samuelson’s, engendering decades of intellectual collaboration and bonhomie. <br /><br />“We talked about what was happening in economics, the economy, what was happening at home,” said Solow. And yet, he said, Samuelson’s sociability was such that “in a sense, we all had an office next to Paul.” Samuelson, he noted, would say that “you could argue whether MIT was the best economics department in the world, but there could be no doubt it was the happiest department in the world.” <br /><br />A principal reason for that, Solow said, is that Samuelson insisted on being treated as just one member of the faculty among equals. “Nobody was pulling rank” at MIT, said Solow, himself a Nobel laureate. “You would have needed an impossible amount of chutzpah to act like a prima donna if Paul Samuelson were your colleague, and he was clearly not acting like a prima donna.”<br /><br />Even in recent years, Samuelson’s sociable and intellectual qualities shone through at events like the department’s weekly lunches, noted Ricardo Caballero PhD ’88, Ford International Professor of Economics at MIT and the current chair of the department. Several years ago, Caballero was so inspired by the then-octogenarian Samuelson’s remarks on mathematical analysis one day at lunch, he simply took the rest of the day off to savor the experience: “I drove home in complete awe.”<br /><br />“He will remain forever our beacon of intellectual life,” said Caballero. “This is his department and nothing can change that.” <br /><br />Samuelson had “an absolute refusal to use authority rather than reason,” added Stanley Fischer PhD ’69, the former MIT economist who is now governor of the Bank of Israel. “He treated his students as fellow economists, rather than as potential future disciples.” <br /><br />One of Paul Samuelson’s sons, William Samuelson, now a professor of business policy and law at Boston University, spoke on behalf of his family. He recalled Paul Samuelson as a caring father whose mind seemed almost constantly in motion; he “usually carried a yellow legal pad and jotted notes and theories in odd places.” <br /><br />That intellectual activity, said William Samuelson, was made possible by the level of personal comfort his father felt at MIT: “How much less would he have accomplished any place else? A lot less. How much less happy would he have been any place else? A lot less.”<br /><strong><br />Ace … in economics </strong><br /><br />Paul Samuelson’s nephew, Lawrence Summers ’75, the former U.S. Treasury secretary who is now director of the National Economic Council at the White House, spoke of the power Samuelson’s ideas have had on government policy, noting that Samuelson’s work was known to presidents from Franklin Roosevelt to Barack Obama. <br /><br />“Paul took pride in saying he never spent more than three consecutive nights in Washington,” said Summers. “But his record of influence was unmatched.” Yet attaining that recognition, Summers added, was not what drove Samuelson. “Anyone who knew Paul knew he didn’t live for the notoriety,” Summers said. “He lived professionally for his scholarship.” <br /><br />Summers also aired a series of personal reflections about Samuelson. <br /><br />“Paul was a bracing and yet a generous uncle,” said Summers, recalling being stumped as a child by Samuelson’s questions about how temperature is measured. <br />“Nobody punctured pomposity better or more definitively than he did,” said Summers. “But he also knew about comforting the afflicted.” <br /><br />When Summers was suffering from a serious illness, he recollected, Samuelson helped him rehabilitate by playing tennis with him. Samuelson took up tennis in his 40s, playing near his home in Belmont, and his aptitude for the sport served as the source of a few good-natured remarks during the service. “It’s not true that Paul did everything extraordinarily well,” Summers joked. <br /><br />Moreover, Solow added later, when <em>Newsweek</em> magazine once profiled Samuelson, it described him as an “amusing lecturer and fine tennis player.” However, Solow noted, MIT economist Cary Brown quipped at the time that the magazine had things backward: “He was a fine lecturer and an amusing tennis player.” <br /><br /><strong>Playful and accessible</strong><br /><br />Nobel laureate Paul Krugman PhD ’77, who shared a three-person office suite with Samuelson and Solow for years as an MIT faculty member, recalled the first time he studied Samuelson’s persona in depth: For an annual graduate student-faculty skit night in the 1970s, Krugman noted, he had to play Samuelson in a sketch, and so spent time honing an imitation of Samuelson’s walk (which Krugman briefly demonstrated for the audience on Saturday). <br /><br />The good humor and cheer Samuelson created, Krugman claimed, was actually an essential part of his working style, helping him produce seminal works in trade, the government provision of public goods, consumer preferences, finance and more. “The greatest economist of the century did not take himself too seriously,” said Krugman. “He was informal, conversational and always, always intellectually playful, eager to try out new ideas and other people’s points of view. That same spirit very much informed his work, and made it possible.” <br /><br />Still, it was a struggle at times for Samuelson’s work to become accepted, noted James Poterba, MIT’s Mitsui Professor of Economics and a former department chair, precisely because Samuelson was creating “a scientific revolution in economics.” Even into the 1950s, Poterba noted, quoting examples from the archives, reviewers at journals were skeptical of Samuelson’s mathematical inclinations. “But history has proven him right,” said Poterba. <br /><br />Samuelson’s influence also extended into the worlds of investment and trading; Helmut Weymar ’58, PhD ’65, the former president of the trading firm Commodities Corp., spoke about how he used insights gleaned from his time as an MIT student to devise pioneering trading techniques. And yet, Weymar said, he never would have been accepted as a graduate student at MIT except that Samuelson took a special interest in his studies. “As gods go, Paul was remarkably accessible,” said Weymar. <br /><br />A portrait of Samuelson, painted by the artist Richard Whitney, was unveiled near the end of the event; Samuelson’s wife Risha has donated the painting to the Department of Economics, where it will hang. A string quartet of current and former MIT students also played at the service.<br /><br />“Having had Paul as a best friend for over 60 years was an extraordinarily important part of my life,” reflected Solow, concluding the service. “You might think that 60 years was a lot, but I, and we, could have used a few more.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[TV outside the box]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/augmented-tv-0409.html</link>
<story_id>15183</story_id>
<featured>0</featured>
<description><![CDATA[Using ordinary cell phones, a Media Lab system would let television programs spill off the TV screen and into the living room.]]></description>
<postDate>Fri, 09 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100408155500-1.png</thumbURL>
<smallURL width='140' height='95'>http://web.mit.edu/newsoffice/images/article_images/w140/20100408155500-1.jpg</smallURL>
<fullURL width='180' height='123'>http://web.mit.edu/newsoffice/images/article_images/20100408155500-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Melanie Gonick]]></imageCredits>
<imageCaption><![CDATA[In the same way that surround sound lets TV viewers hear what’s happening just off-screen, a new system from the Media Lab gives them the option of watching what’s happening, too, on the screen of a handheld device.]]></imageCaption>
</image>
<body><![CDATA[Augmented reality is an emerging discipline that uses handheld devices to superimpose digital data on the real world: If, say, you’re in Paris and point your phone at the Eiffel Tower, the tower’s image would pop up on-screen, along with, perhaps, information about its history or the hours that it’s open. With a TV enhancement called Surround Vision, however, researchers at MIT’s Media Lab are bringing the same technology into the living room. <br /><br /> 










<br /><strong>Video: Melanie Gonick</strong><br /><br />Santiago Alfaro, a graduate student in the lab of Media Lab research scientist Michael Bove, had spent a year investigating the use of cell-phone cameras as digital interfaces when he had an intriguing thought. The soundtracks of contemporary movies, TV shows and video games are often conceived in two dimensions rather than just one: Surround sound technology can allow an audience to, in effect, hear what’s happening off screen. “If you’re watching TV and you hear a helicopter in your surround sound,” Alfaro says, “wouldn’t it be cool to just turn around and be able to see that helicopter as it goes into the screen?”<br /><br />Surround Vision is intended to work with standard, Internet-connected handheld devices. If a viewer wanted to see what was happening off the left edge of the television screen, she could simply point her cell phone in that direction, and an image would pop up on its screen. The technology could also allow a guest at a Super Bowl party, for instance, to consult several different camera angles on a particular play, without affecting what the other guests see on the TV screen.<br /><br />To demonstrate the idea, Alfaro shot video footage of the street in front of the Media Lab from three angles simultaneously. A television set replays the footage from the center camera. If a viewer points a motion-sensitive handheld device directly at the TV, the same footage appears on the device’s screen. But if the viewer swings the device either right or left, it switches to one of the other perspectives. The viewer can, for instance, watch a bus approach on the small screen before it appears on the large screen.<br /><br />Alfaro and Bove envision that, if the system were commercialized, the video playing on the handheld device would stream over the Internet: TV service providers wouldn’t have to modify their broadcasts or their set-top boxes. The handheld would simply use its built-in camera to determine its orientation toward the TV and the selected channel; for both purposes, it might cue off of the insignias that most stations display in the lower right corner of the TV screen. Viewers who opted not to use the enhanced programming need never know they were missing anything. Those who did use it wouldn’t need anything other than a smart phone and a standard Wi-Fi connection.<br /><br />Many existing handheld devices have built-in motion detectors called accelerometers. But switching between viewing angles with the Surround Vision system seems to involve motion too subtle for accelerometers to register. To get his prototype up and running, Alfaro had to attach a magnetometer — a compass — to an existing handheld device and to write software that incorporated its data with that from the device’s other sensors. But Alfaro and Bove say that devices now on the market — including the most recent version of the iPhone — have magnetometers built in. The researchers have answered most of the technical questions about the system; now their chief concern is determining how best to use it.<br /><br />To that end, they plan a series of user studies in the spring and summer, which will employ content developed in conjunction with a number of partners. Bove won’t name names, but, he says, “We’re looking at sports; we’re looking at children’s programming, both live action and cartoons; we’re looking at, let’s say, ordinary entertainment programs, as well as programs shot in a studio like talk shows. And we hope to have examples of several of these fairly soon. There are also one or two other things that defy categorization right now, that you sort of have to see in order to understand what they are.”<br /><br />Since sports broadcasts and other live television shows already feature footage taken from multiple camera angles, they’re a natural fit for the system. Many children’s shows already encourage a kind of audience participation that could be enhanced, Bove says, and viewers of the type of criminal-forensics shows now popular could use Surround Vision to, say, see what the show’s protagonists are looking at through the microscope lens.<br /><br />Of course, today’s TV shows frequently feature short scenes with quick cuts between different camera angles, which would allow little time for exploration of the virtual environment. But “the system as it stands should not be a constant feature,” Alfaro says. “We can’t ask a user to constantly hold a device up at arm’s length: They will quickly tire and not use it at all.” Alfaro suggests that as TV directors grew to appreciate the system’s strengths, they would likely modify their styles to take advantage of them.<br /><br />One partner likely to participate in the user studies is Boston’s public-television station WGBH, which has a long history with Bove’s lab. “We always learn from working with Mike and his group,” says Annie Valva, WGBH’s director of technology and interactive multimedia. Whether an experimental technology leads directly to new applications, it allows the station’s staff to “look at content in the archives from a different perspective, because there’s another emerging platform to make it available,” Valva says. “The broadcast is a very limited medium, and oftentimes what we put on is what makes the best story, and what fits in the programming schedule. But with a technology like Surround Vision, it helps us leverage more value over things that we shoot and create but don’t happen to get to air.”<br /><br />If the user studies conducted by Bove’s group converge on some applications that resonate with viewers, however, Surround Vision could prove to be something more than a spur to the imagination. “In the Media Lab, and even my group, there’s a combination of far-off-in-the-future stuff and very, very near-term stuff, and this is an example of the latter,” Bove says. “This could be in your home next year if a network decided to do it.”<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Volcanic Venus]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/volcanic-venus-0409.html</link>
<story_id>15180</story_id>
<featured>0</featured>
<description><![CDATA[New research highlights recent volcanic activity on Venus, indicating that Earth’s sister planet is alive — geologically speaking]]></description>
<postDate>Fri, 09 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100408130124-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100408130124-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100408130124-1.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA/JPL/ESA]]></imageCredits>
<imageCaption><![CDATA[Based on topographic data obtained by NASA’s Magellan spacecraft, this figure shows the volcanic peak Idunn Mons in the Imdr Regio area of Venus. The colored overlay shows the heat patterns derived from surface brightness data collected by VIRTIS aboard the European Space Agency’s Venus Express spacecraft. The brightness signals the composition of the minerals that have been changed due to lava flow. Red-orange is the warmest area and purple is the coolest.]]></imageCaption>
</image>
<body><![CDATA[Scientists have detected for the first time recent volcanic activity on Venus, the planet that is the most similar to Earth in terms of mass and density, but that has a surface temperature hot enough to melt lead.<br /><br />Knowing that Venus is volcanically active could shed light on the mysterious geological history of Earth’s sister planet, which does not have plate tectonics, meaning the planet’s surface does not evolve through a process of rigid plates slowly shifting across the underlying mantle. Because the runaway greenhouse effect, or the phenomenon that occurs when a planet absorbs more energy from the sun than it can radiate back, was first discovered on Venus, this finding could also lead to a better understanding of climate change in general and, more specifically, how gas emitted from volcanoes may affect a planet’s atmosphere.<br /><br />Although previous data suggested volcanic activity on Venus, it wasn’t until now that scientists were able to estimate how recent that activity was. By analyzing several “hot spots” on the Venusian surface — volcanic areas located on topographic rises that are thousands of kilometers wide — a team of scientists, including Lindy Elkins-Tanton, the Mitsui Career Development Assistant Professor of Geology in MIT’s Department of Earth, Atmospheric and Planetary Sciences, concluded that several lava flows are up to 250 years to 2.5 million years old. Considering Venus is about 4.6 billion years old and its overall surface is thought to be about 500 million years old, even a 2.5 million-year-old lava flow would mean the planet is volcanically active, according to a paper published Thursday in <em>Science</em>.<br /><br />These “recent” lava flows support one theory about the planet’s evolution: that Venus resurfaces gradually through volcanism as a result of Earth-like processes involving heat loss from the interior, rather than through cataclysmic bursts of volcanic activity that are followed by long periods (hundreds of millions of years) of inactivity.<br /><br />Lead author Sue Smrekar, a geophysicist at NASA’s Jet Propulsion Laboratory, explained that studying the rate of volcanism on Venus will help determine how eruptions may have contributed to the high sulphur dioxide concentrations in the planet’s atmosphere that limit how much solar radiation escapes from it. “Understanding the link between interior processes, gases given off during volcanism, and climate conditions on Venus can help us better understand climate processes on Earth, as well as help us interpret new data for planets in other solar systems,” she said.<br /> <br /><strong>'Hot spot' activity</strong><br /><br />Using topography and gravity data collected by NASA’s Magellan spacecraft during the 1990s, scientists indentified nine hot spots on Venus. Smrekar’s team then analyzed data for three hot spots observed by the Visible and Infrared Thermal Imaging Spectrometer (VIRTIS).  VIRTIS is a device that is located onboard the European Space Agency’s Venus Express spacecraft launched in 2006 and that measures thermal variations of the planet’s surface. <br /><br />By comparing the two sets of data, the scientists discovered that certain parts of the hot spots had “anomalously high” emissivity, or the ability to radiate energy. Because calculations and lab experiments replicating the Venusian surface and atmosphere show that emissivity decreases over time, emissivity can be an important indicator of a substance’s age. “These apparently young rocks correspond to the youngest volcanic flows in each hot-spot region, giving us confidence that they formed during recent volcanic activity,” Smrekar said. <br /><br />The scientists believe that the lava flows have such a high emissivity because they are fresh — no more than 250 years to 2.5 million years old. They determined this range by using Magellan data to estimate the volume of the lava flows in the hot spot regions. They then divided the estimated volume levels by various rates of resurfacing that support the estimated date of the planet’s most recent resurfacing event. The results provide an estimated age range for the lava flows.<br /><br />Because lab experiments suggest that weathering, or the process by which rocks react with the atmosphere, proceeds very rapidly on Venus, Smrekar’s team believes that the smallest values are more likely for the age of the lava flows, which indicates that Venus resurfaces through continual smaller volcanic eruptions rather than a more catastrophic process. Smrekar pointed out that the volcanoes could even be currently active, but that there is no data to confirm that.<br /><br />Roger Phillips, a planetary scientist at the Southwest Research Institute, called the result “as close as possible to a smoking gun” in the hunt for evidence that Venus is volcanically active. He added, however, that in order to make even more progress in determining how Venus resurfaces, scientists need to learn exactly how weathering surface rates affect emissivity. “If you knew that rate, you could pinpoint precisely how old the lava flows are,” he explained. <br /><br /><strong>Charting a mission</strong><br /><br />Smrekar’s follow-up work will include deeper analyses of the anomalies seen in the emissivity data, but she noted that such work is hampered by limited and outdated data. “We’d love to know more about the minerals on the surface, but those that we think are there are based on data from Soviet landers that landed on Venus 30 years ago,” she said, adding that the poor resolution of Magellan’s topography data makes it very difficult to interpret the emissivity anomalies in areas of rough topography.<br /><br />This finding of recent volcanic activity contributes to an uptick in Venus research that has occurred in anticipation of a possible mission to the planet, such as one that has been proposed for NASA’s New Frontiers program. As a member of the team that has proposed that mission, Elkins-Tanton believes this discovery reinforces the need for a mission to collect data to refine our understanding of Venus. “We need to reconsider its evolutionary history and the processes that forced Earth’s sister planet to develop into its inhospitable current state,” she said. “There is so little data on Venus that anything we can learn would be revolutionary,” she said.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: Solar power, shaped up]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/slideshow-origami-0408.html</link>
<story_id>15179</story_id>
<featured>0</featured>
<description><![CDATA[3-D shapes covered in solar cells could produce more power than flat panels, MIT researchers find.]]></description>
<postDate>Thu, 08 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100407153220-5.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100407153220-5.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100407153220-5.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[The program explored shapes that included multiple curves and angles.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100407153126-1.jpg</fullURL>
<imageCredits><![CDATA[istockphoto]]></imageCredits>
<imageCaption><![CDATA[Conventional photovoltaic solar panels are flat, and can be installed horizontally, angled to face the average height of the sun, or mounted on a tracking mechanism.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100407153126-2.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[MIT professor Jeffrey Grossman used a computerized system to let possible shapes for solar panels evolve over time, starting with simple basic shapes. This is an example of a shape that was found to be quite effective because it could catch the sun lower in the sky, and also some surfaces could reflect sunlight onto others.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100407153127-3.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[Shapes that were less efficient were culled from the mix, and the best shapes were combined to produce new hybrid forms.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100407153127-4.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[Some of the shapes produced by the computer program were quite complex.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='226'>http://web.mit.edu/newsoffice/images/article_images/20100407153220-6.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[The shape on left was one of the most efficient ones generated by the program, but also one of the most complex. That complexity means it would be impractical to make, so the team produced a simplified version (right) that performed almost as well. Such forms could be designed to be shipped flat, then unfolded at their installation site to their full 3-D shape.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='315'>http://web.mit.edu/newsoffice/images/article_images/20100407153220-7.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[Simulations show that the higher the 3-D panels extend up from the horizontal, the greater their power output, and that they produce a relatively stable output over the course of a day (blue lines) as compared to flat, horizontal panels (red).]]></imageCaption>
</image>
<image>
<fullURL width='368' height='177'>http://web.mit.edu/newsoffice/images/article_images/20100408095634-8.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Jeffrey Grossman, Bryan Myers and Marco Bernardi]]></imageCredits>
<imageCaption><![CDATA[These images illustrate how the genetic algorithm used by the researchers -- based on the principles of evolution -- can start from complete randomness and lead to complex, highly efficient shapes.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<br /><br />Flat solar photovoltaic panels are becoming more widespread, but the power they produce varies over the course of the day as the sun’s position changes — unless the panels are mounted on tracking systems to keep them pointed sunward, which adds complexity and expense.<br /><br />Jeffrey Grossman, the Carl Richard Soderberg Associate Professor of Power Engineering at MIT’s Department of Materials Science and Engineering (DMSE), was inspired by the way trees spread their leaves to capture sunlight and wondered how efficient a three-dimensional shape covered in solar cells could be, and what its optimal shape would look like. He worked with a second-year DMSE graduate student, Marco Bernardi, to create a computer program that mimics biological evolution, starting with basic shapes and letting them evolve, changing slightly each time and selecting those that perform best to start the next generation. He found that such systems could produce relatively constant power throughout the day without the need for tracking, and produce significantly more power overall for a given area — for example two and a half times as much as a flat array when the height equals the length and width. He is continuing to work on finding the best shapes and teaming up with Professors Vladimir Bulovi? and David Perreault (EECS) to build a prototype system. The team believes that solar panels based on this concept could be shipped flat and then unfolded at the site to their complex shapes.<br /><br />These images show some of the varied shapes with improved efficiency that emerged from the evolving simulation.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Using plants to purify canal water]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/italy-water-0407.html</link>
<story_id>15177</story_id>
<featured>0</featured>
<description><![CDATA[Researchers outline a natural way to clean Italy's polluted Pontine Marshes]]></description>
<postDate>Wed, 07 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100406152203-7.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100406152203-7.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100406152203-7.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Case Brown, Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[An artist's rendering of a wetlands in the Pontine Marshes.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='315'>http://web.mit.edu/newsoffice/images/article_images/20100406122718-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[A map of the Pontine Marshes, which cover 300 square miles in Italy’s Lazio region, just south of Rome. The area’s canals, built in the 1930s, have become heavily polluted. The green dots show areas with an excess of nitrates, while the yellow dots indicate an excess of phosphorus.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='274'>http://web.mit.edu/newsoffice/images/article_images/20100406122719-2.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[Alan Berger of MIT’s Department of Urban Studies and Planning has proposed cleaning the Pontine Marshes by building wetlands in the heavily polluted areas, where toxins from the water would be absorbed by plants. An initial design from his study proposes integrating wetlands and new public parkland.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='274'>http://web.mit.edu/newsoffice/images/article_images/20100406122720-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[To test his wetlands concept, Alan Berger joined forces with professor Heidi Nepf and other researchers from MIT’s Department of Civil and Environmental Engineering. The group built multiple models in Parsons Laboratory to study the way water would flow through different wetlands plans.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100406122830-4.jpg</fullURL>
<imageCredits><![CDATA[Photo: Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[Because certain plants absorb toxins, wetlands with copious vegetation can clean up pollution. However, the water flowing through wetlands must move in a slow, evenly-dispersed pattern, to ensure maximum contact with the plants. Here the researchers inject fluorescent dye into the water to analyze its flow pattern. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100406122831-5.jpg</fullURL>
<imageCredits><![CDATA[Photo: Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[The researchers found that one plan worked best: Wetlands with broad channels, heavy vegetation underneath, and small “islands” that move the water laterally. A fluorometer measures the intensity of the fluorescent dye (and hence how broadly the water has spread) as the water exits the model.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100406122831-6.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Case Brown, Project for Reclamation Excellence]]></imageCredits>
<imageCaption><![CDATA[A rendering of a wetlands in the Pontine Marshes. The engineering analysis of Berger’s designs has helped him understand which wetlands configurations would work best. In this way planners and scientists can join forces to help reclaim damaged countryside for the public good.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<br /><br />Just south of Rome lie the Pontine Marshes, a vexed part of the Italian countryside. In ancient times, Roman emperors tried unsuccessfully to drain the marshes, something only achieved in the 1930s through a system of massive pumps and canals that removed enough water to turn the area into productive farmland. Yet today those canals have become heavily polluted, endangering the area’s agriculture and the health of its residents.<br /><br />The conventional way of tackling the problem would be to build a series of large water-treatment plants in the area, which covers about 300 square miles. But Alan Berger, an associate professor of urban design and landscape architecture at MIT, has another idea. Because some plants absorb pollutants as water flows by them, carefully designed wetlands can clean up the countryside while preserving its natural feel and providing public park space.<br /><br />Berger realized this notion could apply to the Pontine Marshes while on a fellowship at the American Academy in Rome in 2007-2008. He then began trying to persuade officials of the Italian region of Lazio, in the province of Latina, where the marshes are located, to consider the notion. Now, after performing innovative interdisciplinary testing with researchers in MIT’s Department of Civil and Environmental Engineering, Berger believes he knows how to make the concept work best. Regional officials have since become enthusiastic about the idea: In September 2009, Lazio’s government received a grant for 4.5 million euros as part of the European Union’s “Life+” program for environmental works, specifically to clean up the area using natural processes. <br /><br />That represents a reversal, Berger claims, from the response he usually got when he started floating the idea in Italy. “People said I was insane,” recalls Berger, who has designed several large-scale environmental cleanup projects in his career. “But if you do good research, you can change the type of project that is done.” As Lazio’s planning director, Carlo Perotto, said of Berger in 2008: “He opened a new way of thinking.” <br /><br /><strong>Why inefficiency, here, works</strong><br /><br />As the waterways of the Pontine Marshes move from inland hills to the sea, they accumulate excess nitrogen and phosphorus from fertilizer used on the adjacent farmland. That affects the crops and livestock using the water further downstream, creating potential hazards throughout the food chain.<br /><br />However, water polluted in this way can be cleansed after coming into contact with the right kind of vegetation. Marsh grasses absorb toxins and effectively remove them from circulation in the ecosystem. The wetlands Berger wants to build in the Pontine Marshes would direct polluted canal water into a twisting channel with vegetation, from which the water would emerge clean. <br /><br />The precise amount of pollution in the marshes varies by area and season, but in some areas near the sea, notes Berger, the canals almost wholly contain waste water. Wetlands can reduce pollution levels by over 90 percent, if wetlands provide a large enough area for the water to flow through. Berger would like to initiate the project by building an extremely large wetlands — 2.3 square kilometers — intended to clean a central artery in the Pontine canal system.<br /><br />But the Pontine Marshes project constitutes an engineering problem with two key challenges. First, the water flowing through the wetlands must move slowly enough so that the plants can absorb the pollutants; second, the pattern of the water flow must give all water molecules equal opportunity to encounter the vegetation.<br /><br />Berger’s solution is to have the water move through an S-shaped course that slows it down to a speed well under one mile per hour. The Italian engineers of the 1930s built perfectly straight canals, since they were simply concerned with transporting water efficiently. But forcing water to meander through winding channels in a wetlands gives more water molecules the best chance of being purified. ”Inefficiency is how environmental systems work,” says Berger.<br /><br />Since Berger is not an engineer, he joined forces with Heidi Nepf, a professor in MIT’s Department of Civil and Environmental Engineering (CEE) who specializes in analyzing fluid dynamics, to test his designs. Nepf, CEE graduate student Jeff Rominger, and a Harvard graduate student, Gena Wirth, who was working for Berger’s design group, the <a href="http://www.theprex.net/" target="_blank">Project for Reclamation Excellence (P-REX)</a>, built multiple models from Berger’s plans, featuring variations of S-shapes and alterations in the density and placement of the wetlands vegetation.<br /><br />To scrutinize the designs, the researchers sent water injected with an fluorescent dye called Rhodamine WT through the models and used a fluorometer to measure the intensity of the light as the water exited, which indicated how broadly the water had spread. Test results indicated that the optimum design was one featuring relatively wide S-shaped channels with lots of vegetation underneath and small “islands” of earth to help the water disperse evenly.<br /><br />“Heidi’s and Jeff’s work gave us a scientific understanding of how these plans functioned, and allowed us to push the design envelope,” says Berger.<br /><br /><strong>Same function, new form </strong><br /><br />And while a few pollution-removing wetlands exist in the United States, Nepf says, they use identical rectangular templates plunked down regardless of the surrounding landscape’s features. “What’s novel is Alan seeing that water treatment can be made attractive,“ says Nepf. “In the wetlands treatment community, you would never go to this step.” Since the Italian countryside is a tourist attraction, she notes, eliminating stretches of concrete canals (and the need for treatment plants) “lets Italy find cultural or economic value” while improving the environment. Berger’s designs would also include park space built adjacent to the wetlands, in order to give the public better access to the countryside.<br /><br />Berger’s approach to the Pontine Marshes acknowledges “that in many cases the landscape can't be returned to its original state,” says Tom Avermaete, a professor of architecture and public building at Delft University of Technology, in the Netherlands. Instead, Avermaete believes, designers should seek new ways of mending damaged landscapes: “These processes of reclamation are one of the main challenges for future architecture and urban planning.”<br /><br />The collaboration between Berger and Nepf thus expands the way treatment wetlands can be built, by finding a new form for the function. Berger is currently part of a group filing plans in Italy this spring that could allow his designs to be implemented, which would require further government approval. Up to 102 wetlands modules of varying size could be placed in the region, he thinks, although the full extent of the project will depend on the Italian government’s ability to acquire additional land rights from some of the local property owners.<br /><br />But the fact that local officials have secured EU funding and now seem intent on implementing a natural cleanup of the marshes strikes Berger as progress in its own right. In planning, he says, “Most of the time we design what we’ve already built. But now, whatever happens in Italy, it won’t be the same old stuff.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: Mini robotic muscles]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/actuators-slideshow-0406.html</link>
<story_id>15164</story_id>
<featured>0</featured>
<description><![CDATA[Shape-memory alloys yield mechanical devices that produce more torque but weigh much less than comparably sized electric motors.]]></description>
<postDate>Tue, 06 Apr 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100401152814-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100401152814-1.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100401152814-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[The researchers developed their new actuators as part of a project to build “printable robots,” whose plastic components, like the one shown here, could be built in layers by machines that resemble ink jet printers. The metal actuator, fastened to the top of the plastic joint, is a fraction of a millimeter thick.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='351'>http://web.mit.edu/newsoffice/images/article_images/20100401153803-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[Rectangular notches carved into the shape-memory alloy increase its electrical resistance, so that only the material around the notches heats up when an electrical current passes through the actuator. The round holes are for bolts that attach the actuator to a mechanical device. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100401152814-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[Different configurations of the actuators cause different types of motions. Here, charging the actuators causes a plastic joint to fold in half.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100401152815-4.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[In early prototypes, the actuators are bolted to the devices they control. But the researchers are currently experimenting with methods for pressing the actuators into the plastic components as they harden, to simplify manufacturing.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100401152815-5.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[The ability to arrange the actuators in series and in parallel addresses a persistent problem with the use of shape-memory alloys: how to harness their high torque and low weight in larger devices.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100401152815-6.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[In a practical device, sheets of actuators would be attached to both sides of a joint like this one, to control both its opening and its closing.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100401152902-7.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[A different arrangement of the actuators is able to lift a weight of almost 17 grams. The researchers’ theoretical analysis suggests that their shape-memory actuators could exert a force 160 times their own weight.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100401152902-8.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[This device consists of five plastic rings connected by actuators.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100401152902-9.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[The actuators connecting the rings can be activated simultaneously to maximize the force they exert, or separately to exert forces in different directions.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='552'>http://web.mit.edu/newsoffice/images/article_images/20100401152903-10.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[Unencumbered, the actuator expands by roughly 60 percent of its original size. Conventional shape-memory actuators that don’t use springs can expand by roughly 3 percent their original size.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100401152903-11.jpg</fullURL>
<imageCredits><![CDATA[Photo: Jason Dorfman/CSAIL]]></imageCredits>
<imageCaption><![CDATA[Because the new actuators use less energy than their predecessors, they can be powered by batteries, a capability demonstrated by this wirelessly controlled rolling robot, which moves like the tread of a tank — but without any wheels.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<br /><br />MIT researchers have found a new way to use shape-memory alloys — metals that change shape when heated — to create small mechanical “muscles” for electronic devices. The mechanical muscles — or actuators — can produce three to six times as much torque as electric motors of similar size but weigh no more than one-20th as much. <br /><br />Previous experimental actuators have used springs made from shape-memory alloys. But the new actuator is easier to manufacture, since it can be cut out of a flat sheet of metal, and to mount, since the sheet can be bolted to a mechanical device’s moving parts. And since only a small section of the new actuator heats up when electrically charged — as opposed to the entire length of the spring — it should dissipate heat more easily and consume less energy.<br /><br />Eduardo Torres-Jara, a postdoctoral associate in the lab of Professor of Computer Science and Engineering Daniela Rus, designed the actuators and fabricated them with the help of graduate student Kyle Gilpin and research assistant Josh Karges. The researchers envision the actuators’ use in devices too small for electric motors, such as the moving parts at the tips of minimally invasive surgical devices, or the tiny cameras built into laptops.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[To starve a tumor]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/cancer-metabolism-0406.html</link>
<story_id>15175</story_id>
<featured>0</featured>
<description><![CDATA[MIT’s Matthew Vander Heiden is part of a new generation of cancer researchers trying to exploit cancer cells’ strange metabolism.]]></description>
<postDate>Tue, 06 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100406114149-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100406114149-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100406114149-0.jpg</fullURL>
<imageCredits><![CDATA[Image: National Cancer Institute]]></imageCredits>
<imageCaption><![CDATA[Cancer cells are shown here in culture from human connective tissue, illuminated by darkfield amplified contrast.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100405143131-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Matthew Vander Heiden, assistant professor of biology and member of the David H. Koch Institute for Integrative Cancer Research at MIT.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Since the 1920s, scientists have known that cancer cells generate energy differently than normal cells, a phenomenon dubbed the “Warburg effect” after its discoverer, German biochemist Otto Warburg. However, the field of cancer-cell metabolism has been largely ignored since the 1970s, when researchers flocked to study newly discovered cancer-causing genes.<br /><br />Now a new generation of researchers is setting its sights on cancer cells’ bizarre and seemingly inefficient metabolism, which appears to be tightly linked to many of the genes already implicated in cancer. <br /><br />Recent discoveries suggest that cancer cells genetically reprogram their energy-generating pathways to create the building blocks they need to grow and divide out of control, wasting a great deal of energy in the process. Potential drugs that block this pathway could offer a new way to treat a range of cancers, says Matthew Vander Heiden, assistant professor of biology and member of the David H. Koch Institute for Integrative Cancer Research at MIT.<br /><br />“All tumors have to deal with shifting metabolism as they proliferate,” says Vander Heiden, who joined the Koch Institute last fall. “If you can interfere with that, you have a chance to make a difference across many tumor types.”<br /><br />Vander Heiden is now studying one of the key enzymes involved in cancer cells’ metabolic pathways, known as pyruvate kinase M2, with an eye toward developing drugs that could alter its activity and effectively starve cancer cells of the materials they need to grow and reproduce.<br /><br /><strong>Wasting energy</strong><br /><br />Human cells use sugar as an energy source, breaking it down through a series of complex chemical reactions that requires oxygen. Warburg discovered that tumor cells switch to a less efficient metabolic strategy known as fermentation, which does not require oxygen and produces much less energy. <br /><br />Because they burn fuel so inefficiently, cancer cells take up more glucose than regular cells. Scientists have exploited this by using PET scans that track glucose metabolism as a diagnostic tool to reveal whether tumors have spread from their original location. <br /><br />After Warburg’s initial discovery, cancer researchers spent several decades, from the 1930s to the 1960s, trying to figure out the biochemistry of cancer metabolism. But in the 1970s, the discovery of cancer-causing genes (oncogenes) and tumor suppressor genes led to the “realization that cancer is a genetic disease, not a metabolic disease like diabetes or obesity,” says Lewis Cantley, professor of systems biology at Harvard Medical School.<br /><br />However, interest in cancer metabolism has surged, especially in the last few years, as evidence mounts that many of those oncogenes play a role in cancer cells’ metabolic switch. “The cancer world is full of people studying Ras [a gene that can cause cancer when mutated], p53 [a tumor-suppressor gene] and all of these different signaling and genetic things,” says Vander Heiden. “They’re all tied to metabolism in different ways.”<br /><br />The cancer metabolism resurgence started gradually in the early 1990s, when Vander Heiden was a graduate student at the University of Chicago. There, he worked with Craig Thompson, one of the pioneers of the rejuvenated field. Vander Heiden soon realized that “there are still a lot of things we didn’t understand about metabolism that seemed to have been forgotten about.”<br /><br />After earning his MD/PhD, Vander Heiden went to Cantley’s lab at Harvard, where he studied pyruvate kinase M2 (PKM2), which controls a key step in fermentation. In 2008, Vander Heiden, Cantley and others at Harvard Medical School reported in the journal Nature that when cells shift between normal and Warburg metabolism, they start using PKM2 instead of pyruvate kinase M1, the enzyme that most adult cells use for glycolysis (the first set of chemical reactions required to break down glucose for energy). <br /><br />One major benefit of potential drugs that interfere with PKM2 is that the enzyme is not found in most normal adult cells, which could eliminate the side effects commonly seen in existing cancer drugs, says Celeste Simon, professor of cell and developmental biology at the University of Pennsylvania.<br /><br />“A big problem in oncology over the past few decades is that so many of the drugs have unfortunate side effects, which limits the dosage you can administer to patients and has a major impact on quality of life,” says Simon, one of the organizers of an American Association for Cancer Research conference on cancer metabolism last fall.<br /><br />A company cofounded by Cantley is now developing potential drugs that target PKM2. (Vander Heiden is a scientific advisor for the company, Agios Pharmaceuticals.) <br /><br />However, PKM2 is not the only possible drug target, says Cantley. “One thing that a lot of people find attractive about targeting metabolic enzymes for drug therapy is that there are a lot of enzymes involved,” he says.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Crossing a threshold of particle physics]]></title>
<author><![CDATA[]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/lhc-celebration-0406.html</link>
<story_id>15174</story_id>
<featured>0</featured>
<description><![CDATA[Scientists celebrate as the Large Hadron Collider achieves its highest-energy collisions yet.]]></description>
<postDate>Tue, 06 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100405180725-1.png</thumbURL>
<smallURL width='140' height='93'>http://web.mit.edu/newsoffice/images/article_images/w140/20100405180725-1.jpg</smallURL>
<fullURL width='368' height='245'>http://web.mit.edu/newsoffice/images/article_images/20100405180725-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Michael Hoch/CERN]]></imageCredits>
<imageCaption><![CDATA[Celebration after the highest energy collisions ever created took place at the LHC, after a few decades of preparation. Pictured are the collaborators from around the world forming the operations crew in the CMS control room, including MIT post-doc Erik Butz (right foreground, in striped shirt) and graduate student Pieter Everaerts (third row back on the left, brown sweater) who were present as "experts on call" for the CMS tracking subdetector.]]></imageCaption>
</image>
<body><![CDATA[<em>The Large Hadron Collider is the world’s largest and highest-energy particle accelerator — a 17-mile loop under the Franco-Swiss border that was built by the European Organization for Nuclear Research (CERN) to help scientists answer some of the most fundamental questions in physics.Scientists fired the first beam through the collider in September 2008, but it was not until last week that that the LHC research program formally began. On Tuesday, March 30, scientists sent particles through the LHC and smashed them into one another at a total force of seven Tera electron volts (TeV) — an energy level three-and-a-half times that achieved at any other particle accelerator. <br /><br />The LHC is expected to run for the next 18 to 24 months, generating data that physicists hope will reveal insights into dark matter, new forces and new particles, including the elusive Higgs boson. Markus Klute, MIT assistant professor of physics and member of the team running one of the detectors at the LHC, describes the scene when the seven-TeV collisions began.</em><br /><br />By 5 a.m. the CMS (Compact Muon Solenoid) control center was packed with scientists monitoring and operating the CMS detector and the trigger and computing systems. Two failed attempts at 6 a.m. and 8 a.m. caused by a problem in one LHC magnet and a newly designed protection system only heightened the tension. The physicists in the detector control rooms prepared for a long day, but the third attempt to accelerate and collide protons succeeded, and first collisions were reported at 12:58 p.m. The applause and cheers lasted several minutes, and the relief was palpable. Almost instantly, the first event displays showing the collisions appeared in the control room, and minutes later first results using reconstructed data were discussed.<br /><br />As CERN Director General Rolf Heuer put it Tuesday afternoon in a press conference, “It’s a great day to be a particle physicist. A lot of people have waited a long time for this moment, but their patience and dedication is starting to pay dividends.”<br /><br />With the first high-energy collision, a new era in particle physics begins. The LHC will deliver enough collisions at seven TeV within the next two years to enable significant advances in a number of research areas. Particle physicists aren't certain what surprises the LHC will uncover, but they do know that new discoveries are hidden at the energy scale tested by the LHC experiments.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Bill Gates to visit MIT on April 21]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/gates-visit-0405.html</link>
<story_id>15173</story_id>
<featured>0</featured>
<description><![CDATA[In a presentation at Kresge Auditorium, the philanthropist will discuss the importance of service.]]></description>
<postDate>Mon, 05 Apr 2010 17:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100405094529-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100405094529-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100405094529-1.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy of the Bill &amp; Melinda Gates Foundation]]></imageCredits>
<imageCaption><![CDATA[Philanthropist and Microsoft co-founder Bill Gates]]></imageCaption>
</image>
<body><![CDATA[Philanthropist and Microsoft co-founder Bill Gates will discuss the value of service during a presentation on Wednesday, April 21, at MIT’s Kresge Auditorium. <br /><br /> <a href="http://amps-webflash.amps.ms.mit.edu/public/MIT/2009-2010/Gates/" target="_blank">Watch the archived webcast</a> <br /><br />Gates’ visit to MIT is part of a three-day tour of five universities across the country. At each campus, he will give a short talk aimed at inspiring students and teachers to focus on issues of inequity. <br /><br />Gates last spoke at MIT in March 2004, when he talked to students about the allure of computer science. It was during that same spring that MIT’s Ray and Maria Stata Center for Computer, Information, and Intelligence Sciences opened. The center, which the <a href="http://www.gatesfoundation.org/ " target="_blank">Bill &amp; Melinda Gates Foundation</a> helped fund, houses the William H. Gates Building.<br /><br />“Bill Gates brings the same dedication and enthusiasm to philanthropy that he brought to the computer industry he transformed,” said MIT President Susan Hockfield. “His deep commitment to helping solve the world’s most pressing problems will powerfully resonate with the MIT community, and I look forward to welcoming him back to campus.”<br /><br />Gates’ presentation in Kresge, “Giving Back: Finding the Best Way to Make a Difference,” will be followed by a Q&amp;A session with audience members. <br /><br />Gates co-founded Microsoft in 1975 but left his full-time duties at the software giant in 2008 to focus on running his foundation. With an endowment of $33.5 billion, the foundation works to help all people lead healthy, productive lives. In developing countries, it focuses on improving people’s health and giving them the chance to lift themselves out of hunger and extreme poverty. In the United States, it seeks to ensure that all people — especially those with the fewest resources — have access to the opportunity they need to succeed in school and life.<br /><br />Tickets for the presentation, which will run from 11:30 a.m.-12:30 p.m., will be distributed via a lottery. To register, please visit <a href="http://web.mit.edu/surveys/event/" target="_blank">http://web.mit.edu/surveys/event/</a>.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[In Profile: Missy Cummings]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/profile-cummings-0405.html</link>
<story_id>15167</story_id>
<featured>0</featured>
<description><![CDATA[Former U.S. Naval fighter pilot aims to improve how humans and computers interact.]]></description>
<postDate>Mon, 05 Apr 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100402144947-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100402144947-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100402144947-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Mary (Missy) Cummings, professor in the Department of Aeronautics and Astronautics and the Engineering Systems Division, holds a quad-rotor.]]></imageCaption>
</image>
<body><![CDATA[Mary (Missy) Cummings was exhilarated the first time she landed a fighter jet aboard an aircraft carrier in 1989, but the young pilot's elation didn't last long. Seconds later, a close friend died while attempting the same landing on the back of the carrier. <br /><br />“I can't tell you how many friends died because of bad designs,” says Cummings, recalling the crash that occurred on the U.S.S. Lexington in the Gulf of Mexico. “After spending so much time as a pilot, I found it incredibly frustrating to work with technology that didn’t work with me.”<br /><br />It wasn’t until Cummings left the Navy after 10 years and chose to pursue a PhD in systems engineering that she realized she could help improve the severely flawed designs of the technological systems she used as a pilot — from confusing radar screens and hand controls to the nonintuitive setup of cockpits — by making an impact at the research level.<br /><br />Today, she is an associate MIT professor with appointments in the Department of Aeronautics and Astronautics and in the Engineering Systems Division, and she directs the Humans and Automation Laboratory (HAL). Her work focuses on “human factors” engineering — specifically, how to develop better tools and technology to help people like pilots and air traffic controllers make good decisions in high-risk, highly automated environments. It is a critical field of research that has burgeoned in recent years with the explosion of automated technology. This has replaced the need for humans in direct manual control with the need for humans as supervisors of complex automatic control systems, such as nuclear reactors or air traffic control systems.<br /><br />But one consequence of these automated domains controlled by humans — known as “humans-in-the-loop” systems — is that the level of required cognition has moved from that of well-rehearsed skill execution and rule-following to higher, more abstract levels of knowledge synthesis, judgment and reasoning. <br /><br /><strong>A novel application</strong><br /><br />Nowhere has this change been more apparent than in the military, where pilots are increasingly being trained to operate unmanned aerial vehicles (UAVs), or drones, to perform certain cognitive tasks, such as getting a closer look at potential snipers. Prompted by the success of drones in Iraq and Afghanistan, U.S. Defense Secretary Robert Gates announced last year that UAV technology would become a permanent part of the defense budget.<br /><br />But as UAV technology becomes more prominent, Cummings wants to make it easier for humans to control portable robots in time-sensitive situations. Her goal is to lower the cognitive overhead for the user, who may not have a lot of time to change complicated menu settings or zoom and pan a camera, so that he or she can focus on more critical tasks. <br /><br />“It’s about offloading skill-based tasks so that people can focus specifically on knowledge-based tasks, such as determining whether a potential sniper is a good or  bad guy by using the UAV to identify him,” Cummings says. The technology could also help responders search more efficiently for victims after a natural disaster.<br /><br />Over the past year, Cummings and her students have designed an iPhone application that can control a small, one pound UAV called a quad-rotor — a tiny helicopter with four propellers and a camera attached to it. When the user tilts the iPhone in the direction he or she wants the UAV to move, the app sends GPS coordinates to the UAV to help it navigate an environment using built-in sensors. The UAV uses fast-scanning lasers to create rapid, electronic models of its environment and then sends these models back to the iPhone in the form of easy-to-read maps, video and photos. Although the military and Boeing are funding the research, the technology could be used for nonmilitary purposes, such as for a police force that needs a device to help monitor large crowds.<br /><br /> 





<br />The app is designed so that anyone who can operate a phone could fly a UAV: The easy-to-use design means it takes only three minutes to learn how to use the system, whereas military pilots must undergo thousands of hours of costly training to learn how to fly drones. “This is all about the mission — you just need more information from an image, and you shouldn’t have to spend $1 million to train someone to get that picture,” she says. <br /><br />The project is valuable for teaching because it represents a “classic scenario” in systems engineering in which a need is conceptualized, a system is designed to address that need and experiments are conducted to test the system, Cummings explains.<br /><br />The HAL group recently conducted experiments of the app in which participants located in one building flew the UAV inside a separate building, positioning it in front of an eye chart so they could read the images the camera captured. Some achieved the equivalent of 20/30 vision, which Cummings says is “pretty good,” pointing out that, more importantly, the device never crashed. As Cummings and her students continue to refine the technology, their next step will be experiments in the real world where the UAV could reach an altitude of 500 feet. Although the group is working with several government agencies and companies on the design, there are no plans to deploy the app just yet.<br /><br /><strong>Learning from boredom</strong><br /><br />Cummings began flying planes after graduating from the U.S. Naval Academy in 1988 and received her master’s degree in space systems engineering from the Naval Postgraduate School in 1994. When the Combat Exclusion Law was repealed in 1993, meaning that women could become fighter pilots for the first time in U.S. history, Cummings had already established herself as an accomplished pilot and was selected to be among the first group of women to fly the F/A-18 Hornet, one of the most technologically advanced fighter jets.<br /><br />Although she loved the thrill of flying, Cummings left the military when her resentful male colleagues became intolerable. “It’s no secret that the social environment wasn’t conducive to my career. Guys hated me and made life very difficult,” she recalls. Cummings details this experience in her book <em>Hornet’s Nest</em> (Writer’s Showcase Press, 2000).<br /><br />But what is most enduring about Cummings’ military experience is that it fueled her desire to improve how humans and computers can work together in complex technical systems. She focuses on how design principles, such as display screens, can affect supervisory control factors, such as attention span, when humans operate complex systems.<br /><br />“In order to build a $1 billion air traffic control system, you can’t just do it by rule-of-thumb, you need to use models that take into account human factors, such as that people get bored by advanced automation systems,” Cummings says. The HAL group is currently developing models of human boredom to help design systems that prevent the people who monitor gauges and dials in automated systems from becoming bored, which is what happened last fall when two Northwest Airlines pilots overshot their destination by 150 miles because they weren’t paying attention. <br /><br />In addition to boredom, other human factors that Cummings studies are what kinds of strategies people use to carry out certain tasks, whether they feel confident or frustrated when using a technology and the level of trust they have toward a system. These factors are tested through a variety of methods, such as analyzing eye movement, the number of times someone clicks or scrolls a mouse, or whether the user responds to alert messages. The iPhone project aims to analyze cognitive workload, and how easily the users are able to carry out certain cognitive tasks. <br /><br />Yossi Sheffi, director of MIT’s Engineering Systems Division, thinks Cummings’ work is extremely important because technology by itself cannot be the answer when designing large-scale systems. “Her research tying the human operator to technology is crucial — both to the design of the technology itself, but also to the operation of the system as a whole, in order to ensure that it operates efficiently and effectively,” he says.<br /><br />But the work also matters to civilians operating increasingly complex small-scale systems like cell phones and remote controls, according to MIT Professor of Computer Science and Engineering Randall Davis, who has worked with Cummings on several projects and praises her understanding of how people process information. “As we are increasingly surrounded by technology of all sorts, it becomes increasingly important for someone to understand how to design this stuff so that it’s easy to use; otherwise, we’ll be surrounded by incomprehensible technology,” he says.<br /><br />Cummings’ ability to infuse the quantitative methods of hard science with the human behavior models of soft science like psychology is what makes her unique, according to HAL Research Specialist Dave Pitman, a former graduate student of Cummings who is working on the iPhone project. “She has a very good understanding of both sides of the coin, and because her background is in hard science, she tries to bring this rigor more into the soft sciences, which allows for better research,” he explains.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[‘Failure is good’]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/johnson-bankers-0405.html</link>
<story_id>15168</story_id>
<featured>0</featured>
<description><![CDATA[In a talk to promote his new book, MIT’s Simon Johnson lambastes a finance industry he sees as lacking a healthy fear of losing money.]]></description>
<postDate>Mon, 05 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100406095930-2.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100406095930-2.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100406095930-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Justin Knight]]></imageCredits>
<imageCaption><![CDATA[Simon Johnson, the Ronald A. Kurtz (1954) Professor of Entrepreneurship at the MIT Sloan School of Management, discusses his new book, "13 Bankers," at an event Friday.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='605'>http://web.mit.edu/newsoffice/images/article_images/20100406095929-0.jpg</fullURL>
<imageCredits><![CDATA[Image: Knopf Doubleday]]></imageCredits>
</image>
</otherImages>
<body><![CDATA[“Greed is good” became a popular phrase in the aftermath of the 1987 movie Wall Street, implying that the riches of individual financiers were justified by the role those people played initiating large-scale wealth creation throughout society.<br /><br />Two decades and several financial meltdowns later, that slogan could probably use an update, something gamely attempted by Simon Johnson in a talk on campus Friday, supporting his new book about the financial crisis, <em>13 Bankers</em> (Knopf Doubleday, 2010).<br /><br />“Failure is good,” said Johnson, the Ronald A. Kurtz (1954) Professor of Entrepreneurship at the MIT Sloan School of Management. “This country is based on our ability to take risks, to create new things — and our ability to fail, individually and in a corporate setting. If you become too big to fail, it’s no longer a market economy.”<br /><br />Over the last 18 months, Johnson has become one of America’s most prominent critics of both the banking industry and what he sees as an ineffectual government effort to straighten out those banks in the wake of the financial crisis September 2008. As Johnson sees it, the whole notion that financial institutions can become “too big to fail” is a distortion of basic market incentives; instead, risks and rewards must co-exist so that banks, rather receiving bailouts after losing billions, will have motivation to perform well. <br /><br />“Ask yourself this,” said Johnson, speaking to a packed house at Wong Auditorium. “If you had a lifetime exemption from speeding tickets, would you speed? You might say, ‘No, no, I’m a responsible driver, I’m really very careful all the time.’ But … who in this room would be able to withstand the temptation of speeding under those circumstances? That’s how it is in banking.”<br /><br />To be sure, in <em>13 Bankers</em>, Johnson and his co-author James Kwak allow that government intervention in September 2008 helped prevent a larger economic disaster. But Johnson and Kwak believe that as the ill health of the banks lingered, the Obama administration did not extract enough reforms in return for the hundreds of billions of dollars it made available to financial firms. <br /><br />Additionally, Johnson believes finance does not help the overall economy as much the industry’s leaders claim. In 2003, he noted, the financial sector accounted for 40 percent of corporate profits in the United States, an all-time high. “That’s extraordinary,” said Johnson. “That’s incredible. That’s ridiculous.” <br /><br />As Johnson and Kwak point out in <em>13 Bankers</em>, financial-sector profits grew six-fold from 1980 through 2009, whereas the non-financial industry profits only doubled during the same time. If Wall Street were truly driving overall growth, those rates of increase would be more similar. “There is no evidence that this process has benefited the broader, non-financial economy,” Johnson said on Friday. <br /><strong><br />Is too big really too bad?</strong><br /><br />As a remedy for what they see as oversize banks, Johnson and Kwak would like to limit how much money banks can lend to each other. Still, the authors acknowledge that more active oversight will also be required.<br /><br />“We’re not saying fixing bank size is sufficient,” Johnson said. “But it is necessary.” <br /><br />That issue was emphasized in a lively question-and-answer session. MIT Sloan Professor Erik Brynjolfsson, for one, asked Johnson and Kwak whether a system of more numerous but smaller banks wouldn’t be prone to the same kinds of bubbles and meltdowns. <br /><br />Kwak, who joined Johnson onstage to answer questions, said in response, “It’s certainly theoretically possible that if you had 10 small Goldman Sachses, they could all adopt the exact same strategy, but … if you have competing banks, their strategies are less likely to be completely correlated.”<br /><br />In turn, Johnson suggested that a critical problem with large financial institutions is not just economic, but political: Bigger banks hold more influence in Washington, reducing government latitude for action. Johnson, the former chief economist of the International Monetary Fund, calls this industry “capture of the state.” <br /><br />As for the future of financial regulation, Johnson doubts the current bills in Congress will become law (even though, Johnson noted, Senate Banking Committee Chairman Chris Dodd, a Democrat from Connecticut, has been reading <em>13 Bankers</em> this week). <br /><br />“I don’t think this will get fixed in this legislative cycle,” Johnson said. His campaign critiquing the banks “is about changing the consensus,” he added, concluding: “Without that, we will not make any progress in this country.”<br /><br /> 






<br /><br /><strong>Video from Simon Johnson's book talk</strong><br /><a href="http://mitworld.mit.edu" target="_blank">From MIT World</a><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: David MacKay on renewable energy]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/3q-mackay-0405.html</link>
<story_id>15169</story_id>
<featured>0</featured>
<description><![CDATA[Science advisor to the British government talks about Climategate, the Copenhagen conference, and emission-free energy technology]]></description>
<postDate>Mon, 05 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100402151028-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100402151028-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100402151028-1.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy David MacKay]]></imageCredits>
<imageCaption><![CDATA[David MacKay]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='526'>http://web.mit.edu/newsoffice/images/article_images/20100402151156-2.jpg</fullURL>
<imageCredits><![CDATA[Graphic courtesy David MacKay]]></imageCredits>
</image>
</otherImages>
<body><![CDATA[<em>David MacKay (pronounced mac-EYE), a professor of physics at the University of Cambridge, UK, was recently appointed to a three-year term as chief scientific advisor to the UK’s Department of Energy and Climate Change. In an April 1 talk at MIT, he described what specifically would be required to shift the world’s energy use entirely to non-carbon emitting sources. </em><br /><br /><strong>Q.</strong> How much of an effect do you expect the so-called “Climate-Gate” episode to have on public attitudes to climate change, and do you think this will have any long-term repercussions?<br /><br /><strong>A.</strong> It clearly had some impact, and my feeling is that it doesn’t deserve to have had impact. I think the impression that the public has gained of what happened is entirely based on mudslinging and invention. People allege that there was deliberate manipulation of data, and I’m not aware of any evidence that that’s so. In fact, a recent House of Commons committee has just found that although they’re not completely happy with everything, but they say the reputations of the scientists involved is intact. So I guess what’s needed for the long term is a clear retelling and proper exposition of the science so that the conclusions of the public will be more robust in the future.<br /><br /><strong>Q.</strong> What do you think the prospects are for reaching any significant international agreement on greenhouse emissions, in the post-Copenhagen world, and do you see the Copenhagen process as having made such agreements more likely?<br /><br /><strong>A.</strong> I think there was some progress made at Copenhagen. I think it was the first time an international agreement mentioned aiming at a target of 2 degrees warming or less. Also there was a move toward really substantial international transfers of money. So I don’t think Copenhagen was all bad news. I can’t predict the future, and I don’t know what’s going to happen. I’m still an optimist. I feel that there is an international consensus that there is a problem and something needs to be done. The difficulty is the notion of equity, of fairness, of what should different countries do. And it’s human nature to try to get away with doing as little as possible, so that’s what makes it difficult. But the good news is that there’s a general agreement that international action is what’s needed.<br /><br /><strong>Q.</strong> What do you see as the most under-appreciated potential technology or policy that could help the world avoid catastrophic climate change?<br /><br /><strong>A.</strong> Well, the one that I’m obsessing about at the moment is seasonal heat storage. In Britain, our demand for energy peaks during the coldest weeks in winter, and it can be twice as big as our energy consumption at other times of the year. In parts of America, demand peaks in the summer, during the hottest periods. So if we could develop low-cost technologies for moving heat in time, from the winter to the summer and vice versa, I could see that making quite a difference to the scale of new energy sources that may be needed in the future.<br /><br />My impression is the Dutch are currently the leaders in the creative deployment of heat storage methods. They store heat underneath roads in the summer and then pump it out again in the winter. And in my talk I mentioned a community in Canada called Drake’s Landing, where they have a community heat storage which collects heat from solar hot water panels, and stores it in a huge hole in the ground, and then they can pump it back out in winter, and it provides heat for quite a few dwellings. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Paul Samuelson memorial to be held on April 10]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/samuelson-memorial.html</link>
<story_id>15166</story_id>
<featured>0</featured>
<description><![CDATA[Event in Kresge Auditorium will celebrate the Nobel laureate’s life and legacy.]]></description>
<postDate>Fri, 02 Apr 2010 16:04:27 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100402120650-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100402120650-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100402120650-1.jpg</fullURL>
<imageCaption><![CDATA[Paul Samuelson]]></imageCaption>
</image>
<body><![CDATA[Family, friends and colleagues of the late Paul Samuelson will celebrate the MIT economist’s life and legacy during a memorial service at 11 a.m. on Saturday, April 10, in <a href="http://whereis.mit.edu/?go=W16" target="_blank">Kresge Auditorium</a>.<br /><br />Samuelson, the Nobel laureate whose mathematical analysis provided the foundation on which modern economics is built and whose textbook influenced generations of students, died in December at the age of 94.<br /><br />MIT Institute Professor Emeritus Robert Solow will emcee the event, which will feature remarks by MIT President Susan Hockfield; Ricardo Caballero, the Ford International Professor of Economics and head of MIT’s Department of Economics; Samuelson’s son, William; Lawrence Summers ’75, the director of U.S. President Barack Obama's National Economics Council who formerly served as U.S. Treasury secretary and Harvard University president; Stanley Fischer PhD ’69, an emeritus member of the MIT faculty and governor of the Bank of Israel; Nobel laureate and former MIT Professor Paul Krugman PhD ’77; James Poterba, the Mitsui Professor of Economics and former head of MIT’s Department of Economics; and commodities trader Helmut Weymar ’58, PhD ’65.<br /><br />A reception will follow the service in Kresge lobby until 2 p.m. Parking will be available at the West Annex Lot.<br /><br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slideshow: A Terrascope spring break]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/terrascope-slideshow.html</link>
<story_id>15160</story_id>
<featured>0</featured>
<description><![CDATA[Students visit Abu Dhabi to learn about plans for the world’s first carbon-neutral city]]></description>
<postDate>Fri, 02 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100331153952-12.png</thumbURL>
<smallURL width='140' height='92'>http://web.mit.edu/newsoffice/images/article_images/w140/20100331153952-12.jpg</smallURL>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100331153952-12.jpg</fullURL>
<imageCredits><![CDATA[Photo: Vicki McKenna]]></imageCredits>
<imageCaption><![CDATA[During the trip, the group visited a coastal sabkha, which is a flat, salt-encrusted desert that may sit atop oil and gas reserves. Here, Stephen Lokier, a professor at the Petroleum Institute in Abu Dhabi, explains the composition of each layer of the sediment, which is a mixture of carbonate sands, muds and minerals.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100331152701-8.jpg</fullURL>
<imageCredits><![CDATA[Photo: Vicki McKenna]]></imageCredits>
<imageCaption><![CDATA[A group of 58 MIT students, alumni and staff spent five days over spring break in Abu Dhabi as part of the annual Terrascope program for first-year students. Each year, students in the popular class are tasked with solving a very complex problem. This year’s mission is the global problem of rising levels of carbon dioxide and other greenhouse gases. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100331153952-9.jpg</fullURL>
<imageCredits><![CDATA[Photo: Vicki McKenna]]></imageCredits>
<imageCaption><![CDATA[The Masdar Institute of Science and Technology hosted the Terrascope group’s visit. Masdar is the Middle East’s first graduate research institution dedicated to alternative energy, environmental technologies and sustainability. This view shows a library, laboratory cluster, living quarters and utility/transportation space.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100331153952-10.jpg</fullURL>
<imageCredits><![CDATA[Photo: Vicki McKenna]]></imageCredits>
<imageCaption><![CDATA[Students view the planned development of Saadiyat Island, a project in Abu Dhabi that includes a cultural district, residential areas, recreation areas and a business district.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100331153952-11.jpg</fullURL>
<imageCredits><![CDATA[Photo: Vicki McKenna]]></imageCredits>
<imageCaption><![CDATA[The group toured greenhouses where initial experiments are growing plants under varying conditions of salinity, watering frequency and fertilizer. Conditions that prove beneficial in the greenhouse are then further tested in a protected outdoor area over a longer term.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='490'>http://web.mit.edu/newsoffice/images/article_images/20100331154110-6.jpg</fullURL>
<imageCredits><![CDATA[Photo: Lauren Kuntz]]></imageCredits>
<imageCaption><![CDATA[Known as the Beam Down Project, this small, experimental solar farm at the Masdar Institute has ground solar collectors that reflect light to the central, aerial reflector. That reflector then directs the beams to a ceramic plate located at ground level that measures the amount of energy generated from the sunlight. ]]></imageCaption>
</image>
<image>
<fullURL width='368' height='612'>http://web.mit.edu/newsoffice/images/article_images/20100331154110-7.jpg</fullURL>
<imageCredits><![CDATA[Photo: Vicki McKenna]]></imageCredits>
<imageCaption><![CDATA[The group also spent time at Bastakiya, one of the oldest historic sites in Dubai. The wind towers seen here are some of the last in existence and were built in the 1890s by wealthy textile and pearl traders from Iran. Designed to funnel cool air into building interiors, they were essentially old-fashioned air conditioners.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100331154110-8.jpg</fullURL>
<imageCredits><![CDATA[Photo: Lauren Kuntz]]></imageCredits>
<imageCaption><![CDATA[Part of the group’s guided tour of Abu Dhabi included a desert safari. Here they climb up one of many sand dunes.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<br /><br />A group of 58 MIT students, alumni and staff recently returned from a five-day trip to Abu Dhabi to study the practical application of technology for renewable energy solutions.<br /><br />The trip was part of Terrascope, a yearlong program for first-year students that begins each fall with a mission to solve an “unsolvable” or very complex problem. The students spend the fall semester designing a viable solution and then travel during spring break to a location related to the complex problem where they experience the reality of the solutions they have proposed.<br /><br />This year’s mission is the global problem of rising levels of carbon dioxide and other greenhouse gases. During the fall, the class explored ways to reduce global output, as well as methods for carbon capture and sequestration. They developed a plan for the coming decades and presented their solution on a <a href="http://web.mit.edu/12.000/www/m2013/" target="_blank">web site</a>.<br /><br />In Abu Dhabi, the MIT group learned about Masdar City, a planned 50,000-person metropolis that will have no carbon footprint. The trip, which was supported by the <a href="http://www.massiah.com/" target="_blank">Massiah Foundation</a>, also included visits to various pilot sites currently testing carbon sequestration techniques.<br /><br />Sam Bowring, Terrascope director and professor of earth, atmospheric, and planetary sciences, said the trip gave his students a chance to see firsthand “the challenges of working with multiple global partners to develop commercial-scale, sustainable energy solutions.”<br /><br />The visit was hosted by faculty and students from the <a href="http://www.masdar.ac.ae/" target="_blank">Masdar Institute of Science and Technology</a>, the Middle East’s first graduate research institution dedicated to alternative energy, environmental technologies and sustainability. MIT’s Technology and Development Program is providing assistance and scholarly assessment in the development of the Institute.<br /><br />Read more about the visit at the <a href="http://mission2013trip.typepad.com/blog/" target="_blank">students’ blog</a>.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[A step toward lighter batteries]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/liair-batteries-0402.html</link>
<story_id>15163</story_id>
<featured>0</featured>
<description><![CDATA[Research shows metal catalysts play important role in improving the efficiency of lithium-oxygen batteries]]></description>
<postDate>Fri, 02 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100401144503-0.png</thumbURL>
<smallURL width='140' height='115'>http://web.mit.edu/newsoffice/images/article_images/w140/20100401144503-0.jpg</smallURL>
<fullURL width='368' height='302'>http://web.mit.edu/newsoffice/images/article_images/20100401144503-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Doctoral student Yi-Chun Lu holds an experimental lithium-air battery that was used for testing at MIT.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100401144252-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Test battery has inlet and outlet on the sides to provide a flow of air, providing oxygen for the battery’s operation.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Lightweight batteries that can deliver lots of energy are crucial for a variety of applications — for example, improving the range of electric cars. For that reason, even modest increases in a battery’s energy-density rating — a measure of the amount of energy that can be delivered for a given weight — are important advances. Now a team of researchers at MIT has made significant progress on a technology that could lead to batteries with up to three times the energy density of any battery that currently exists. <br /><br />Yang Shao-Horn, an MIT associate professor of mechanical engineering and materials science and engineering, says that many groups have been pursuing work on lithium-air batteries, a technology that has great potential for achieving great gains in energy density. But there has been a lack of understanding of what kinds of electrode materials could promote the electrochemical reactions that take place in these batteries.<br /><br />Lithium-oxygen (also known as lithium-air) batteries are similar in principle to the lithium-ion batteries that now dominate the field of portable electronics and are a leading contender for electric vehicles. But because lithium-air batteries use lightweight porous carbon electrodes and oxygen drawn from a flow of air to take the place of heavy solid compounds used in lithium-ion batteries, the batteries themselves can be much lighter. That’s why leading companies, including IBM and General Motors, have committed to major research initiatives on lithium-air technology.<br /><br />In a paper <a href="http://scitation.aip.org/journals/doc/ESLEF6-ft/vol_13/iss_6/A69_1.html" target="_blank">published this week</a> in the journal <em>Electrochemical and Solid-State Letters</em>, Shao-Horn, along with some of her students and visiting professor Hubert Gasteiger, reported on a study showing that electrodes with gold or platinum as a catalyst show a much higher level of activity and thus a higher efficiency than simple carbon electrodes in these batteries. In addition, this new work sets the stage for further research that could lead to even better electrode materials, perhaps alloys of gold and platinum or other metals, or metallic oxides, and to less expensive alternatives.<br /><br />Doctoral student Yi-Chun Lu, lead author of the paper, explains that this team has developed a method for analyzing the activity of different catalysts in the batteries, and now they can build on this research to study a variety of possible materials. “We’ll look at different materials, and look at the trends,” she says. “Such research could allow us to identify the physical parameters that govern the catalyst activity. Ultimately, we will be able to predict the catalyst behaviors. ”<br /><br />One issue to be dealt with in developing a battery system that could be widely commercialized is safety. Lithium in metallic form, which is used in lithium-air batteries, is highly reactive in the presence of even minuscule amounts of water. This is not an issue in current lithium-ion batteries because carbon-based materials are used for the negative electrode. Shao-Horn says the same battery principle can be applied without the need to use metallic lithium; graphite or some other more stable negative electrode materials could be used instead, she says, leading to a safer system.<br /><br />A number of issues must be addressed before lithium-air batteries can become a practical commercial product, she says. The biggest issue is developing a system that keeps its power through a sufficient number of charging and discharging cycles for it to be useful in vehicles or electronic devices. <br /><br />Researchers also need to look into details of the chemistry of the charging and discharging processes, to see what compounds are produced and where, and how they react with other compounds in the system. “We’re at the very beginning” of understanding exactly how these reactions occur, Shao-Horn says.<br /><br />Gholam-Abbas Nazri, a researcher at the GM Research &amp; Development Center in Michigan, calls this research “interesting and important,” and says this addresses a significant bottleneck in the development of this technology: the need find an efficient catalyst. This work is “in the right direction for further understanding of the role of catalysts,” and it “may significantly contribute to the further understanding and future development of lithium-air systems,” he says. <br /><br />While some companies working on lithium-air batteries have said they see it as a 10-year development program, Shao-Horn says it is too early to predict how long it may take to reach commercialization. “It’s a very promising area, but there are many science and engineering challenges to be overcome,” she says. “If it truly demonstrates two to three times the energy density” of today’s lithium-ion batteries, she says, the likely first applications will be in portable electronics such as computers and cell phones, which are high-value items, and only later would be applied to vehicles once the costs are reduced.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Fill out your census form and be counted!]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/census-ia.html</link>
<story_id>15157</story_id>
<featured>0</featured>
<description><![CDATA[An important message to members of the MIT community who live on campus]]></description>
<postDate>Thu, 01 Apr 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100331105149-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100331105149-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100331105149-1.jpg</fullURL>
<imageCredits><![CDATA[U.S. Census Bureau, Public Information Office ]]></imageCredits>
<imageCaption><![CDATA[If you live on the MIT campus in Institute-approved housing (which includes all residence halls, fraternities, sororities and independent living groups), then you need to be counted by filling out a census form.]]></imageCaption>
</image>
<body><![CDATA[April 1, 2010, is National Census Day. The census, a count of all people who currently reside in the United States, takes place every 10 years and is a vital exercise for the nation. The confidential data collected by the U.S. Census Bureau are used for purposes as wide-ranging as the allocation of funding for transportation services, public safety and road repair; the determination of the number of seats in the House of Representatives; planning for emergency preparedness; and a host of other public priorities that impact every person in the United States.<br /><br />MIT cares about the collection of census data. Research efforts in sociology, economics, political science, urban studies and many other disciplines rely on the accurate compilation of information about the residents of our country. Census data are also used to determine funding for critical tuition grant and loan programs.<br /><br />Like every college and university, MIT is required by the federal government to count each person who resides in Institute-approved housing on our campus. That includes undergraduate and graduate students, housemasters, resident advisors, graduate resident tutors, post-doctoral scholars and any other visiting scientists, scholars or faculty members — more than 6,000 people in all. <br /><br />If you live on the MIT campus in Institute-approved housing (which includes all residence halls, fraternities, sororities and independent living groups), then you need to be counted by filling out a census form.<br /><br />It doesn’t matter if you are a resident of another state, a citizen of another country or registered to vote in your hometown, you need to be counted here on campus.<br /><br />All you have to do is fill out the brief questionnaire that will be provided to you by your house manager or resident advisor. The form asks for basic personal information and takes just a few minutes to complete and return.<br /><br />Your responses to the census questions will not be shared with anyone. All data collected by the U.S. Census are confidential, and according to federal law cannot be distributed to another governmental agency or organization of any kind.<br /><br />Please take a look at MIT’s Census 2010 web site <a href="http://web.mit.edu/census/" target="_blank">http://web.mit.edu/census/</a> to learn more about the census effort at MIT. If you have any questions, kindly address them to <a href="mailto:census2010@mit.edu" target="_blank">census2010@mit.edu</a>. Thank you for your part in ensuring the MIT provides a full and accurate count to the U.S. Census Bureau!<br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Melissa Nobles on the U.S. Census]]></title>
<author><![CDATA[]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/3q-nobles-0401.html</link>
<story_id>15159</story_id>
<featured>0</featured>
<description><![CDATA[As America’s decennial headcount gets under way, an MIT political scientist discusses the history of race and ethnicity in the U.S. Census.]]></description>
<postDate>Thu, 01 Apr 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100331114559-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100331114559-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100331114559-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Associate Professor of Political Science Melissa Nobles]]></imageCaption>
</image>
<body><![CDATA[<em>April 1 marks National Census Day, the official date of this year’s U.S. Census. To help put the census in context, MIT News spoke with Associate Professor of Political Science Melissa Nobles, whose teaching and research interests span the comparative study of racial and ethnic politics, and issues of retrospective justice. Her book, </em><a href="http://www.sup.org/book.cgi?id=651" target="_blank">Shades of Citizenship: Race and the Census in Modern Politics</a><em> (Stanford University Press, 2000), examined the political origins and consequences of racial categorization in demographic censuses in the United States and Brazil.</em><br /><br /><strong>Q.</strong> You’ve noted in your book that the initial impetus for census-taking was political, and yet the earliest censuses also included racial categories. Why are race and ethnicity included in the U.S. Census?<br /><br /><strong>A.</strong> Census-taking in the U.S. is as old as the Republic. The U.S. Constitution mandates that an “actual enumeration” be conducted every 10 years to allow for representational apportionment. The initial impetus for census taking was political. Yet the earliest censuses also included racial categories. The inclusion of these categories offers important insights into the centrality of racial and ethnic identifications in American political, economic and social life. This centrality continues to this day.<br /><br />So, if the initial reason for census-taking was determining apportionment for representation, why did the earliest censuses include race? Representation depended on civil status — whether a person was free or a slave — and not on racial status. There were free colored persons, after all. Yet racial identification was combined with civil status in the census because race was a salient political and social marker. Censuses from the years 1790 through 1840 asked few questions beyond those related to population.  <br /><br />These censuses variously counted free white males and free white females, subdivided into age groups; slaves; and all other free (colored) persons, except Indians not taxed. The 1840 and 1850 censuses were directly intertwined with debates about slavery. Data from the embattled, and largely discredited, 1840 census purportedly disclosed higher rates of insanity among free blacks, thereby proving that freedom drove free black people crazy. The 1850 census first introduced the category “mulatto,” at the behest of a southern physician, in order to gather data about the presumed deleterious effects of “racial mixture.” Post-Civil War censuses, which continued to include the “mulatto” category, reflected the enduring preoccupation with “racial mixing.” The introduction of “Chinese” and “Japanese” in the 1870 and 1890 censuses, respectively, also reflected growing concerns about Japanese and Chinese immigration.<br /><br /><strong>Q.</strong> What are the census data on race and ethnicity used for today?<br /><br /><strong>A.</strong> Twentieth-century racial and ethnic census categorization remained intertwined with the century’s core political and social issues: de jure and de facto racial segregation, the eventual establishment of legal equality, and immigration. In regards to segregation, categories and instructions for the censuses from 1930 to 1950 largely mirrored the racial status quo in politics and law. Southern laws defined persons with any trace of “Negro blood” as legally “Negro” and subject to all of the political, economic and social disabilities such designation conferred. Southern law treated other “non-white” persons similarly. Census categories and definitions followed suit, essentially bringing the logic of racial segregation into national census-taking itself. The early 20th century was also a period of tremendous demographic change. Census-taking too sought to register these changes, with the addition of several new categories. <br /> <br />The Civil Rights movement and resulting Civil Rights legislation dramatically changed the political context and purposes of racial and ethnic categorization. Today, racial and ethnic categorization supports civil rights and minority representation. Categorization today also seeks to capture more reliably our country’s growing diversity and the self-identification of citizens. Indeed, for the first time in American census history, the 2000 census allowed respondents to check more than racial category. Moreover, prior to the introduction of self-identification in the 1960 census, enumerators determined the person’s race by visual observation, based on the definitions provided in official instructions. <br /><br />The history of race and ethnicity in the census is complicated, and at times deeply discomforting, but revealing. It clearly shows that categorization cannot be separated from its larger political and social context. Fortunately, census-taking in the 21st century reflects our country’s tremendous political and social progress. <br /><br /><strong>Q.</strong> This year’s census form is much shorter than previous versions. Why is that, and does this mean that the data will be less complete/useful than in previous years?<br /><br /><strong>A. </strong>Yes, the 2010 census form is much shorter than in previous years. The Census Bureau has introduced the American Community Survey (ACS), which basically asks the same questions that past censuses have. The Census Bureau will send the longer ACS to a random sample of addresses and send the shorter census form to all addresses. Most importantly, the ACS is conducted every year, rather than once every 10 years, making it quite useful to researchers, government and the general public because it provides the most current information. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Charting a course toward cleaner cars]]></title>
<author><![CDATA[Nancy Stauffer, MIT Energy Initiative]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/car-report-0401.html</link>
<story_id>15161</story_id>
<featured>0</featured>
<description><![CDATA[MIT team recommends strategy for reducing automotive fuel use, emissions]]></description>
<postDate>Thu, 01 Apr 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100331155725-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100331155725-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100331155725-1.jpg</fullURL>
</image>
<body><![CDATA[The emissions from cars and light trucks account for 16 percent of the total greenhouse gas (GHG) emissions in the United States, and these vehicles use 47 percent of all the petroleum consumed in this country. Without strong action, those numbers are expected to keep rising, but reducing the nation’s impact on global climate change and dependence on oil imports has presented a daunting task. Now, a new MIT report outlines a set of policies that could accomplish that goal in the next few decades. <br /><br />The recommended policies would require manufacturers to make more fuel-efficient cars, and encourage consumers to buy them and then drive them in a fuel-efficient manner. Meanwhile, the report calls for the nation to develop a comprehensive strategy on fuels, setting long-term targets that account for the life-cycle emissions as well as production, distribution and vehicle requirements for each possible fuel.<br /><br />“If we’re serious about reducing petroleum consumption and GHG emissions, we need to look at the whole system — at everyone who makes, buys, and uses vehicles and their associated fuels,” says John Heywood, professor of mechanical engineering, emeritus. “All the pieces are interrelated, and we need them to work together. For example, tighter regulations can push industry toward higher fuel economy, but then we need to create incentives for consumers to buy those cars, which may be smaller, lighter and more expensive than they’re used to.”<br /><br />For policymakers in Washington, taking a systems view is difficult because of the politics involved. Interest groups are constantly diverging into separate camps and lobbying for separate policies. <br /><br />To help demonstrate how a systems approach could work, Heywood turned to 10 of his graduate students in the Sloan Automotive Laboratory who study different aspects of the transportation problem — from technology and fuels options to consumer behavior to the impacts of specific policies. Heywood issued this group a challenge: come up with a sensible, effective and realistic policy portfolio. <br /><br />Guided by Heywood and feedback from several outside experts, graduate students Valerie Karplus and Donald MacKenzie of MIT’s Engineering Systems Division integrated the group’s ideas into a report called “An Action Plan for Cars.” The study was supported in part by the MIT Energy Initiative.<br /><br />The first group of policies aims to reduce the fuel consumption of new vehicles. The report recommends that the U.S.:<br /> 
<ul>
<li>Continue to tighten the Corporate Average Fuel Economy (CAFE) standards for the fuel economy of new cars—well beyond the current 2016 target of 34.1 mpg.</li>
<li>Establish a “feebate” system under which buyers of new cars would get a rebate if they choose fuel-efficient models, or pay a fee if they go for gas-guzzlers.</li>
<li>Increase taxes on motor fuels by 10 cents per gallon each year for at least the next 10 years.</li>
</ul>
To make rising fuel taxes more palatable, drivers would see reductions on their income taxes or payroll taxes. But some of the revenue would be used to improve the U.S. transportation infrastructure. “Our roads and bridges are in real need of maintenance and improvement,” says Heywood. <br /><br />Other policies would help consumers buy and drive more wisely by giving them more information, including a clear presentation on fuel-economy stickers of miles per gallon for both highway and city driving. In addition, the report suggests teaching people better driving behaviors: Driving at a steady speed and avoiding rapid braking and accelerating can reduce fuel use by 10 percent or more compared with more aggressive driving. <br /><br />As for fuels, Heywood notes that current initiatives, laws and requirements are “piecemeal.” The MIT team recommends that all transportation fuels should be evaluated on the basis of their full life-cycle GHG emissions. “While that may seem obvious, the devil is in the details on this one,” says Karplus. For example, growing biofuels in place of food crops in Iowa may push up food imports from Brazil, where the need for added agricultural land could lead to destruction of rainforest, and thus loss of an important absorber of atmospheric carbon dioxide. And although electric vehicles emit no tailpipe emissions, recharging their batteries may increase electricity generation from GHG-emitting power plants. <br /><br />The report suggests a high-level strategy to identify fuels and technologies that can best contribute to displacing petroleum and reducing GHG emissions. That policy should address the need for developing fuel production capacity, distribution infrastructure and compatible vehicles at the same time. <br /><br />Heywood says the report is meant to demonstrate a set of integrated policies that can help us “define where we want to get to, decide whether a given path shows potential for getting us there and then plan so that we can both get moving and get wiser.”<br /><br />David Friedman, research director of the Clean Vehicles Program for the Union of Concerned Scientists, says that "American transportation and energy policy has suffered from a fundamental flaw: we haven't had one. Instead we've had a policy of either ignoring the problems or casting about for silver bullets and magic wands to solve them. The MIT team takes on this flaw with their outline of a comprehensive approach” to the problems. “Many of the recommendations,” he says, “deserve significant attention from the president and leaders in Congress who are fighting to create new jobs and a clean energy future."<br /><br />Karplus adds: “We aren’t necessarily saying to adopt this set of policies exactly as written, but we’re demonstrating how Washington can think about policies in a way that considers how they might best work together.”<br /><br /><em>Additional reporting by David L. Chandler</em>, <em>MIT News Office</em><br /><br />]]></body>
</item>
<item>
<title><![CDATA[A manufacturing renaissance for America?]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/future-manufacture-0331.html</link>
<story_id>15155</story_id>
<featured>0</featured>
<description><![CDATA[At an MIT forum, experts examine new ways to pursue a good old idea: making things.]]></description>
<postDate>Wed, 31 Mar 2010 20:15:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100330163823-3.png</thumbURL>
<smallURL width='140' height='126'>http://web.mit.edu/newsoffice/images/article_images/w140/20100330163823-3.jpg</smallURL>
<fullURL width='368' height='331'>http://web.mit.edu/newsoffice/images/article_images/20100330163823-3.jpg</fullURL>
<imageCaption><![CDATA[Pills are fed through a sorter in a pharmaceutical manufacturing facility. New manufacturing methods in industries such as drug production may be a key to reviving the United States economy.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100330161334-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Christine Ortiz, an associate professor in MIT’s Department of Materials Science and Engineering, discusses “bio-inspired” synthetic materials at an MIT forum, “The Future of Manufacturing Innovation — Advanced Technologies," held on Monday.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100330161334-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Richard Roth, a research associate in MIT’s Engineering Systems Division, discusses the idea of manufacturing lightweight automobiles at the forum.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Over the last few decades, the sector of the U.S. economy devoted to manufacturing has lost ground to the services sector. The number of U.S. manufacturing jobs has declined from nearly 20 million in 1979 to about 12 million today. Yet as the recent global recession suggests, services can propel the economy only so far. There is no substitute for making tangible, useful products. <br /><br />But what form will new kinds of manufacturing take? At an MIT roundtable discussion on Monday titled “The Future of Manufacturing — Advanced Technologies,” more than a dozen of the Institute’s faculty shared converging ideas about how to reinvigorate America’s goods-producing businesses. The roundtable followed a broader campus forum hosted by MIT President Susan Hockfield on March 1, in which faculty members, some of whom also participated in Monday’s discussion, offered ideas about how to strengthen America innovation and thus its overall economy. These meetings are part of a larger effort by MIT to contribute the Institute’s expertise in emerging technologies and innovation policies to the national effort to revitalize the American economy.<br /><br />Monday’s discussion cast specific issues of manufacturing in the light of broad economic considerations. “To recover from the current economic downturn, it has been estimated that we need to create on the order of 17 million to 20 million new jobs in the coming decade,” noted Hockfield in her opening remarks at the event, which was co-sponsored by the Council on Competitiveness, an industry group. “And it’s very hard to imagine where those jobs are going to come from unless we seriously get busy reinventing manufacturing.” That question should be of great concern to scientists and engineers — 64 percent of whom, Hockfield noted, are employed in the manufacturing sector. <br /><br />Hockfield also directly addressed the commonly held notion that the United States cannot compete in manufacturing against low-wage countries, citing the success of Japan and Germany, both of which feature trade surpluses and high wages. “I take this as positive proof that building a strong advanced manufacturing sector is not impossible, but very much worth pursuing,” Hockfield said. In addition to new business practices and continued strength in education, Hockfield added, “A key hope for progress lies in tapping unprecedented new manufacturing technologies.” <br /><br />Suzanne Berger, a professor of political science and author of How We Compete: What Companies Around the World Are Doing to Make It in Today’s Global Economy, asserted that Americans need to be disabused of the notion that manufacturing is “a ‘sunset sector’ that should be allowed to sink over the horizon.” Increased productivity per worker means the United States still produces 22 percent of the world’s goods, Berger noted, a figure that has been roughly constant for 30 years, and which makes the United States the world’s top manufacturer. Yet the country “has failed to exploit new opportunities for exporting U.S. goods,” she said. “The big problem is not that we can’t compete with China on low wages,” Berger added, but that the United States has “not developed enough kinds of manufacturing that could generate both high profits and also good jobs.”  <br /><br /><strong>Material benefits</strong><br /><br />The roundtable discussion was organized into two consecutive panels, the first of which focused on innovation in materials science. Gerbrand Ceder, a professor in MIT’s Department of Materials Science and Engineering (DMSE), outlined how a “Materials Genome Project” can catalogue the properties of known materials and allow designers to better model potential devices, thus accelerating product development. “Clearly there are some things that would be useful to apply in many types of manufacturing,” Ceder noted. <br /><br />Christine Ortiz, an associate professor also in DMSE, described her research group’s efforts to study the nano-scale properties of “high-strength, lightweight, penetration-resistant” biological materials. Those properties could then be transferred to synthetic materials, expanding the range of products that can be manufactured through methods such as 3-D printing. <br /><br />Charles Fine, a professor at the MIT Sloan School of Management, and Richard Roth, of the Engineering Systems Division, both discussed lightweight automobiles as an alternative to traditional vehicles and an area where the United State could re-establish a competitive advantage in manufacturing. “If we take on the hard challenges and succeed, it’s not so easy to copy,” Fine said. And as Roth noted, “Batteries are extraordinarily expensive,” so new materials research leading to lighter cars would lower those costs by reducing vehicle battery size. In turn, that could make electric automobiles more affordable for consumers and a more appealing investment for manufacturers. <br /><br />But the actual techniques of manufacturing are what most need to be reinvented, asserted Martin Culpepper, an associate professor in MIT’s Department of Mechanical Engineering. Over the last 150 years, suggested Culpepper, heavy industry has refined large-scale production techniques effectively, and developed myriad tools suited to its needs. But businesses today must invent equally useful nanotechnology-based manufacturing techniques, he believes, allowing firms to better manipulate matter at the smallest scales in order to produce everything from new industrial materials to cutting-edge medicine. <br /><br />“We don’t have the tools and technologies right now to do a lot of nano-manufacturing in a really practical way,” said Culpepper. Moreover, he believes, researchers today who want to commercialize lab discoveries underestimate the difficulties of “integrating the science and the [manufacturing] process … this is not a trivial thing.” <br /><br />Culpepper’s own research group aims to create those kinds of small-scale manufacturing tools. Working with one bioscience research institute, he noted, they have been able to roll back the size boundaries of nano-scale DNA arrays, which could make drug production more efficient. <br /><br />Still, these advances are also restricted, Culpepper said, by the limited number of people with a thorough knowledge of nano-scale manufacturing. “In my lab, it’s like an apprenticeship,” Culpepper said. “It takes a long time to learn how to do this stuff properly.” Universities and their partners, he stated, need to help rectify this problem: “We would like to have more support for training.” <br /><br /><strong>Production values</strong><br /><br />The second panel discussion centered on technology advances for transforming production. Rodney Brooks of MIT’s Computer Science and Artificial Intelligence Laboratory suggested it is increasingly hard for industry to find places that provide low-cost labor, meaning that U.S. firms should instead seek low-cost manufacturing  technologies. Specifically, manufacturers who use robotics, Brooks said, have gotten “stuck in what was developed in the 1960s. There’s very little integration of sensors and computation with these robots.” As a result of this adherence to inflexible technology, Brooks added, “the integration cost of using robots in industry is 5 to 10 times the capital cost of the robots, and only makes sense if you do the same thing again and again.”  <br /><br />Bernhardt Trout, a chemical engineering professor and director of the Novartis-MIT Center for Continuous Manufacturing, asserted that the traditional, small-molecule part of the pharmaceuticals industry — firms that make over-the-counter medicines, for instance — invest a “shockingly low” portion of their capital in further product development, instead reaping high profits from existing products, while basic manufacturing technologies have not changed for decades. Trout suggested a streamlined drug-approval process would help motivate industry innovation, but equally claimed the “financialization” of the industry has hurt product development; firms see themselves as “marketing and supply companies.” Advances in academic research, Trout said, are thus especially critical if the industry is to move forward. <br /><br />The rapid spread of manufacturing know-how has had double-edged effects, observed Sanjay Sarma, an associate professor of mechanical engineering. Profitable industries can now be located around the world, while multinational firms build global supply chains to move products in bulk. “The thing that really hurts manufacturing in the U.S. is the flattening effect that comes from economies of scale,” Sarma said. In response, Sarma suggested domestic manufacturing can become lucrative with the use of “small-lot logistics,” technologies that reduce production and transportation costs and can make many businesses, such as apparel firms, more viable.<br /><br />However, making new manufacturing environmentally sustainable will be a large challenge, said Timothy Gutowski, also a professor of mechanical engineering. “Here’s the problem: Underpriced ecosystem services provide a competitive advantage,” said Gutowski, meaning that companies who extract natural resources cheaply still have edges in manufacturing. Cooperation between industry and government — and between governments — will be necessary to put new manufacturing on a sound environmental foundation.<br /><br />Charles Cooney, a professor of chemical engineering moderating the second panel, concluded that three things are important to improving U.S. manufacturing: an understanding of systems thinking, which can help create new, possibly local and regional forms of manufacture and distribution; a recognition that sound public policy will be a necessary part of new development; and a multi-agency approach to science and technology funding, to improve the odds that more forms of research will move from the lab to the factory. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Moral judgments can be altered ... by magnets]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/moral-control-0330.html</link>
<story_id>15150</story_id>
<featured>0</featured>
<description><![CDATA[By disrupting brain activity in a particular region, neuroscientists can sway people’s views of moral situations.]]></description>
<postDate>Tue, 30 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100329160959-1.png</thumbURL>
<smallURL width='140' height='118'>http://web.mit.edu/newsoffice/images/article_images/w140/20100329160959-1.gif</smallURL>
<fullURL width='368' height='310'>http://web.mit.edu/newsoffice/images/article_images/20100329160959-1.gif</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='351' height='320'>http://web.mit.edu/newsoffice/images/article_images/20100330120705-2.jpg</fullURL>
<imageCredits><![CDATA[Images courtesy Rebecca Saxe laboratory, MIT]]></imageCredits>
<imageCaption><![CDATA[MRI brain scans showing the location of the right temporoparietal junction (blue circle). The purple triangle shows a nearby region that the researchers disrupted with magnetic stimulation as a control experiment. ]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[To make moral judgments about other people, we often need to infer their intentions — an ability known as “theory of mind.” For example, if one hunter shoots another while on a hunting trip, we need to know what the shooter was thinking: Was he secretly jealous, or did he mistake his fellow hunter for an animal?<br /><br />MIT neuroscientists have now shown they can influence those judgments by interfering with activity in a specific brain region — a finding that helps reveal how the brain constructs morality.<br /><br />Previous studies have shown that a brain region known as the right temporo-parietal junction (TPJ) is highly active when we think about other people’s intentions, thoughts and beliefs. In the new study, the researchers disrupted activity in the right TPJ by inducing a current in the brain using a magnetic field applied to the scalp. They found that the subjects’ ability to make moral judgments that require an understanding of other people’s intentions — for example, a failed murder attempt — was impaired. <br /><br />The researchers, led by Rebecca Saxe, MIT assistant professor of brain and cognitive sciences, <a href="http://www.pnas.org/content/early/2010/03/11/0914826107.full.pdf+html" target="_blank">report their findings</a> in the <em>Proceedings of the National Academy of Sciences</em> the week of March 29. Funding for the research came from The National Center for Research Resources, the MIND Institute, the <a href="http://www.nmr.mgh.harvard.edu/martinos/aboutUs/index.php" target="_blank">Athinoula A. Martinos Center for Biomedical Imaging</a>, the <a href="http://sfari.org/" target="_blank">Simons Foundation</a> and the David and Lucille Packard Foundation.<br /><br />The study offers “striking evidence” that the right TPJ, located at the brain’s surface above and behind the right ear, is critical for making moral judgments, says Liane Young, lead author of the paper. It’s also startling, since under normal circumstances people are very confident and consistent in these kinds of moral judgments, says Young, a postdoctoral associate in MIT’s Department of Brain and Cognitive Sciences.<br /><br />“You think of morality as being a really high-level behavior,” she says. “To be able to apply (a magnetic field) to a specific brain region and change people’s moral judgments is really astonishing.”<br /><br /><strong>Thinking of others</strong><br /><br />Saxe first identified the right TPJ’s role in theory of mind a decade ago — a discovery that was the subject of her MIT PhD thesis in 2003. Since then, she has used functional magnetic resonance imaging (fMRI) to show that the right TPJ is active when people are asked to make judgments that require thinking about other people’s intentions.<br /><br />In the new study, the researchers wanted to go beyond fMRI experiments to observe what would happen if they could actually disrupt activity in the right TPJ. Their success marks a major step forward for the field of moral neuroscience, says Walter Sinnott-Armstrong, professor of philosophy at Duke University.<br /><br />“Recent fMRI studies of moral judgment find fascinating correlations, but Young et al usher in a new era by moving beyond correlation to causation,” says Sinnott-Armstrong, who was not involved in this research.<br /><br />The researchers used a noninvasive technique known as transcranial magnetic stimulation (TMS) to selectively interfere with brain activity in the right TPJ. A magnetic field applied to a small area of the skull creates weak electric currents that impede nearby brain cells’ ability to fire normally, but the effect is only temporary.<br /><br />In one experiment, volunteers were exposed to TMS for 25 minutes before taking a test in which they read a series of scenarios and made moral judgments of characters’ actions on a scale of one (absolutely forbidden) to seven (absolutely permissible). <br /><br />In a second experiment, TMS was applied in 500-milisecond bursts at the moment when the subject was asked to make a moral judgment. For example, subjects were asked to judge how permissible it is for a man to let his girlfriend walk across a bridge he knows to be unsafe, even if she ends up making it across safely. In such cases, a judgment based solely on the outcome would hold the perpetrator morally blameless, even though it appears he intended to do harm. <br /><br />In both experiments, the researchers found that when the right TPJ was disrupted, subjects were more likely to judge failed attempts to harm as morally permissible. Therefore, the researchers believe that TMS interfered with subjects’ ability to interpret others’ intentions, forcing them to rely more on outcome information to make their judgments.<br /><br />“It doesn’t completely reverse people’s moral judgments, it just biases them,” says Saxe.<br /><br />When subjects received TMS to a brain region near the right TPJ, their judgments were nearly identical to those of people who received no TMS at all.<br /><br />While understanding other people’s intentions is critical to judging them, it is just one piece of the puzzle. We also take into account the person’s desires, previous record and any external constraints, guided by our own concepts of loyalty, fairness and integrity, says Saxe.<br /><br />“Our moral judgments are not the result of a single process, even though they feel like one uniform thing,” she says. “It’s actually a hodgepodge of competing and conflicting judgments, all of which get jumbled into what we call moral judgment.”<br /><br />Saxe’s lab is now studying the role of theory of mind in judging situations where the attempted harm was not a physical threat. The researchers are also doing a study on the role of the right TPJ in judgments of people who are morally lucky or unlucky. For example, a drunk driver who hits and kills a pedestrian is unlucky, compared to an equally drunk driver who makes it home safely, but the unlucky homicidal driver tends to be judged more morally blameworthy.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[A grand unified theory of AI]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/ai-unification.html</link>
<story_id>15151</story_id>
<featured>0</featured>
<description><![CDATA[A new approach unites two prevailing but often opposed strains in the history of artificial-intelligence research.]]></description>
<postDate>Tue, 30 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100329171930-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100329171930-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100329171930-1.jpg</fullURL>
</image>
<body><![CDATA[In the 1950s and ’60s, artificial-intelligence researchers saw themselves as trying to uncover the rules of thought. But those rules turned out to be way more complicated than anyone had imagined. Since then, artificial-intelligence (AI) research has come to rely, instead, on probabilities — statistical patterns that computers can learn from large sets of training data.<br /><br />The probabilistic approach has been responsible for most of the recent progress in artificial intelligence, such as voice recognition systems, or the system that recommends movies to Netflix subscribers. But Noah Goodman, an MIT research scientist whose department is Brain and Cognitive Sciences but whose lab is Computer Science and Artificial Intelligence, thinks that AI gave up too much when it gave up rules. By combining the old rule-based systems with insights from the new probabilistic systems, Goodman has found a way to model thought that could have broad implications for both AI and cognitive science.<br /><br />Early AI researchers saw thinking as logical inference: if you know that birds can fly and are told that the waxwing is a bird, you can infer that waxwings can fly. One of AI’s first projects was the development of a mathematical language — much like a computer language — in which researchers could encode assertions like “birds can fly” and “waxwings are birds.” If the language was rigorous enough, computer algorithms would be able to comb through assertions written in it and calculate all the logically valid inferences. Once they’d developed such languages, AI researchers started using them to encode lots of commonsense assertions, which they stored in huge databases.<br /><br />The problem with this approach is, roughly speaking, that not all birds can fly. And among birds that can’t fly, there’s a distinction between a robin in a cage and a robin with a broken wing, and another distinction between any kind of robin and a penguin. The mathematical languages that the early AI researchers developed were flexible enough to represent such conceptual distinctions, but writing down all the distinctions necessary for even the most rudimentary cognitive tasks proved much harder than anticipated.<br /><br /><strong>Embracing uncertainty</strong><br /><br />In probabilistic AI, by contrast, a computer is fed lots of examples of something — like pictures of birds — and is left to infer, on its own, what those examples have in common. This approach works fairly well with concrete concepts like “bird,” but it has trouble with more abstract concepts — for example, flight, a capacity shared by birds, helicopters, kites and superheroes. You could show a probabilistic system lots of pictures of things in flight, but even if it figured out what they all had in common, it would be very likely to misidentify clouds, or the sun, or the antennas on top of buildings as instances of flight. And even flight is a concrete concept compared to, say, “grammar,” or “motherhood.”<br /><br />As a research tool, Goodman has developed a computer programming language called Church — after the great American logician Alonzo Church — that, like the early AI languages, includes rules of inference. But those rules are probabilistic. Told that the cassowary is a bird, a program written in Church might conclude that cassowaries can probably fly. But if the program was then told that cassowaries can weigh almost 200 pounds, it might revise its initial probability estimate, concluding that, actually, cassowaries probably can’t fly.<br /><br />“With probabilistic reasoning, you get all that structure for free,” Goodman says. A Church program that has never encountered a flightless bird might, initially, set the probability that any bird can fly at 99.99 percent. But as it learns more about cassowaries — and penguins, and caged and broken-winged robins — it revises its probabilities accordingly. Ultimately, the probabilities represent all the conceptual distinctions that early AI researchers would have had to code by hand. But the system learns those distinctions itself, over time — much the way humans learn new concepts and revise old ones.<br /><br />“What’s brilliant about this is that it allows you to build a cognitive model in a fantastically much more straightforward and transparent way than you could do before,” says Nick Chater, a professor of cognitive and decision sciences at University College London. “You can imagine all the things that a human knows, and trying to list those would just be an endless task, and it might even be an infinite task. But the magic trick is saying, ‘No, no, just tell me a few things,’ and then the brain — or in this case the Church system, hopefully somewhat analogous to the way the mind does it — can churn out, using its probabilistic calculation, all the consequences and inferences. And also, when you give the system new information, it can figure out the consequences of that.”<br /><br /><strong>Modeling minds</strong><br /><br />Programs that use probabilistic inference seem to be able to model a wider range of human cognitive capacities than traditional cognitive models can. At the 2008 conference of the Cognitive Science Society, for instance, Goodman and Charles Kemp, who was a PhD student in BCS at the time, presented work in which they’d given human subjects a list of seven or eight employees at a fictitious company and told them which employees sent e-mail to which others. Then they gave the subjects a short list of employees at another fictitious company. Without any additional data, the subjects were asked to create a chart depicting who sent e-mail to whom at the second company.<br /><br />If the e-mail patterns in the sample case formed a chain — Alice sent mail to Bob who sent mail to Carol, all the way to, say, Henry — the human subjects were very likely to predict that the e-mail patterns in the test case would also form a chain. If the e-mail patterns in the sample case formed a loop — Alice sent mail to Bob who sent mail to Carol, and so on, but Henry sent mail to Alice — the subjects predicted a loop in the test case, too.<br /><br />A program that used probabilistic inference, asked to perform the same task, behaved almost exactly like a human subject, inferring chains from chains and loops from loops. But conventional cognitive models predicted totally random e-mail patterns in the test case: they were unable to extract the higher-level concepts of loops and chains. With a range of collaborators in the Department of Brain and Cognitive Sciences, Goodman has conducted similar experiments in which subjects were asked to sort stylized drawings of bugs or trees into different categories, or to make inferences that required guessing what another person was thinking. In all these cases — several of which were also presented at the Cognitive Science Society’s conference — Church programs did a significantly better job of modeling human thought than traditional artificial-intelligence algorithms did.<br /><br />Chater cautions that, while Church programs perform well on such targeted tasks, they’re currently too computationally intensive to serve as general-purpose mind simulators. “It’s a serious issue if you’re going to wheel it out to solve every problem under the sun,” Chater says. “But it’s just been built, and these things are always very poorly optimized when they’ve just been built.” And Chater emphasizes that getting the system to work at all is an achievement in itself: “It’s the kind of thing that somebody might produce as a theoretical suggestion, and you’d think, ‘Wow, that’s fantastically clever, but I’m sure you’ll never make it run, really.’ And the miracle is that it does run, and it works.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Update on planning for high-performance computing center]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/hpcc-update-0329.html</link>
<story_id>15149</story_id>
<featured>0</featured>
<description><![CDATA[Consortium members report 'considerable progress' in several aspects of project in Holyoke, Mass.]]></description>
<postDate>Mon, 29 Mar 2010 18:36:48 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100329144045-1.png</thumbURL>
<smallURL width='140' height='111'>http://web.mit.edu/newsoffice/images/article_images/w140/20100329144045-1.jpg</smallURL>
<fullURL width='368' height='294'>http://web.mit.edu/newsoffice/images/article_images/20100329144045-1.jpg</fullURL>
</image>
<body><![CDATA[<em>Following is a joint statement by the Massachusetts Institute of Technology, the University of Massachusetts, Boston University and Northeastern University on planning for a green, high-performance computing center (GHPCC) in Holyoke, Mass.</em><br /><br />Back in November of last year, the consortium pledged a commitment to work diligently for the next 120 days with the Governor and other cabinet officials, Congressman Olver, Holyoke officials and our colleagues in industry, notably EMC Corp. and Cisco Systems, to move to the next stage of planning the HPCC. Today, we report on the considerable progress made concerning academic, organizational and capital elements of the project.<br /><br />The consortium is pleased to announce that another of the state’s leading research universities — Northeastern University — has indicated its full intention to join the HPCC consortium and commit a $10 million contribution to the GHPCC facility. Meanwhile, we continue to have discussions with other potential partners and other users about the GHPCC. With Northeastern’s commitment added to that of the original three partners, the universities are committing $40 million to building the GHPCC facility. Combined with the estimated $65 million of equipment that will be located in the facility, the universities are now committed to investing over $100 million in Holyoke. <br /><br />On an organizational front, the planning of a 501(c)(3) entity which will operate the GHPCC facility on behalf of the consortium is nearly complete. In addition, the search process for the executive director of the not-for-profit entity has begun along with the creation of a blueprint for an external advisory board, through which others in government, industry and academia in the region will have the opportunity to connect with the GHPCC.<br /><br />We have started planning for a joint R&amp;D, education and outreach program, including an April 15 workshop, in partnership with Holyoke Community College (HCC) and Springfield Technical Community College (STCC), on educational opportunities associated with a green HPCC; and an RFP for collaborative research projects focused on developing processes and metrics for “green” high performance scientific computing.<br /><br />Also, preparation is underway, in partnership with HCC and STCC, of a proposal requesting funding from the National Science Foundation for a new program on cyber infrastructure training, education, advancement, and mentoring for the 21st Century workforce. <br /><br />We are also happy to report that after working closely with The Westmass Area Development Corporation, an affiliate of the Economic Development Council of Western Massachusetts, the consortium has narrowed the field of candidate sites down to two, both of which are in the Holyoke Innovation District. Due diligence on both sites is being conducted and the consortium expects to be able to announce site selection in the summer. In addition, the consortium has identified a number of qualified design and engineering firms, with plans to issue an RFP for this work in April and make the final selection of a design and engineering firm in May.<br /><br />The consortium is especially appreciative of Governor Patrick's commitment to invest up to $25 million of state funds to support the development of the GHPCC facility. This represents a strategic investment in the state’s Innovation Economy, which will have long-term benefits for Holyoke, the home communities of the HPCC consortium members and those of our other industry and academic partners, and the Commonwealth as a whole. While the $25 million investment in the facility does not entirely close the funding gap, we are confident of our ability to get the project done with this level of state support. All the university partners look forward to engaging with state officials, our industry partners EMC and Cisco, along with others in the coming weeks to secure the necessary agreements to bring this project to fruition. It remains our shared objective to begin the development work in fall of 2010. <br /><br />The establishment of the GHPCC would represent the most significant economic development partnership among state government, leading research universities and private industry in the history of the Commonwealth. It will provide a vital piece of infrastructure for our scientists and researchers, act as a hub that will facilitate collaboration in R&amp;D and education among our higher education institutions, and serve as a catalyst for technology-based economic development.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Rough calculations]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/street-fight-0329.html</link>
<story_id>15146</story_id>
<featured>0</featured>
<description><![CDATA[Sanjoy Mahajan’s new book, Street-Fighting Mathematics, lays out practical tools for educated guessing and down-and-dirty problem-solving]]></description>
<postDate>Mon, 29 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100326145645-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100326145645-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100326145645-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='460'>http://web.mit.edu/newsoffice/images/article_images/20100326145752-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Sanjoy Mahajan, associate director for teaching initiatives at MIT’s Teaching and Learning Laboratory]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Time for some quick arithmetic: Is 3600 x 4.4 x 10<sup>4</sup> x 32 larger or smaller than 3 x 10<sup>9</sup>? <br /><br />Finding the right answer, says Sanjoy Mahajan, associate director for teaching initiatives at MIT’s Teaching and Learning Laboratory, does not require crafting a long, tedious calculation. Instead, the key to solving this problem — and many others — lies in having informal tools on hand that let us attack the problem. Though the result may not be perfectly precise, he believes, intuitive mathematical reasoning is often sufficient for our needs. <br /><br />“That’s not to say exact answers aren’t useful,” says Mahajan, “but if looking for them is your only approach, you may never get any answer at all. Sometimes it’s better to start with something rough.”<br /><br />So while conventional math teaching is often a highly formal affair, with an emphasis on definitions, theorems, and proofs, Mahajan believes we should learn practical math tools and understand why they work. He outlines this philosophy — and explains those tools — in a new book, <em>Street-Fighting Mathematics: The Art of Educated Guessing and Opportunistic Problem-Solving</em>, being published this month by MIT Press. <br /><br /><em>Street-Fighting Mathematics</em> has its origins in a course Mahajan, who has a PhD in physics, began teaching at MIT three years ago during the Independent Activities Period (IAP), the winter-break session in which students can take extra courses that emphasize hands-on learning. He says the book is intended for “students, practicing engineers, scientists, anyone who has to use mathematics to solve problems and get rough answers quickly.”<br /><br />Given its practical focus, <em>Street-Fighting Mathematics</em> is not organized around traditional math topics, such as differential equations, but ways of thinking: reasoning by analogy, visualizing geometric problems, and more. Readers can then answer all manner of questions: Guessing the number of babies in the United States, calculating the bond angles in methane, or determining the drag that air exerts on a 747. <br /><br /><strong>‘Math is not a spectator sport’</strong><br /><br />Mathematicians with an interest in the public understanding of science are impressed with Mahajan’s effort. “There’s a certain bravery in Sanjoy’s book,” says Steven Strogatz, a professor of mathematics at Cornell, and author of an ongoing series of columns on math at The New York Times online. “He comes out defiantly and says, Of course I’m being imprecise — then you fill in the details. I wish more people in math would do that. Math is not a spectator sport. It’s an active enterprise. That’s what we all know, but we don’t all teach it that way.”<br /><br />To make math an active enterprise for his students, Mahajan requires that they give him feedback about the course readings in advance of his lectures. Building on the work of Sacha Zyto, a PhD student in mechanical engineering, Mahajan has helped develop an online system in which students annotate the reading materials via PDF files. This method helps Mahajan evaluate the clarity of his presentations and see which ideas stymie students most often, while even allowing students to answer other students’ queries.  <br /><br />“You want the students to wrestle with the material, to make the knowledge their own,” says Mahajan. <br /><br />Mahajan’s unconventional teaching practices stem from his focus, as a physicist, on finding quick, practical answers. Then again, perhaps rolling up one’s sleeves and hacking through problems is how everyone works. “There is a culture in pure mathematics that emphasizes rigor and careful proofs,” says Strogatz. “Yet all practicing mathematicians know we also use our intuitions, then we clean our answers up.” Strogatz is hopeful that Street-Fighting Mathematics can help form a pedagogical trend away from rote learning, and toward a more practical approach. <br /><br />So let’s get back to the initial question (the numbers relate to the storage capacity of a data CD-ROM). The key to solving it, says Mahajan, is to recognize that the components of the first, messy-looking number can be broken into powers of 10. Then we can temporarily set aside these powers of 10 — Mahajan calls this “taking out the big part,” one of his tenets of problem-solving — while handling the smaller, simpler multiplication problem. <br /><br />Okay: Picture the number as (3.6 x 10<sup>3</sup>) x (4.4 x 10<sup>4</sup>) x (3.2 x 10<sup>1</sup>). To multiply powers of 10 in practice, we add them, here producing 10<sup>8</sup>. Leave that aside momentarily and multiply 3.6 x 4.4 x 3.2. The answer is about 50, or 5.0 x 10<sup>1</sup>. Combine that with 10<sup>8</sup>, and we have our answer: Roughly 5.0 x 10<sup>9</sup>, which is bigger than 3 x 10<sup>9</sup>. Street-fighting math, and we barely got a scratch.  <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Pushing droplets around]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/asymm-wetting-0329.html</link>
<story_id>15147</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers find a way to make drops on a surface move in just one direction, with possible applications ranging from biology to electronics]]></description>
<postDate>Mon, 29 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100326152040-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100326152040-1.jpg</smallURL>
<fullURL width='368' height='370'>http://web.mit.edu/newsoffice/images/article_images/20100326152040-1.jpg</fullURL>
<imageCredits><![CDATA[Images: Kuang-Han Chu, Rong Xiao and Evelyn N. Wang]]></imageCredits>
<imageCaption><![CDATA[A symmetrical droplet (top) forms on a surface with straight nano-pillars, while on a surface with bent pillars (bottom) the droplet is asymmetrical, extending out only to the right. Inset images are micrographs of the surface structure.]]></imageCaption>
</image>
<body><![CDATA[Controlling the way liquids spread across a surface is important for a wide variety of technologies, including DNA microarrays for medical research, inkjet printers and digital lab-on-a-chip systems. But until now, the designers of such devices could only control how much the liquid would spread out over a surface, not which way it would go.<br /><br />New research from mechanical engineers at MIT has revealed a new approach that, by creating specific kinds of tiny structures on a material’s surface, can make a droplet spread only in a single direction.<br /><br />A report on the new work, by Esther and Harold E. Edgerton Assistant Professor of Mechanical Engineering Evelyn N. Wang and graduate students Kuang-Han Chu and Rong Xiao, was published on March 28 in the journal <em>Nature Materials</em>.<br /><br />The system Wang and her team developed is completely passive, based on producing a textured surface with tiny pillars shaped in specific ways to propel liquid in one direction and restrict its movement in others. Once the surface is prepared, no mechanical or electrical controls are needed to propel the liquid in the desired direction, and a droplet placed at any point on the surface will always spread the same way.<br /><br />It’s just the shapes on the surface that control how the drops spread, rather than the particular materials used, Wang says. The chips used for testing were made at the MIT Microsystems Technology Lab by etching a silicon wafer surface to produce a grid of tiny pillars, which then were selectively coated with gold on one side to make the pillars bend in one direction. To prove that the effect was caused just by the bent shapes rather than some chemical process involving the silicon and gold, the researchers, with the help of Professor Karen Gleason’s group in the Department of Chemical Engineering, then coated the surface with a thin layer of a polymer so that the water would only come in contact with a single type of material. The pillars are all curved in one direction, and cause the liquid to move in that direction.<br /><br />“Nobody had really studied this kind of geometry, because it’s hard to fabricate,” Wang says. <br /><br />Wang explains that while this work is still early-stage basic research, in principle such systems could be used for a wide variety of applications. For example, it could provide new ways to manipulate biological molecules on the surface of a chip, for various testing and measurement systems. It might be used in desalination systems to help direct water that condenses on a surface toward a collection system. Or it might allow more precise control of cooling liquids on a microchip, directing the coolant toward specific hotspots rather than letting them spread out over the whole surface. <br /><br />“It’s a big deal to be able to cool local hotspots on a chip,” Wang says, especially as the components on a chip continue to get smaller and thermal management becomes ever more critical. The research was funded in part by the National Science Foundation, DARPA, and Northrup Grumman.<br /><br />Mark Shannon, professor of mechanical science and engineering at the University of Illinois, Urbana-Champaign, agrees that this method might be further developed for a variety of applications, including biomedical lab-on-a-chip systems for the detection of specific biomolecules in blood, for example. “Droplet manipulation has been heavily developed for moving samples from station to station for different analysis steps,” he says, and this new method might provide a useful way to do that with minimal energy requirements, but to do so will require the ability to create multiple regions on a surface that propel the liquid in different directions for each stage. “This research will help enable these unit operations,” he says, in combination with related research currently being carried out in other places.<br /><br />Howard Stone, professor of mechanical and aerospace engineering at Princeton University, who was not involved in this research, says researchers have taken several approaches to surface patterning and control in recent years, some inspired by nature and some by materials applications. “This research advance for one-dimensional asymmetric spreading is a nice addition to the toolbox for surface patterning to control liquid spreading,” he says.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Slackers and superstars of the microbial workplace]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/drug-manufacture-0326.html</link>
<story_id>15141</story_id>
<featured>0</featured>
<description><![CDATA[MIT chemical engineers find that yeast engineered to manufacture drugs vary widely in their productivity]]></description>
<postDate>Fri, 26 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100325132818-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100325132818-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100325132818-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Wikimedia]]></imageCredits>
<imageCaption><![CDATA[MIT chemical engineers have shown that some populations of yeast engineered to produce drugs secrete large quantities of the drug, while others lag behind, even though the yeast are genetically identical.]]></imageCaption>
</image>
<body><![CDATA[Drug companies often use yeast to manufacture drugs, especially proteins such as antibodies and enzymes. It has been assumed that a batch of genetically identical yeast will secrete such drugs at uniform rates, but MIT chemical engineers have made the surprising discovery that drug productivity varies greatly among individual yeast cells.<br /><br />The research team, led by J. Christopher Love, assistant professor of chemical engineering, found that while a small subset of yeast is highly productive, a significant minority of the population releases nothing at all. The finding, <a href="http://www3.interscience.wiley.com/cgi-bin/fulltext/123278425/PDFSTART" target="_blank">reported in the journal</a> <em>Biotechnology and Bioengineering</em>, could give drug companies new targets to maximize yeast productivity. <br /><br />“This is clearly a very powerful tool for selecting cells based on their rate of synthesis, which has a number of applications, including the obvious ability to pick out high-producing cell lines,” says Barry Buckland, a former research and development executive at Merck Research Laboratories, who was not involved in the research.<br /><br />Love and his colleagues studied a strain of yeast known as Pichia pastoris, which can be used to manufacture proteins including antibodies, growth factors, enzymes and erythropoietin (a hormone that controls red blood cell production). There are now 151 such proteins approved for therapeutic use in the United States or Europe. In 2008, sales of these biopharmaceuticals in the United States exceeded $45 billion, with nearly half of them produced by microbes, including yeast.<br /><br />To measure yeast productivity, the researchers used a technique that Love had previously developed and used to study immune cells, specifically B cells, called microengraving. The technique allows researchers to look at the quantities of proteins released by single cells. <br /><br />To do this, they place cells into individual wells arranged in a lattice pattern on a soft rubber surface. Borrowing from an artistic engraving technique used for printmaking, the researchers use that array of cells to “print” the proteins produced by the cells onto the surface of multiple identical glass slides. By measuring how much protein each cell “prints” onto the slides, the researchers can determine the productivity of individual cells.<br /><br />The yeast in the study were engineered to produce a fragment of a human antibody molecule, but the researchers were surprised to find that a subset of the yeast population (about 35 percent) did not secrete measureable amounts of protein. <br /><br />“At any given time, there’s a large fraction of the culture that does not contribute to the overall yield,” says Kerry Routenberg Love, a postdoctoral associate in chemical engineering and lead author of the paper.<br /><br />Intrigued, the researchers ran a longer study, over an eight-hour period, and identified three subpopulations among the cells: those that secrete very little protein over this time, those that secrete consistently high quantities, and those that fluctuate between states of high and low productivity.<br /><br />Because the yeast used in the study (and in drug manufacturing) are genetically identical, genetic differences cannot account for the discrepancies in productivity. Instead, the difference appears to be epigenetic — meaning that it involves differences in the processing that the proteins undergo after they are assembled, such as protein folding and secretion from the cell.<br /><br />The researchers are not sure yet why these differences occur, but they have some theories. “It could be due to stress imposed on the cells that have been engineered to overproduce a product that’s not a natural gene product for them,” says Routenberg Love. <br /><br />Even though the protein itself is not toxic to the cells, it can have adverse effects such as overloading the cell’s protein-folding machinery and secretory pathways. When the cells get overloaded, they need to stop and “reset.”<br /><br />In future work, the researchers hope to discover more about why there is such variability in yeast productivity, which could lead to new ways to improve drug yields by enhancing productivity in low-producing cells.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The sound and the query]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/uttering-trees-0326.html</link>
<story_id>15142</story_id>
<featured>0</featured>
<description><![CDATA[Why do questions take the form they do? An MIT linguist explains how the noises we make help to shape the sentences we speak.]]></description>
<postDate>Fri, 26 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100325141954-0.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100325141954-0.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100325141954-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[MIT linguistics professor Norvin Richards stands in front of a tree-like sentence diagram often used by linguists.]]></imageCaption>
</image>
<body><![CDATA[In linguistic terms, a question is largely the re-ordering of a statement. Shuffle the words around, make a couple of other changes, and “John rode a horse” becomes “What did John ride?”<br /><br />Linguists call this re-arranging of words “<em>wh</em>-movement,” due to the <em>wh</em>-words used in questions (who, when, and so on) and they believe it occurs in two forms. English displays what linguists call “overt <em>wh</em>-movement,” in which word order is shuffled heavily, since many questions begin with <em>wh</em>-words. (There are exceptions: “John rode a <em>what?</em>”) But some languages, like Japanese, deploy “covert <em>wh</em>-movement,” in which word order remains largely intact as a statement becomes a question, and the <em>wh</em>-words appear in a variety of locations.<br /><br />But what determines which of these options a given language uses? In a new book, “Uttering Trees,” MIT linguistics professor Norvin Richards asserts that if we carefully study prosody — the way the pitch of our voices goes up and down — we can determine which kind of <em>wh</em>-movement any language will employ. In turn, Richards believes, this suggests that for all languages, the sound pattern in sentences is more integral to the syntax — the processes and principles that govern the structure of sentences — than scholars have generally thought.<br /><br />“If you were to ask a syntactician why English forms its questions one way and Japanese forms its questions another way, there isn’t really an answer,” says Richards. “But I’m shooting for a deeper explanation in the book. And the idea is that there’s a universal principle explaining the prosody we observe when we ask questions.”<br /><br />If the pattern Richards has detected holds up, it could persuade more linguists that the relationship between sound and syntax is a necessity, not a contingency. “If what I’m saying is right,” Richards says, “then I think that as the syntax begins to build the sentence structure, it’s also making a rough draft of the prosody.” That could alter linguists’ views about how the rules of language are laid down, and provide more evidence for the notion of Universal Grammar, the idea that an innate facility for language helps shape the form of languages globally. <br /><br /><strong>Sound system</strong><br /><br />There are actually two distinct essays in “Uttering Trees,” published this month by MIT Press. (The title refers to the tree-like sentence diagrams linguists use.) The first essay analyzes how similar elements of sentences must be separated for the sake of comprehension, while the second develops Richards’ thesis about sound and questions, surveying English, Japanese, Basque, Bengali, Tagalog, French, Portugese, and more. To get a flavor for his work, consider this second issue.<br /><br />Recordings of Japanese spoken in Tokyo, when charted by the frequency of the speaker’s voice, show that the statement “Naoya-ga nanika-o nomiya-de nonda” (“Naoya drank something at the bar”) maintains a rising-and-falling pitch pattern. But in the question, “Naoya-ga nani-o nomiya-de nonda no?” (“What did Naoya drink at the bar?”), the words “nomiya-de nonda” have a low, flat frequency, interrupting the rising-and-falling pattern. This flat intonation falls between the <em>wh</em>-word, “nani-o,” and the question-indicating complementizer, “no,” at the end of the sentence.<br /><br />“There’s this prosodic phenomenon,” says Richards. “You get a big pitched peak on the <em>wh</em>-word, then people mutter, so the prosody is kind of flat between the <em>wh</em>-word and the complementizer, then it goes back up. People squash these words so that they’re lower than they would be in the corresponding statement. We have nice phonetic evidence that they’re creating a prosodic domain that starts with the <em>wh</em>-word and ends with the question mark.”<br /><br />So while there are two kinds of <em>wh</em>-movement, Richards proposes that every language ends up like Japanese, in the sense that <em>wh</em>-words and complementizers exist in a single prosodic domain.  In some languages, including Japanese, this is accomplished by direct changes in the prosody, while, others, like English, require a greater transportation of <em>wh</em>-words, so they can be located in the same prosodic domain as question marks.  But if we can locate the question mark and understand how prosody is organized, Richards argues, we can predict where the <em>wh</em>-word will be situated in every tongue — which provides evidence for the common foundations of all languages. In Bengali, Richards has found, the sentence, “ora Suneche ke abe” (“They have heard who will come”) shifts into a question as, “ora ke abe Suneche?” (“Who have they heard will come?”). In this case, “ke” serves as the <em>wh</em>-word, with the relevant prosodic domain lying between it and the closing question indicator. <br /><br />Colleagues have praised “Uttering Trees.” Answering questions by e-mail, Elena Anagnostopoulou, a professor of linguistics at the University of Crete, says Richards “asks a question that has rarely been addressed before, namely why some languages have overt and others covert <em>wh</em>-movement,” and defends his thesis “very convincingly.” The book, she believes, lays out a “novel research agenda, where new proposals are raised and the proposals are highly predictive.” The predictive aspect of Richards’ theory is precisely what will be tested by future research. For all the languages he samples, scores more await evaluation in terms of <em>wh</em>-movement. For now, though, Richards’ idea is sound.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[No harm, no foul]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/moral-judgment-0325.html</link>
<story_id>15137</story_id>
<featured>0</featured>
<description><![CDATA[Study of moral judgment finds that patients with a specific brain defect lack the emotional reaction necessary to find fault with attempted murderers]]></description>
<postDate>Thu, 25 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100324155208-1.png</thumbURL>
<smallURL width='140' height='145'>http://web.mit.edu/newsoffice/images/article_images/w140/20100324155208-1.jpg</smallURL>
<fullURL width='368' height='382'>http://web.mit.edu/newsoffice/images/article_images/20100324155208-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Patrick Gillooly]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='279'>http://web.mit.edu/newsoffice/images/article_images/20100324160801-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Wikipedia]]></imageCredits>
<imageCaption><![CDATA[A diagram of the human brain with the ventromedial prefrontal cortex (VMPC) highlighted in red.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='215'>http://web.mit.edu/newsoffice/images/article_images/20100324160801-3.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Liane Young]]></imageCredits>
<imageCaption><![CDATA[These brain scans show the locations of VMPC lesions in the nine patients who took part in this study.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='258'>http://web.mit.edu/newsoffice/images/article_images/20100324160802-4.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of Liane Young]]></imageCredits>
<imageCaption><![CDATA[For most scenarios, patients with VMPC lesions performed similarly to normal subjects and patients with damage to other brain regions, but differences emerged when they were asked to judge people who attempted (but failed) to harm someone else.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Imagine this scenario: A woman and her friend are touring a chemical factory. They come to a coffee machine and, next to it, a container labeled “toxic.” The woman sees the label but goes ahead and scoops a powdery white substance from the container into a cup of coffee she has brewed for her friend. The friend drinks the coffee but is unharmed, because it turns out the powder was only sugar.<br /><br />Most people would say the woman’s actions were morally repugnant. However, in a new study, patients with damage to a part of the brain known as the ventromedial prefrontal cortex (VMPC) reacted very differently. They were unable to conjure a normal emotional response to the situation, and based their judgment only on the outcome — that is, no harm was done. In their view, the friend’s actions were morally permissible.<br /><br />That suggests that the human brain’s ability to respond appropriately to intended harms — that is, with outrage toward the perpetrator — is seated in the VMPC, a brain region associated with regulating emotions.<br /><br />The finding offers a new piece to the puzzle of how the human brain constructs morality, says Liane Young, a postdoctoral associate in MIT’s Department of Brain and Cognitive Sciences and lead author of a <a href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WSS-4YP14T1-D&amp;_user=501045&amp;_coverDate=03%2F25%2F2010&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_acct=C000022659&amp;_version=1&amp;_urlVersion=0&amp;_userid=501045&amp;md5=9212c20c0e1293d29f47868fd613f4fc" target="_blank">paper</a> describing the findings in the March 25 issue of the journal <em>Neuron</em>.<br /><br />“We’re slowly chipping away at the structure of morality,” says Young. “We’re not the first to show that emotions matter for morality, but this is a more precise look at how emotions matter.”<br /><br /><strong>Judging others</strong><br /><br />Working with researchers at the University of Southern California, led by Antonio Damasio, Young studied a group of nine patients with damage (caused by aneurisms or tumors) to the VMPC, a plum-sized area located above and behind the eyes. <br /><br />Such patients have difficulty processing social emotions such as empathy or embarrassment, but “they have perfectly intact capacity for reasoning and other cognitive functions,” says Young.<br /><br />A 2007 study by Damasio, Young and their colleagues showed that such patients are more willing than non-brain-damaged adults to judge killing or harming another person as morally permissible if doing so would save others’ lives. That led the researchers to suspect that the brain-damaged patients lacked appropriate emotional responses to moral harms and relied instead on calculating, rational approach to moral dilemmas.<br /><br />In the new Neuron study, the researchers tried to tease out the exact role of emotional responses in making moral judgments. They gave the subjects a series of 24 scenarios and asked for their reactions. The scenarios of most interest to the researchers were ones featuring a mismatch between the person’s intention and the outcome — either failed attempts to harm or accidental harms.<br /><br />“Every time we make a judgment, there are lots of factors that influence it, and two of the most important are what the agent wants to do, and what actually happens,” says Young.<br /><br />When confronted with failed attempts to harm, the patients had no problems understanding the perpetrator’s intentions, but they failed to hold them morally responsible. The patients even judged attempted harms as more permissible than accidental harms (such as accidentally poisoning someone) — a reversal of the pattern seen in normal adults.<br /><br />“They can process what people are thinking and their intentions, but they just don’t respond emotionally to that information,” says Young. “They can read about a murder attempt and judge it as morally permissible because no harm was done.”<br /><br />This supports the idea that making moral judgments requires at least two processes — a logical assessment of the intention, and an emotional reaction to it, says Michael Koenigs, a neuroscientist at the University of Wisconsin who has also studied patients with VMPC damage.<br /><br />“There's no doubt that our moral sense is informed by our ability to infer the intentions of others, and to generate an affective response to those intentions,” says Koenigs. “This study implicates VMPC as a key area for integrating intention with affect [emotional feeling] to yield moral judgment.”<br /><br />The ability to blame others who try to cause harm, even when they fail, may have evolved as a way to protect ourselves from those with malevolent intentions, says Young. “This information is critical for making judgments about whom to be friends with and whom to trust,” she says. “We don’t want people wishing harm on us, even if they fail.”<br /><br />In future work, Young hopes to study patients who incurred damage to the VMPC when they were younger, to see if they have the same impaired judgment. She also plans to study patient reactions to situations where the harmful attempts may be directed at the patient and therefore more personal.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Dynamo theory]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/explained-dynamo-0325.html</link>
<story_id>15138</story_id>
<featured>0</featured>
<description><![CDATA[Recent discoveries raise questions about how small planets can have self-sustaining magnetic fields]]></description>
<postDate>Thu, 25 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100324160258-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100324160258-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100324160258-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
<imageCaption><![CDATA[Earth’s magnetic field is generated from liquid iron located within the planet’s outer core that constantly moves as the planet cools. The outer core is located nearly 3,000 kilometers beneath Earth’s surface.]]></imageCaption>
</image>
<body><![CDATA[The Earth’s global magnetic field is generated in its metallic core, located nearly 3,000 kilometers beneath the planet’s surface. The field has existed on Earth for at least 3.5 billion years and offers clues about how other planets, stars and celestial bodies may have formed.<br /><br />As scientists refine their understanding of how this field works in their ongoing probe of planetary history, one idea they use to explain this process is dynamo theory — the idea that a large dynamo, or magnetic field generator, exists within Earth’s outer core, where liquid iron constantly moves as the planet cools. This continuous motion creates electric currents as electrons move through the liquid. Through this process, the energy of the moving fluid is converted into a magnetic field that can be sustained for billions of years. <br /><br />Knowing that planetary bodies like Earth, the Moon, Mars, and even asteroids have, or once had, a magnetic field is crucial for understanding their history and internal structure.  This is because the presence of a magnetic field inside a body reveals that it also likely formed a metallic core that generated that field, according to Benjamin Weiss, an associate professor in the Department of Earth, Atmospheric and Planetary Sciences. Such a field is one of the few ways to remotely sense a metallic core buried so deep beneath a body’s surface.<br /><br />If a fragment or rock from a planetary body is magnetized, this suggests that the body experienced large-scale melting in which heavier material sunk to the interior to form a metallic core and lighter material floated to the surface to create a rocky crust. This process gives a planet its history. “Otherwise, it would be a pile of space dust,” Weiss said.  <br /><br />Determining whether a planet generated a magnetic field in the past is not only important for inferring the presence of a core, but also may be important for  learning about the origin of the planetary body and even the history of climate change for that body. <br /><br />For example, although Mars does not have a magnetic field generated by a core dynamo today, Weiss and his colleagues have identified magnetization in Martian rocks, which indicates that Mars did have a strong global field billions of years ago. It appears that the disappearance of this early dynamo roughly coincided with the loss of Mars’ early thick atmosphere and the transition from an early warm, wet climate to the planet’s current cold and inhospitable conditions. <br /><br />But scientists’ understanding of dynamo theory has been complicated by recent discoveries of magnetized rocks from the moon and ancient meteorites, as well as an active dynamo field on Mercury — places that were thought to have perhaps cooled too quickly or be too small to generate a self-sustaining magnetic field. It had been thought that smaller bodies couldn’t have dynamos because they cool more rapidly and are therefore more likely to have metallic cores that do not stay in liquid form for very long. <br /><br />In 2008, an MIT-led group that included Weiss discovered magnetic traces within chunks from small, rocky objects called planetesimals that are believed to have slammed together to form the rocky planets 4.5 billion years ago. Planetesimals had previously been thought to be too small to have formed core dynamos. According to Weiss, the finding suggests that sustaining a magnetic field like the one on Earth might not require a large, cooling core that constantly moves liquid and creates currents, but could also be somehow generated by the cores of smaller bodies like planetesimals — some of which are only 160 kilometers wide.<br /><br />Scientists will soon have a chance to explore the relationship between a body’s size and its ability to have a dynamo thanks to NASA’s Dawn spacecraft, which was launched in September 2007 to study Ceres and Vesta, the two largest asteroids in the asteroid belt located between Mars and Jupiter. Dawn is slated to go in orbit around Vesta in 2011, and one of the main goals of the mission is to test whether Vesta, which has a mean diameter of 530 kilometers, has a core. A group of magnetized meteorites known as the HED meteorites are thought to be from Vesta and could be evidence of an early core dynamo on the asteroid.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[A change of mind]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/brain-plasticity-0324.html</link>
<story_id>15134</story_id>
<featured>0</featured>
<description><![CDATA[One protein appears to control neurons’ ability to react to new experiences, MIT scientists show.]]></description>
<postDate>Wed, 24 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100323144046-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100323144046-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100323144046-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Jason Shepherd]]></imageCredits>
<imageCaption><![CDATA[MIT neuroscientists have shown that the protein Arc is necessary for neurons like this one to adjust their responses to new sensory stimuli. (The blue circle is the neuron’s nucleus, and the red strands are actin filaments.)]]></imageCaption>
</image>
<body><![CDATA[Plasticity — the brain’s ability to change in response to external input — is critical for most cognitive functions, including learning and memory. Those changes usually involve a strengthening or weakening of synapses, the connections between brain cells (neurons).<br /><br />MIT neuroscientists have now found that a single protein, known as Arc, appears to control neurons’ ability to strengthen and weaken their synapses by regulating the number of neurotransmitter receptors on their surfaces. The finding could help researchers identify new drug targets for Fragile X and Angelman syndromes — inherited forms of mental retardation that have been linked to deficits of Arc. <br /><br />“The more we understand the chain of cellular events that Arc is involved in, the more we can identify particular targets where we could intervene,” says Mriganka Sur, head of MIT’s Department of Brain and Cognitive Sciences (BCS).<br /><br />Sur and Mark Bear, the Picower Professor of Neuroscience, are senior authors of a paper on the work that appeared in the <a href="http://www.nature.com/neuro/journal/vaop/ncurrent/abs/nn.2508.html" target="_blank">March 14 online edition</a> of <em>Nature Neuroscience</em>.<br /><strong><br />A surprising discovery</strong><br /><br />Jason Shepherd, co-lead author of the paper and a postdoctoral associate in Bear’s lab, began studying Arc as a grad student at Johns Hopkins University. He and his colleagues showed that Arc weakens synapses by removing receptors for glutamate, a neurotransmitter that stimulates neuron activity, from neuron cell membranes. They also discovered that when the Arc gene is turned off in mice, they lose their ability to form long-term memories.<br /><br />In the new study, Shepherd and co-lead author Cortina McCurry, a recent PhD recipient in BCS, did a series of experiments designed to pinpoint the role of Arc in the visual cortex of mice. They started with a classic experimental setup that involves sealing one eye for two days, depriving the eye of visual input. In normal mice, this strengthens synapses in the part of the cortex receiving input from the open eye, and weakens them in cortical cells wired to the closed eye.<br /><br />Shepherd and McCurry performed their experiments on mice with a mutation in the Arc gene that renders the protein ineffective. Because of Arc’s known role in weakening synapses, they expected mice without Arc not to show any synaptic weakening from the closed eye. That prediction came true, but to the researchers’ surprise, they found that the synaptic strengthening normally seen from the open eye also disappeared.<br /><br />“If you knock out the gene, you don’t get either response,” says Shepherd. “The brain is not responsive at all to the changes in sensory input.” <br /><br />They observed the same lack of plasticity in studies of Arc-deficient mice that were repeatedly exposed to the same visual stimulus (for example, a horizontal bar), which normally provokes neurons to enhance their response to that particular stimulus.<br /><br />The results suggest that Arc has an indirect role in inserting glutamate receptors in the cell membrane, as well as its previously known role in removing them, according to the researchers. “It’s remarkable to find one single gene and its protein to be so responsible” in different types of plasticity, says Sur. <br /><br />While others have shown in experiments with neurons that Arc appears to have a role in both suppressing and stimulating synapses, this paper is the first to demonstrate the effect in living animals, says Hey-Kyoung Lee, associate professor of biology at the University of Maryland, who was not involved in the research. “The current paper clearly shows that Arc plays a critical role in shaping cortical synapses with sensory experience,” she says.<br /><br />Shepherd is now planning experiments to image the Arc protein in single cells in the visual cortex. He also plans to further investigate the protein’s role in Fragile X and Angelman syndromes.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[A system that’s worth its salt]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/desalination-0323.html</link>
<story_id>15132</story_id>
<featured>0</featured>
<description><![CDATA[New approach to water desalination could lead to small, portable units that could be sent to disaster sites or remote locations.]]></description>
<postDate>Tue, 23 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100322115617-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100322115617-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100322115617-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[A single unit of the new desalination device, fabricated on a layer of silicone. In the Y-shaped channel (in red), seawater enters from the right, and fresh water leaves through the lower channel at left, while concentrated brine leaves through the upper channel. ]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100322115617-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Jongyoon Han, right, and Sung Jae Kim explain their new process for water desalination. In foreground is a microscope used to observe the prototype unit in action.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Potable water is often in high demand and short supply following a natural disaster like the Haiti earthquake or Hurricane Katrina. In both of those instances, the disaster zones were near the sea, but converting salty seawater to potable fresh water usually requires a large amount of dependable electrical power and large-scale desalination plants — neither of which were available in the disaster areas.<br /><br />A new approach to desalination being developed by researchers at MIT and in Korea could lead to small, portable units that could be powered by solar cells or batteries and could deliver enough fresh water to supply the needs of a family or small village. As an added bonus, the system would also remove many contaminants, viruses and bacteria at the same time.<br /><br />The new approach, called ion concentration polarization, is described in a paper by Postdoctoral Associate Sung Jae Kim and Associate Professor Jongyoon Han, both in MIT’s Department of Electrical Engineering and Computer Science, and colleagues in Korea. The paper was published on March 21 in the journal <a href="http://www.nature.com/nnano/journal/vaop/ncurrent/abs/nnano.2010.34.html" target="_blank"><em>Nature Nanotechnology</em></a>.<br /><br />One of the leading desalination methods, called reverse osmosis, uses membranes that filter out the salt, but these require strong pumps to maintain the high pressure needed to push the water through the membrane, and are subject to fouling and blockage of the pores in the membrane by salt and contaminants. The new system separates salts and microbes from the water by electrostatically repelling them away from the ion-selective membrane in the system — so the flowing water never needs to pass through a membrane. That should eliminate the need for high pressure and the problems of fouling, the researchers say.<br /><br />The system works at a microscopic scale, using fabrication methods developed for microfluidics devices — similar to the manufacture of microchips, but using materials such as silicone (synthetic rubber). Each individual device would only process minute amounts of water, but a large number of them — the researchers envision an array with 1,600 units fabricated on an 8-inch-diameter wafer — could produce about 15 liters of water per hour, enough to provide drinking water for several people. The whole unit could be self-contained and driven by gravity — salt water would be poured in at the top, and fresh water and concentrated brine collected from two outlets at the bottom.<br /><br />That small size could actually be an advantage for some applications, Kim explains. For example, in an emergency situation like Haiti’s earthquake aftermath, the delivery infrastructure to get fresh water to the people who need it was largely lacking, so small, portable units that individuals could carry would have been especially useful.<br /><br />So far, the researchers have successfully tested a single unit, using seawater they collected from a Massachusetts beach. The water was then deliberately contaminated with small plastic particles, protein and human blood. The unit removed more than 99 percent of the salt and other contaminants. “We clearly demonstrated that we can do it at the unit chip level,” says Kim. The work was primarily funded by a grant from the National Science Foundation, as well as a SMART Innovation Centre grant<br /><br />While the amount of electricity required by this method is actually slightly more than for present large-scale methods such as reverse osmosis, there is no other method that can produce small-scale desalination with anywhere near this level of efficiency, the researchers say. If properly engineered, the proposed system would only use about as much power as a conventional lightbulb.<br /><br />Mark A. Shannon of the Center of Advanced Materials for the Purification of Water with Systems at the University of Illinois at Urbana-Champaign, who was not involved in this work, agrees with that assessment. In a News &amp; Views piece that accompanies the <em>Nature Nanotechnology</em> paper, he writes that the new system achieves “perhaps the lowest energy ever for desalinating microliters of water,” and when many of these micro-units are combined in parallel, as Kim and his co-authors propose, “it could be used to supply liters of water per hour using only a battery and gravity flow of water.” That meets a significant need, he says, since at present there are few efficient methods for small-scale desalination, both for emergencies and for use in remote areas in poor countries.<br /><br />Alex Iles, a research scientist at the University of Hull in Britain, says that while further testing must be done to establish long-term stability and fabrication techniques, “This is an elegant new concept for water desalination.” He says it is likely to produce a low-cost, low-maintenance system that could be “ideal for applications such as disaster relief.” When it was initially presented at a conference he attended last year, Iles says, “I thought it was probably the most significant new work at the entire conference, even though it was only a poster.”<br /><br />The basic principle that makes the system possible, called ion concentration polarization, is a ubiquitous phenomenon that occurs near ion-selective materials (such as Nafion, often used in fuel cells) or electrodes, and this team and other researchers have been applying the phenomenon for other applications such as biomolecule preconcentration. This application to water purification has not been attempted before, however.<br /><br />Since the separation occurs electrostatically, it doesn’t work for removing contaminants that have no electric charge. To take care of these remaining particles — mostly industrial pollutants — the researchers suggest the unit could be combined with a conventional charcoal filter system, thus achieving pure, safe drinking water through a single simple device.<br /><br />Having proved the principle in a single-unit device, Kim and Han plan to produce a 100-unit device to demonstrate the scaling-up of the process, followed by a 10,000-unit system. They expect it will take about two years before the system will be ready to develop as a product.<br /> <br />“After that,” says Kim, “we’ll know if it’s possible” for this to work as a robust, portable system, “and what problems might need to be worked on.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Taming the wild phonon]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/perfect-insulator-0322.html</link>
<story_id>15124</story_id>
<featured>0</featured>
<description><![CDATA[‘Particles’ of heat are everywhere, and usually a nuisance, but newly designed materials could help put them to good use.]]></description>
<postDate>Mon, 22 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100319141058-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100319141058-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100319141058-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Edwin L. Thomas, head of the Department of Materials Science and Engineering and the Morris Cohen Professor of Materials Science and Engineering, uses a Scanning Acoustic Microscope in MIT's Laboratory for Advanced Materials.]]></imageCaption>
</image>
<body><![CDATA[Researchers at MIT and elsewhere have succeeded in creating a synthetic crystal that can very effectively control the transmission of heat — stopping it in its tracks and reflecting it back. This advance could lead to insulating materials that could block the escape of heat more effectively than any present insulator.<br /><br />This crystal structure was built using alternating layers of silicon dioxide (the basis of the dielectric layers in most microchips) and a polymer material. The resulting two-component material successfully reflected phonons — vibrational waves that are the carriers of ordinary heat or sound, depending on their frequency. In this case, the phonons were in the gigahertz range — in other words, low-level heat. <br /><br />Edwin L. Thomas, head of MIT’s Department of Materials Science and Engineering and the Morris Cohen Professor of Materials Science and Engineering, was a co-author of a new paper, published on March 10 in the journal <em>Nano Letters</em>, that describes this creation of phononic crystals in the hypersonic range (that is, above the frequency range of sound, and thus can be considered in the range of heat).<br /><br />Phonons may sometimes be thought of as particles, and sometimes as vibrational waves, analogous to the dual wave and particle nature of light. Physically, the phonons are manifested as a wave of density variation passing through a material, like the wave of compression that travels along a child’s Slinky toy when you stretch it out and give one end a shove.<br /><br />Thomas says phonons, which exist in all solids, are usually a nuisance that must be disposed of with cooling systems. They have been “denigrated and ignored, but they could be the future star attraction if we can train them to do tricks for us.” Among other things, this could lead to highly efficient ways of scavenging heat that is now wasted, in everything from computers and cell phones to cars and power plants, in order to produce electricity. This latest research, funded by the National Science Foundation and its German equivalent, DFG, is still at the level of simple tricks, he says: “It’s a step on the path.” <br /><br />Phonons can be controlled through manufactured crystal-like structures. In this latest research, Thomas and his colleagues in Germany and Greece fabricated a “one-dimensional periodic” crystal structure, which means that although the material has three dimensions, its regularly varying molecular structure — in this case, alternating layers of two different materials — only varies along one direction, like a stack of vanilla and chocolate ice cream where the layers alternate. So if you look at a single layer, there’s just a uniform color, but if you drill through the stack, you find regularly alternating layers. When the spacing between similar layers matches the wavelength of the phonons, those phonons are blocked and reflected back.<br /><br />The phonons that are reflected from this newly developed material are in the range of low-frequency heat (since anything above absolute zero, or minus 273 degrees Celsius, is considered heat, which is just due to the movement of vibrational waves). Hence, this reflector currently only works at sub-freezing temperatures. Further work on decreasing the thickness of the layers could bring them closer to the range of a theoretical “perfect insulator” that could block heat of a certain frequency range in an ordinary room-temperature environment. And this could open up a host of potential applications.<br /><br />No material is ever going to be perfect, but even a material that reflects back a very high percentage of heat could be a big improvement over present insulators. For example, a shell of such material could be used to maintain the temperature in a package of delicate research instruments in a frigid environment.<br /><br />How far off are such applications? “It’s close, if you don’t worry about price,” Thomas says — which may be the case for some uses such as spacecraft, or instruments deployed in Antarctica. And as the technology develops and as production gets scaled up, prices could eventually come down far enough to enable more widespread applications.<br /> <br />Ihab El-Kady, a researcher at Sandia National Laboratories, says that while much current research on phonons involves the creation of two- or three-dimensional crystals, which may have greater long-term applications, there are some advantages to studying one-dimensional crystals as Thomas and his co-authors did in this case. “One-D systems are still preferable for their ease of fabrication, and can offer particular insight into the basic physical mechanics of phononic crystals,” he says. “In that light, this paper represents a novel and insightful tool” for analyzing fundamental wave phenomena, as well as the interactions between phonons and other particles such as photons.<br /><br />Most early work on phonons dealt with sound-wave frequencies, which can be manipulated using larger crystal structures, but advances in nanotechnology have made it possible to create materials with structures small enough to handle the high-frequency, short-wavelength phonons associated with heat. <br /><br />The best way to understand the enormous potential of devices that control phonons is by comparing them to devices that control electrons and photons, says Thomas. He explains that our growing understanding of electrons and photons — which carry electricity and light, respectively — has led to decades of technological innovation, including the invention of lasers, transistors, photovoltaic cells and microchips. These basic inventions, in turn, made possible most of the devices that define modern life, including cell phones, computers, DVD players and flat-screen TVs. Now there are a lot of people trying to understand phonons, he says, which could lead to a similar proliferation of new — and impossible to predict — technologies.<br /><br />As a result, Thomas says, the field of phononics “has the potential to rocket off.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Mapping Venus]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/venus-mapping-0322.html</link>
<story_id>15126</story_id>
<featured>0</featured>
<description><![CDATA[New analysis supports theory that Venus’ surface evolved through extreme makeover, not plate tectonics]]></description>
<postDate>Mon, 22 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100319151322-1.png</thumbURL>
<smallURL width='140' height='113'>http://web.mit.edu/newsoffice/images/article_images/w140/20100319151322-1.jpg</smallURL>
<fullURL width='368' height='299'>http://web.mit.edu/newsoffice/images/article_images/20100319151322-1.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA Jet Propulsion Laboratory]]></imageCredits>
<imageCaption><![CDATA[This computer-generated view of the surface of Venus was created from radar images taken during NASA’s Magellan mission during the 1990s. The images suggest that the Venus surface evolves through a periodic resurfacing process, possibly caused by volcanic activity.]]></imageCaption>
</image>
<body><![CDATA[Venus and Earth have long been thought of as sister planets. Given its similar size and proximity to Earth in the inner Solar System, Venus might seem like a promising candidate for having a surface that evolves through a tectonic process similar to what occurs on Earth, where rigid plates slowly shift across the underlying mantle. <br /><br />But a recent analysis by Peter James, a graduate student in the Department of Earth, Atmospheric and Planetary Sciences, highlights the fact that Earth’s plate tectonics seem to be the exception rather than the rule for rocky planets like Venus, Mars and Mercury. <br /><br />James provides new evidence that the generation and recycling of the surface on Venus occurs through a process that is actually quite different from what happens on Earth. His finding supports a theory that first arose in the early 1990s, when NASA’s Magellan spacecraft orbited Venus and took radar images of the planet’s surface. Before Magellan, most scientists assumed that the surface of Venus was influenced by some form of plate tectonics or volcanism.<br /><br />The Magellan images revealed a distribution of craters that suggest that most of Venus’ surface was formed around the same time — about 500 million years ago, which is young considering that the planet’s age is estimated at about 4.6 billion years. As a result of this uniform age of the surface, scientists hypothesized that the Venus surface is not made of moving plates like Earth, nor is it inactive like the moon. Instead it evolves through a periodic resurfacing process, possibly caused by volcanic activity. <br /><br /><strong>Probing the crust</strong><br /><br />Geologists study features of a planet’s crust, such as its thickness and composition, for clues about that planet’s history. These clues shed light on the physical processes that made the crust, which is usually produced by partial melting of mantle material.  <br /><br />To study Venus’ crust, James used gravity and topography data collected by Magellan between 1990 and 1994. Analyzing these data, James mapped the thickness of the planet’s crust, which he calculated to be about 30 kilometers (Earth’s is about 20 kilometers, on average). He could identify regions where Venus’ convecting mantle is pushing or pulling on its crust as the planet cools.<br /><br />While these results provide a better picture of the Venus crust, what is most compelling about the analysis, which James presented on March 1 at the Lunar and Planetary Science Conference, is the discovery that there are no large mass concentrations, or “mascons,” buried beneath the surface of Venus.<br /><br />Existing on Mars and the moon, mascons are gravity anomalies that correspond to large craters and basins created billions of years ago by massive impacts from large meteoroids. These mascons exert a slightly stronger gravitational pull — detected by spacecraft or satellites — than that of a smooth surface. While the process of mascon formation is not well understood, James explained that the extra gravitational pull likely comes from two sources: dense rock in the craters from volcanic flow and the placement of denser mantle material near the surface.<br /><br />James expected to find remnants of these crustal structures on Venus, given that they are prominent features on Mars and the moon. He believes that the absence of mascons is consistent with the idea that the Venus surface experienced some sort of “catastrophic overturning” at least 500 million years ago. “If the mascons were erased in the event 500 million years ago, that would require a mechanism that more thoroughly reworks the crust,” he explained.<br /><br />Brown University geologist Marc Parmentier agreed with James that the lack of mascons indicates that some sort of mechanism — perhaps large-scale volcanic activity — periodically creates a new surface on Venus. <br /><br />He praised the analysis for ensuring that research about Venus remains an active area in planetary science, which is currently heavily focused on Mars and the moon. “His work lets us continue to address one of the questions of Venus, which is how this so-called resurfacing process took place,” he said. <br /><br />James hopes to address this question in future research by using more finite element modeling to understand how mascons are formed and evolve. He said that NASA’s upcoming GRAIL mission to the moon will gather unprecedented gravity data that will provide some basis for comparing the lunar and Venus crusts.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Climate sensitivity]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/explained-climate-sensitivity.html</link>
<story_id>15114</story_id>
<featured>0</featured>
<description><![CDATA[If we double the Earth’s greenhouse gases, how much will the temperature change? That’s what this number tells you.]]></description>
<postDate>Fri, 19 Mar 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100317170739-1.png</thumbURL>
<smallURL width='140' height='120'>http://web.mit.edu/newsoffice/images/article_images/w140/20100317170739-1.jpg</smallURL>
<fullURL width='368' height='316'>http://web.mit.edu/newsoffice/images/article_images/20100317170739-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[<em>This is the second part of an “Explained” on climate change. <a href="http://web.mit.edu/newsoffice/2010/explained-radforce-0309.html" target="_blank">Part one</a> dealt with radiative forcing.</em><br /><br />Climate sensitivity is the term used by the Intergovernmental Panel on Climate Change (IPCC) to express the relationship between the human-caused emissions that add to the Earth’s greenhouse effect — carbon dioxide and a variety of other greenhouse gases — and the temperature changes that will result from these emissions.<br /><br />Specifically, the term is defined as how much the average global surface temperature will increase if there is a doubling of greenhouse gases (expressed as carbon dioxide equivalents) in the air, once the planet has had a chance to settle into a new equilibrium after the increase occurs. In other words, it’s a direct measure of how the Earth’s climate will respond to that doubling.<br /><br />That value, according to the most recent IPCC report, is 3 degrees Celsius, with a range of uncertainty from 2 to 4.5 degrees.<br /><br />This sensitivity depends primarily on all the different feedback effects, both positive and negative, that either amplify or diminish the greenhouse effect. There are three primary feedback effects — clouds, sea ice and water vapor; these, combined with other feedback effects, produce the greatest uncertainties in predicting the planet’s future climate. <br /><br />With no feedback effects at all, the change would be just 1 degree Celsius, climate scientists agree. Virtually all of the controversies over climate science hinge on just how strong the various feedbacks may be — and on whether scientists may have failed to account for some of them. <br /><br />Clouds are a good example. Clouds can have either a positive or negative feedback effect, depending on their altitude and the size of their water droplets. Overall, most scientists expect this net effect to be positive, but there are large uncertainties. <br /><br />“There is still lots of uncertainty in what the climate sensitivity is,” says Andrei Sokolov, a research scientist in MIT’s Center for Global Change Science, who has been doing research on climate sensitivity for many years. “Feedback is what’s driving things,” he says. <br /><br />It is important to note that climate sensitivity is figured on the basis of an overall doubling, compared to pre-industrial levels, of carbon dioxide and other greenhouse gases. But the temperature change given by this definition of climate sensitivity is only part of the story. The actual increase might be greater in the long run because greenhouse gas levels in the atmosphere could more than double without strong policies to control emissions. But in the short run, the actual warming could be less than suggested by the climate sensitivity, since due to the thermal inertia of the ocean, it may take some time after a doubling of the concentration is reached before the climate reaches a new equilibrium.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Bacteria divide like clockwork]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/clockwork-bacteria-0319.html</link>
<story_id>15122</story_id>
<featured>0</featured>
<description><![CDATA[MIT researchers show how circadian rhythms in bacteria control their rate of reproduction.]]></description>
<postDate>Fri, 19 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100318154924-2.png</thumbURL>
<smallURL width='140' height='138'>http://web.mit.edu/newsoffice/images/article_images/w140/20100318154924-2.jpg</smallURL>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100318154924-2.jpg</fullURL>
<imageCredits><![CDATA[Image courtesy of micro*scope]]></imageCredits>
<imageCaption><![CDATA[A colony of cyanobacteria]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100318154649-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Physics Professor Alexander van Oudenaarden, left, works in his lab with physics graduate student Bernardo Pando.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[It’s well established that critical human body functions, including sleep, hormone production and regulation of body temperature, follow a circadian (24-hour) cycle. These genetically programmed patterns stay in effect even under isolation from the naturally occurring daily light-dark cycles of the sun, and are found in nearly all organisms — animals, plants, fungi and even some bacteria.<br /><br />Although bacteria don’t “sleep” in the same way humans do, a type of bacteria known as cyanobacteria undergoes daily cycles of activity and rest. These bacteria depend on sunlight to perform photosynthesis, so they have evolved to be most active in daylight.<br /><br />A new study of cyanobacteria by a team of researchers at MIT and the University of California at San Diego has revealed, for the first time, how those circadian rhythms control the bacteria’s rate of cell division (their method of reproduction) in single cells. “These cells have to keep dividing, and the circadian oscillator regulates when they divide,” says Bernardo Pando, an MIT graduate student in physics and one of the lead authors of a paper describing the findings in the March 18 online edition of <em>Science</em>.<br /><br />In multicellular animals, including humans, cell division is critical for renewal and repair, while out-of-control cell division leads to cancer, so “understanding how cells are dividing is really of fundamental importance,” says Susan Golden, <a href="http://biology.ucsd.edu/faculty/sgolden.html" target="_blank">professor of molecular biology</a> at the University of California at San Diego and an author of the paper.<br /><br /><strong>Divide, rest, repeat</strong><br /><br />In 1998, Golden and other researchers identified three so-called “clock proteins” that control circadian rhythms in cyanobacteria. There has been some evidence that the circadian clock regulates cell division, but the exact relationship was unclear. <br /><br />The MIT-UCSD team, led by MIT biophysics professor Alexander van Oudenaarden, found that under conditions of moderate constant light, the cyanobacteria undergo cell division about once per day, and the divisions take place mostly at the midpoint of the 24-hour cycle. <br /><br />The researchers then sped up the cell cycle by boosting the intensity of light. That enabled the cells to photosynthesize more, increasing the amount of energy available to them. The cells did divide more frequently, but in a pattern still linked to the circadian clock — they divided a quarter of the way into the cycle, and again three-quarters into the cycle.<br /><br />The team also showed that in all conditions, the cyanobacteria enter a resting phase about 19 hours into the circadian cycle, after which they will not divide until the next cycle begins.<br /><br />Key to the new finding is a technique the researchers devised to study circadian rhythms in single cells, says van Oudenaarden, the senior author of the paper. The team tracked individual cells over a week. Proteins that govern the circadian clock were tagged with yellow fluorescent protein, so each cell’s position in the 24-hour cycle could be pinpointed. Cells were also photographed every 40 minutes, so researchers could see when they divided.<br /><br />The single-cell tracking technique could also be used to reveal linkages between the circadian clock and other cyclical cell process, such as metabolism. “The circadian cycle has to ride herd on all these other cycles and make sure they stay in synchrony and that cell activity is not chaotic,” says Golden.<br /><br />Golden is planning follow-up studies in cyanobacteria, but yeast and mammalian cells could also be logical targets for such studies, says van Oudenaarden.<br /><br />In a recent paper published in the journal <em>Cell</em>, van Oudenaarden and Golden reported the molecular mechanism of how clock proteins control the cell cycle in cyanobacteria. The proteins (KaiA, KaiB and KaiC) control the action of another protein called FtsZ, preventing it from going to the middle of the cell and forming a ring necessary for cell division.<br /><br />Taken together, the papers are “excellent work connecting cell division to the circadian clock,” says Paul Hardin, a biologist at Texas A&amp;M University who studies circadian rhythms in fruit flies.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Homework copying can turn As into Cs, Bs into Ds]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/homework-copying-0318.html</link>
<story_id>15113</story_id>
<featured>0</featured>
<description><![CDATA[MIT physics educators investigate a bad habit’s perils — and demonstrate a solution.]]></description>
<postDate>Thu, 18 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100317144434-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100317144434-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100317144434-1.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100317144434-2.jpg</fullURL>
<imageCaption><![CDATA[Former postdoctoral fellow Young-Jin Lee, left, and David E. Pritchard, the Cecil and Ida Green Professor of Physics]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Copying a few answers from another student’s math or science homework assignment occurs much more frequently than copying during examinations or plagiarism on term papers. It is rarely prosecuted by discipline committees and is regarded by many American college students as either not cheating at all or simply a minor infraction. Now educators at MIT have shown that homework copying is associated with greatly decreased learning — and have developed changes in instructional format that reduced copying by a factor of four in certain physics classes at MIT. <br /><br />This research was conducted by the Research in Learning, Assessment, and Tutoring Effectively (RELATE) program headed by David E. Pritchard, the Cecil and Ida Green Professor of Physics. By analyzing records of student submissions to Mastering Physics, an online homework and tutorial system, a team led by graduate student David Palazzo developed algorithms to detect copied answers based on earlier work led by Postdoctoral Fellow Rasil Warnakulasooriya. Postdoctoral Fellow Young-Jin Lee also contributed to the research, <a href="http://prst-per.aps.org/toc/PRSTPER/v6/i1" target="_blank">published today online</a> in <em>Physical Review Special Topics: Physics Education Research</em>.<br /><br />The group investigated the effect of homework copying on students’ performance. It found that the copying of problems that require algebraic responses correlated with two letter grades’ worse performance on problems demanding similar responses on a final exam (but found that copying did not adversely affect grades on conceptual questions). This decline caused repetitive copiers — students who copy over 30 percent of their homework problems — to have over three times the failure rate of the rest of the students in spite of their starting the semester with equal ability in math and physics. <br /><br />According to Lee, now assistant professor of educational technology at the University of Kansas, “The decrease of copiers’ relative performance over the semester is as strong as anything in the education literature. Since the copiers do learn physics topics on which they don’t copy the homework, it strongly implies that copying caused their declining relative performance on the algebraic problems later in the semester.”<br /><strong><br />Tracking a bad habit</strong><br /><br />The copying showed surprisingly strong temporal patterns. Students who copied little or none of their homework had completed about half of their weekly assignment two nights before it was due, whereas repetitive copiers had done only about 15 percent. More dramatically, the rate of copying increased notably over the semester and was three times higher after midterm exams than during the first three weeks of the semester. “It took them about three weeks to establish their networks,” said Palazzo, now a professor at the U.S. Military Academy. <br /><br />Homework copying was detected based on the elapsed time between when a student opened the problem in his browser and the time when he had correctly submitted all the answers. When the elapsed time was too short for the average student to read the problem and enter the several required answers, the problem was regarded as having been copied. <br /><br />By measuring actual copying, this work provides a checkpoint for the large body of work on academic dishonesty based on anonymous surveys of students. The actually observed rate was about 50 percent higher than on a similar anonymous survey given to the MIT students. In addition, survey data combined with actual copying patterns confirmed three demographic indicators found in previous self-reported work: men copied significantly more than women, business majors copied much more than either scientists or engineers, and those most interested in obtaining a grade copied more than those most interested in obtaining an education. In addition, it appears that students interested merely in obtaining a passing grade delay starting serious work and resort to copying under pressure of the deadline. <br /><br />In surveys, students nationally report both more academic dishonesty and more moral tolerance for it than do students at MIT. “It’s hard to escape the conclusion that homework copying is an even worse problem nationally than in the worst semester we studied at MIT,” said Palazzo. The surveys also reveal that more copying is reported for written than for online homework; at MIT, students reported nearly twice as much copying of written homework as of online homework.<br /><br /><strong>A remedy for copying</strong><br /><br />The MIT group observed the overall copy rate decline by a factor of four over three years as changes were made to the instructional format. The largest reduction of copying occurred when Physics I: Classical Mechanics (8.01) was changed from a lecture-recitation format to the more intimate technology-enabled active learning (TEAL) format, which features more personal contact with teachers and teaching assistants (and a student-to-staff ratio below 20). A further factor of two reduction occurred in Physics II: Electricity and Magnetism (8.02) when the interface in the course’s tutoring program was changed to make copying more difficult and the grading system changed from pass/no record to A, B, C/no record.<br /><br />Pritchard concludes from his research that copying written homework is a serious cause of course failure nationally, especially in large lecture-recitation courses. “We came upon homework copying through our research on learning in an online environment, rather than through moral concern,” Pritchard says. “But our results are so strong that they place a moral imperative on teachers to confront homework copying and to reduce it. Fortunately, we found some changes that dramatically reduce copying without turning teachers into policemen.”<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Web sites that can take a punch]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/web-attacks-0317.html</link>
<story_id>15106</story_id>
<featured>0</featured>
<description><![CDATA[By preventing web applications from deviating from their normal behavior, a new MIT system can keep them online even during a cyberattack.]]></description>
<postDate>Wed, 17 Mar 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100315144338-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100315144338-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100315144338-1.jpg</fullURL>
<imageCredits><![CDATA[Image: istockphoto]]></imageCredits>
</image>
<body><![CDATA[The recent, well-publicized cyberattack on Google was just the latest skirmish in a long war. And like most long wars, this one features an arms race, as hackers seek out new security holes, and web site administrators try to close them.<br /><br />Systems for detecting attacks against networked computers are commercially available, and academic and industrial researchers are constantly improving them. But when a web site is under attack, its only viable defense may be to take its servers offline, which, in the short term, can cost it money in lost revenue and productivity and, in the long term, could hurt its credibility. Indeed, knocking a site offline may be an attackers’ sole intention.<br /><br />MIT researchers have developed a system to keep web servers — or, for that matter, any Internet-connected computers — running even when they’re under attack. The work was funded largely by the U.S. Defense Department’s Defense Advanced Research Projects Agency (DARPA), and in a pair of tests whose thoroughness is unusual in academia, DARPA hired a group of computer security professionals outside MIT to try to bring down a test network protected by the new system. In both tests, says Martin Rinard, the professor of electrical engineering and computer science who led the research, the system exceeded all the performance criteria that DARPA set for it.<br /><br />The MIT system was developed by a host of researchers, including not only Rinard but Jeff Perkins, a research scientist at MIT’s Computer Science and Artificial Intelligence Lab, Postdoctoral Fellow Stelios Sidiroglou-Douskos and Professor Michael Ernst, who has since moved to the University of Washington. During normal operation, it monitors the programs running on an Internet-connected computer to determine their normal range of behavior, and during an attack, it simply refuses to let them wander outside that range.<br /><br />To take a simple example, suppose that a program running on a web server routinely stores data in one of two memory locations — call them A and B. During an attack, malicious code tries to trick the program into storing data at location C instead. The MIT system won’t let it: instead, it sends the data to either location A or location B.<br /><br />Of course, the data may not be of a type that belongs at either of those locations. And the system will modify behaviors that could be even more disruptive than data storage. But in sites with large banks of servers, the MIT system gets several chances to find the best response to an attack. If storing at location A causes one server in the bank to crash, the MIT system will tell the other servers to store it at location B, instead.<br /><br />“The idea is that you’ve got hundreds of machines out there,” Rinard says. “We’re saying, ‘Okay, fine, you can take out six or 10 of my 200 machines.’” But, he adds, “by observing what happens with the executions of those six or 10 machines, we’ll be able to deploy patches out to protect the rest of the machines.” The entire process of recognizing an attack, testing a number of countermeasures and deploying the most effective ones can take a matter of seconds.<br /><br /><strong>Baptism by fire</strong><br /><br />In the first of DARPA’s two field tests, engineers at a computer security firm — the so-called red team — were given the code for the MIT defense system. (In the real world, a company that marketed such a system would make every effort to keep its code secret, but Rinard says that it’s standard practice in the security field to consider the worst-case scenario.) The red team had several months in which to devise attacks against a hypothetical network protected by the system. During the test itself, no malicious code was allowed to execute on the protected computers, and in 70 percent of cases, the MIT system kept the applications running on those computers from going down. DARPA also set performance goals for the system, such as the amount of extra processing power it required, and the extent to which it altered the applications’ normal operation. In all cases, the system was well within DARPA’s prescribed limits.<br /><br />The <a href="http://groups.csail.mit.edu/pag/pubs/automatic-patching-sosp2009-abstract.html" target="_blank">first red-team exercise</a> considered cases in which hackers tried to infect computers with malicious code, and the MIT researchers presented the results of the test at the Association for Computing Machinery’s Symposium on Operating Systems Principles last fall. A second red-team exercise, testing an updated version of the defense system that the MIT researchers developed together with defense contractor BAE Systems, concluded at the end of January. That test evaluated the system’s ability to handle a different kind of attack, which seeks to circumvent security checks that web applications typically perform to ensure that users have permission to access protected information. Although the researchers are still sorting through the data from that test, Sidiroglou-Douskos says that the system’s success rate in keeping applications up and running rose from 70 percent to 90 percent.<br /><br />Angelos Keromytis, an associate professor of computer science at Columbia University, who works on related techniques for combating cyberattacks, says that the MIT approach is “very original,” but cautions that Web developers may be reluctant to adopt it anytime soon. “They’re wary of a system that changes another system automatically,” Keromytis says. “When they manually make changes to their systems, they break them, so they think that automatically doing it is going to be worse.” Keromytis points out, however, that while DARPA has run a number of red-team exercises evaluating new technologies in a range of areas, “This is probably one of the most successful exercises that I have seen.” The mere fact that DARPA was willing to spend so much money testing the system, Keromytis says, indicates that “they think it’s close enough to a rough prototype that works, which is more than one can say for most academic research.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Weather in a Tank]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/weather-tank-0317.html</link>
<story_id>15112</story_id>
<featured>0</featured>
<description><![CDATA[A curriculum built around a rotating-tank experiment could improve weather and climate education]]></description>
<postDate>Wed, 17 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100316163559-1.png</thumbURL>
<smallURL width='140' height='104'>http://web.mit.edu/newsoffice/images/article_images/w140/20100316163559-1.jpg</smallURL>
<fullURL width='368' height='275'>http://web.mit.edu/newsoffice/images/article_images/20100316163559-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: John Marshall, EAPS]]></imageCredits>
<imageCaption><![CDATA[The Weather in a Tank curriculum includes several rotating tank experiments, that are designed to help students understand how Earth's atmosphere
and oceans work - an image from the Dye stirring experiment is shown here.]]></imageCaption>
</image>
<body><![CDATA[In recent years, U.S. undergraduates have shown an increasing interest in introductory meteorology, oceanography and climate classes. But many students find it difficult to grasp the non-intuitive nature of rotating fluids, which is critical to understanding how weather systems and climate work. Part of the problem, it turns out, is that instructors usually have to teach these abstract concepts using only equations or computer simulations because of the limited resources available for lab experiments. <br /><br />That may be about to change, thanks to the work of two educators from the Department of Earth, Atmospheric and Planetary Sciences. For nearly a decade, Lodovica Illari, an EAPS senior lecturer, and John Marshall, professor of atmospheric and oceanic sciences, have been developing an undergraduate weather and climate curriculum that’s now being adopted by dozens of schools — and could have a wide impact on science education at many levels.<br /><br />Known as “Weather in a Tank,” the experiment-based curriculum was designed by Illari and Marshall in 2001 after they began offering an introductory weather and climate class that would also fulfill their students’ lab requirements. <br /><br />Since 2006, the curriculum has been tested in a project funded by the National Science Foundation (NSF), which involves MIT and five other universities. The intent was to bridge the gap between real-world weather phenomena and the theories and equations that describe those phenomena. Illari says that we should think of lab experiments as the third leg of a three-legged pedagogical stool that includes observation and theory.<br /><br /><strong>Demonstrating fluid behavior </strong><br /><br />The centerpiece of Weather in a Tank is the equipment: an acrylic tank atop a rotating turntable on a portable cart. Experiments conducted in the rotating tank demonstrate the fluid dynamics of geophysical systems — how the motion of water and air influences Earth’s climate.<br /><br />About the price of a high-end laptop, according to Illari, the equipment can help explain a range of topics related to the atmosphere, oceans and climate, including how Earth’s rotation creates weather systems that play a role in keeping the tropics warm and the poles cool.<br /><br />To simulate these processes, a bucket of ice is placed in the center of the rotating tank of water. The ice creates a range of temperatures inside the cylindrical tank — colder near the bucket, warmer farther away. This temperature gradient is analogous to that of Earth, which is colder near the poles. As the turntable rotates, its motion causes eddies, or small currents, inside the tank. A few drops of food coloring can help students see this. <br /><br />Because of the temperature gradient, the eddies act like atmospheric weather systems. They carry warm water from the edge of the tank (which simulates the equator) toward the bucket of ice at the center. Simultaneously, they carry cold fluid from the bucket (which simulates the poles) toward the periphery of the tank. Students can then change certain parameters, such as the rotation rate, and see how those changes affect the eddies’ circulation at different latitudes. About a dozen of these experiments have been developed, along with associated curriculum materials.<br /><br /><strong>Making the abstract real</strong><br /><br />Illari cautions that the Weather in a Tank curriculum isn’t designed to avoid difficult equations or computer simulations. Instead, it’s meant to provide lab experiments that help students understand firsthand the processes that the equations and simulations describe. “We think it’s a mistake to give students the phenomena without the tools to understand,” she says. <br /><br />To date, about 25 colleges and universities have either asked for equipment plans, built their own turntables or bought the portable system from the manufacturer that made the original prototype. <br /><br />Todd Ellis, a meteorology professor at SUNY Oneonta who has used the equipment in his classes, says that teaching fluid dynamics can be hard because the topic is very theoretical and uses complicated math to describe unusual phenomena. While these concepts aren’t difficult for oceanography or meteorology majors, they can be tricky for non-majors learning about the equations for the first time.<br /><br />Ellis agreed that nothing compares to observing how fluids behave in real time. “We spend so much time letting the computers do the figures, and it’s so easy to lose track that these are real substances — that the oceans and atmospheres are fluids,” he says. “You can partly lose the forest for the trees when you always rely on computers to tell you what’s going to happen.”<br /><br />When the NSF project concludes in July, Illari plans to enhance the curriculum by adding experiments. She also intends to try to drum up interest in it among high schools and other educational institutions.<br /><br />Illari is already at work with EAPS principal research scientist Chris Hill and other collaborators on another NSF-funded project, which will use cloud computing — the outsourcing of computational tasks to networked servers — to help people conduct virtual experiments on their computers. The goal of the Cloud-Computing Infrastructure and Technology for Education (CITE) project is to bring the Weather in a Tank curriculum to a larger audience by allowing schools to incorporate the tank experiments into their online curricula. A prototype is currently being tested in MIT undergraduate and graduate courses.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Ray Stata to deliver 2010 Commencement address]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/commencement-speaker.html</link>
<story_id>15111</story_id>
<featured>0</featured>
<description><![CDATA[A ‘loyal friend’ of the Institute for more than 50 years, the alumnus will speak to graduates in June]]></description>
<postDate>Tue, 16 Mar 2010 20:24:08 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100316162522-1.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100316162522-1.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100316162522-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: MIT Corporation]]></imageCredits>
<imageCaption><![CDATA[Raymond S. Stata ’57, chairman and cofounder of Analog Devices Inc., will deliver the address at MIT’s 144th Commencement exercises on Friday, June 4.]]></imageCaption>
</image>
<body><![CDATA[Raymond S. Stata ’57, chairman and cofounder of Analog Devices Inc., a leading manufacturer of signal-processing chips that power digital products, will deliver the address at MIT’s 144th Commencement exercises on Friday, June 4, in Killian Court.<br /><br />A pioneer in the semiconductor industry, Stata exemplifies the kind of leader that MIT has a tradition of cultivating: a disciplined, modest individual whose passions for engineering, entrepreneurship and education have inspired a lifelong commitment to excellence and public service. Much of that commitment has been to his alma mater, where Stata received both his SB and SM in electrical engineering.<br /><br />“Ray Stata represents the essence of MIT,” said President Susan Hockfield. “He is an engineer’s engineer — an irrepressible problem-solver, a brilliant entrepreneur and an outstanding citizen of the larger community and of MIT. His analytical rigor, entrepreneurial spirit, dedication to hard work and philanthropic vision will inspire our new graduates and their families.”<br /><br />Elected to the MIT Corporation in 1984, Stata remains a life member emeritus, as well as a member of the Development Committee. From 1987 to 1988, he served as president of the MIT Alumni Association. In 1997, he and his wife, Maria, made a generous financial gift to the Institute that enabled the construction of the Ray and Maria Stata Center, which opened in 2004. <br /><br />“While much has changed over my 50-year relationship with MIT, the fundamentals that make the MIT community so unique and important have remained very much the same,” Stata said. “I look forward to sharing my perspectives, both as an insider and an outsider, on the enormous impact which MIT has had on society, on its graduates and on me personally.”  <br /><br />Alex Hamilton Chan, president of the Graduate Student Council and member of the Commencement Committee, expressed delight that “this loyal friend of MIT” will deliver the commencement address. “Ray Stata is a successful entrepreneur, engineer and a fellow nerd,” he said. “He has given MIT so much — his time and money, and now, pearls of wisdom for our graduating class.” <br /><br /><strong>A storied career</strong><br /><br />Born in rural Pennsylvania, Stata founded Analog Devices, a Norwood, Mass.-based manufacturer of high-performance integrated circuits used in analog and digital signal processing applications, with MIT classmate Matthew Lorber ’56, SM ’58, in 1965. Stata served as president of the company from 1971 to 1991 and CEO from 1973 to 1996. He was named chairman of the company’s board of directors in 1973. <br /><br />Analog Devices has received much attention in recent years for the microelectromechanical devices it makes for the controllers of Nintendo’s wildly popular Wii gaming console.<br /><br />Stata is also the founder of Stata Venture Partners, a venture-capital firm that invests in technology start-ups, including Providence-based NABsys Inc., a company that recently received $7 million from Stata Venture Partners to develop a DNA sequencing technology based on semiconductor technology.<br /><br />Throughout his career, Stata has remained committed to key education initiatives, advocating that engineering education and university research funding are a shared responsibility of government and industry. In 1977, he co-founded the Massachusetts High Technology Council, a nonprofit lobbying organization composed of CEOs from the state's top technology employers. He remains a member of the board of directors and is co-chair of an MHTC initiative known as the Massachusetts Science, Technology, Engineering and Mathematics (STEM) Collaborative, which is dedicated to nurturing interest in math and science among K-12 students. <br /><br />A member of the American Academy of Arts and Sciences and the National Academy of Engineering, Stata was the recipient of the Institute for Electrical and Electronics Engineers (IEEE) Founder's Medal in 2003. He holds a number of honorary doctorate degrees from various universities. <br /><br />Stata is the latest in a long line of accomplished alumni invited to speak at commencement, including U.S. Federal Reserve Board Chairman Ben Bernanke PhD ’79 (2006), Qualcomm Co-Founder and Chairman Irwin Jacobs SM '57, ScD '59 (2005), and former U.N. Secretary General Kofi Annan SM ’72 (1997).<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Regression analysis]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/explained-reg-analysis-0316.html</link>
<story_id>15105</story_id>
<featured>0</featured>
<description><![CDATA[Sure, it’s a ubiquitous tool of scientific research, but what exactly is a regression, and what is its use?]]></description>
<postDate>Tue, 16 Mar 2010 04:00:03 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100315144150-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100315144150-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100315144150-1.jpg</fullURL>
</image>
<body><![CDATA[Regression analysis. It sounds like a part of Freudian psychology. In reality, a regression is a seemingly ubiquitous statistical tool appearing in legions of scientific papers, and regression analysis is a method of measuring the link between two or more phenomena. <br /><br />Imagine you want to know the connection between the square footage of houses and their sale prices. A regression charts such a link, in so doing pinpointing “an average causal effect,” as MIT economist Josh Angrist and his co-author Jorn-Steffen Pischke of the London School of Economics put it in their 2009 book, “Mostly Harmless Econometrics.” <br /><br />To grasp the basic concept, take the simplest form of a regression: a linear, bivariate regression, which describes an unchanging relationship between two (and not more) phenomena. Now suppose you are wondering if there is a connection between the time high school students spend doing French homework, and the grades they receive. These types of data can be plotted as points on a graph, where the x-axis is the average number of hours per week a student studies, and the y-axis represents exam scores out of 100. Together, the data points will typically scatter a bit on the graph. The regression analysis creates the single line that best summarizes the distribution of points. <br /><br />Mathematically, the line representing a simple linear regression is expressed through a basic equation: Y = a<sub>0</sub> + a<sub>1</sub> X. Here X is hours spent studying per week, the “independent variable.” Y is the exam scores, the “dependent variable,” since — we believe — those scores depend on time spent studying. Additionally, a<sub>0</sub> is the y-intercept (the value of Y when X is zero) and a<sub>1</sub> is the slope of the line, characterizing the relationship between the two variables.<br /><br />Using two slightly more complex equations, the “normal equations” for the basic linear regression line, we can plug in all the numbers for X and Y, solve for a<sub>0</sub> and a<sub>1</sub>, and actually draw the line. That line often represents the lowest aggregate of the squares of the distances between all points and itself, the “Ordinary Least Squares” (OLS) method mentioned in mountains of academic papers. <br /><br />To see why OLS is logical, imagine a regression line running 6 units below one data point and 6 units above another point; it is 6 units away from the two points, on average. Now suppose a second line runs 10 units below one data point and 2 units above another point; it is also 6 units away from the two points, on average. But if we square the distances involved, we get different results: 6<sup>2</sup> + 6<sup>2</sup> = 72 in the first case, and 10<sup>2</sup> + 2<sup>2</sup> = 104 in the second case. So the first line yields the lower figure — the “least squares” — and is a more consistent reduction of the distance from the data points. (Additional methods, besides OLS, can find the best line for more complex forms of regression analysis.) <br /><br />In turn, the typical distance between the line and all the points (sometimes called the “standard error”) indicates whether the regression analysis has captured a relationship that is strong or weak. The closer a line is to the data points, overall, the stronger the relationship. <br /><br />Regression analysis, again, establishes a correlation between phenomena. But as the saying goes, correlation is not causation. Even a line that fits the data points closely may not say something definitive about causality. Perhaps some students do succeed in French class because they study hard. Or perhaps those students benefit from better natural linguistic abilities, and they merely enjoy studying more, but do not especially benefit from it. Perhaps there would be a stronger correlation between test scores and the total time students had spent hearing French spoken before they ever entered this particular class. The tale that emerges from good data may not be the whole story. <br /><br />So it still takes critical thinking and careful studies to locate meaningful cause-and-effect relationships in the world. But at a minimum, regression analysis helps establish the existence of connections that call for closer investigation.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Self-assembling computer chips]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/self-assembly-0316.html</link>
<story_id>15107</story_id>
<featured>0</featured>
<description><![CDATA[Molecules that arrange themselves into predictable patterns on silicon chips could lead to microprocessors with much smaller circuit elements.]]></description>
<postDate>Tue, 16 Mar 2010 04:00:02 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100315160923-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100315160923-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100315160923-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Yeon Sik Jung and Joel Yang]]></imageCredits>
<imageCaption><![CDATA[MIT researchers coaxed tiny, chainlike molecules to arrange themselves into complex patterns, like this one, on a silicon chip. Previously, self-assembling molecules have required some kind of template on the chip surface — either a trench etched into the chip, or a pattern created through chemical modification. But the MIT technique instead uses sparse silicon “hitching posts.” The molecules attach themselves to the posts and spontaneously assume the desired patterns.]]></imageCaption>
</image>
<body><![CDATA[The features on computer chips are getting so small that soon the process used to make them, which has hardly changed in the last 50 years, won’t work anymore. One of the alternatives that academic researchers have been exploring is to create tiny circuits using molecules that automatically arrange themselves into useful patterns. In a <a href="http://www.nature.com/nnano/journal/vaop/ncurrent/abs/nnano.2010.30.html" target="_blank">paper that appeared Monday</a> in <em>Nature Nanotechnology</em>, MIT researchers have taken an important step toward making that approach practical.<br /><br />Currently, chips are built up, layer by layer, through a process called photolithography. A layer of silicon, metal, or some other material is deposited on a chip and coated with a light-sensitive material, called a photoresist. Light shining through a kind of stencil — a “mask” — projects a detailed pattern onto the photoresist, which hardens where it’s exposed. The unhardened photoresist is washed away, and chemicals etch away the bare material underneath.<br /><br />The problem is that chip features are now significantly smaller than the wavelength of the light used to make them. Manufacturers have developed various tricks to get light to produce patterns smaller than its own wavelength, but they won’t work at smaller scales. <br /><br />The obvious way to continue shrinking chip features would be to use beams of electrons to transfer mask patterns to layers of photoresist. But unlike light, which can shine through a mask and expose an entire chip at once, an electron beam has to move back and forth across the surface of a chip in parallel lines, like a harvester working along rows of wheat. “It’s like the difference between writing by hand and printing a page all at once,” says Karl Berggren, the Emanuel E. Landsman Associate Professor of Electrical Engineering, who along with Caroline Ross, the Toyota Professor of Materials Science and Engineering, led the new work. The slow, precise scanning of electron-beam lithography makes it significantly more expensive than conventional optical lithography.<br /><br /><strong>Hitchin’ posts</strong><br /><br />The MIT approach — which Berggren and Ross developed together with Yeon Sik Jung and Joel Yang, who were graduate students at the time — is to use electron-beam lithography sparingly, to create patterns of tiny posts on a silicon chip. They then deposit specially designed polymers — molecules in which smaller, repeating molecular units are linked into long chains — on the chip. The polymers spontaneously hitch up to the posts and arrange themselves into useful patterns.<br /><br />The trick is that the polymers are “copolymers,” meaning they’re made of two different types of polymer. Berggren compares a copolymer molecule to the characters played by Robert De Niro and Charles Grodin in the movie <em>Midnight Run</em>, a bounty hunter and a white-collar criminal who are handcuffed together but can’t stand each other. Ross prefers a homelier analogy: “You can think of it like a piece of spaghetti joined to a piece of tagliatelle,” she says. “These two chains don’t like to mix. So given the choice, all the spaghetti ends would go here, and all the tagliatelle ends would go there, but they can’t, because they’re joined together.” In their attempts to segregate themselves, the different types of polymer chain arrange themselves into predictable patterns. By varying the length of the chains, the proportions of the two polymers, and the shape and location of the silicon hitching posts, Ross, Berggren, and their colleagues were able to produce a wide range of patterns useful in circuit design.<br /><br />One of the two polymers that the MIT researchers used burns away when exposed to a plasma (an electrically charged gas), while the other, which contains silicon, turns to glass. The glass layer could serve the same purpose that a photoresist does in ordinary lithography, protecting the material beneath it while that around it is etched away.<br /><br /><strong>Free expression</strong><br /><br />Dan Herr, the director of nanomanufacturing science research at the Semiconductor Research Corporation, an industry and academic research consortium, says that four or five years ago, his organization polled engineers to determine the seven fundamental shapes that self-organizing molecules would have to be able to assume in order to be useful for circuit manufacture. Since then, he says, researchers have gotten molecules to self-assemble into all seven shapes. But to do so, they’ve “changed the chemistry on the surface or etched down a trench in the surface and used that as a channel for the self-assembling process,” Herr says. Since Berggren and Ross’s technique requires no such channels to guide the self-assembling molecules, it reduces the need for electron-beam lithography. According to Herr, “That will save tremendously in terms of throughput” — that is, the efficiency with which chips can be manufactured. <br /><br />Much more research is required, however, before self-assembling molecules can provide a viable means for manufacturing individual chips. Nearer term, Berggren and Ross see the technique’s being used to produce stamps that could impart nanoscale magnetic patterns to the surfaces of hard disks, or even to produce the masks used in conventional lithography: today, state-of-the art masks for a single chip require electron-beam lithography and can cost millions of dollars. In the meantime, Ross and Berggren are working to find arrangements of their nanoscale posts that will produce functioning circuits in prototype chips, and they’re trying to refine their technique to produce even smaller chip features.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT Lincoln Laboratory’s prime contract extended]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/air-force-contract.html</link>
<story_id>15128</story_id>
<featured>0</featured>
<description><![CDATA[]]></description>
<postDate>Tue, 16 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100324133511-1.png</thumbURL>
<smallURL width='140' height='78'>http://web.mit.edu/newsoffice/images/article_images/w140/20100324133511-1.jpg</smallURL>
<fullURL width='368' height='205'>http://web.mit.edu/newsoffice/images/article_images/20100324133511-1.jpg</fullURL>
</image>
<body><![CDATA[MIT Lincoln Laboratory has received official notification that the U.S. government has exercised the Laboratory’s contract with an extension to March 2015. <br /><br />The Laboratory operates under a prime contract with the U.S. Air Force with sponsors from all branches of the Department of Defense (DoD) and government agencies to develop technology in support of national security. Additional research is sponsored by non-DoD agencies such as the Federal Aviation Administration and NASA.<br /><br />For more information about Lincoln Laboratory research, <a href="http://www.ll.mit.edu" target="_blank">visit its web site</a>.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Zooming in on cells]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/micro-peptides-0315.html</link>
<story_id>15101</story_id>
<featured>0</featured>
<description><![CDATA[New microscopy technique offers close-up, real-time view of how proteins kill bacteria]]></description>
<postDate>Mon, 15 Mar 2010 04:00:01 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100312114457-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100312114457-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100312114457-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Georg Fantner]]></imageCredits>
<imageCaption><![CDATA[This image, taken with atomic force microscopy, shows E. coli bacteria after they have been exposed to the antimicrobial peptide CM15. The peptides have begun destroying the bacteria’s cell walls.]]></imageCaption>
</image>
<body><![CDATA[For two decades, scientists have been pursuing a potential new way to treat bacterial infections, using naturally occurring proteins known as antimicrobial peptides (AMPs) that kill bacteria by poking holes in their cell membranes. Now, MIT scientists have recorded the first real-time microscopic images showing the deadly effects of AMPs in live bacteria.<br /><br />Researchers led by MIT Professor Angela Belcher modified an existing, extremely sensitive technique known as high-speed <a href="http://ocw.mit.edu/courses/materials-science-and-engineering/3-052-nanomechanics-of-materials-and-biomaterials-spring-2007/" target="_blank">atomic force microscopy (AFM)</a> to allow them to image the bacteria in real time. Their method, described in the March 14 online edition of <em>Nature Nanotechnology</em>, represents the first way to study living cells using high-resolution images recorded in rapid succession.<br /><br />Using this type of high-speed AFM could allow scientists to study how cells respond to other drugs and to viral infection, says Belcher, the Germeshausen Professor of Materials Science and Engineering and Biological Engineering and a member of the Koch Institute for Integrative Cancer Research at MIT. <br /><br />It could also be useful in studying cell death in mammalian cells, such as the nerve cell death that occurs in Alzheimer’s patients, says Paul Hansma, a physics professor at the University of California at Santa Barbara who has been developing AFM technology for 20 years. “This paper is a highly significant advance in the state-of-the-art imaging of cellular processes,” says Hansma, who was not involved in the research.<br /><br /><strong>High speed</strong><br /><br />Atomic force microscopy, invented in 1986, is widely used to image nanoscale materials. Its resolution (about 5 nanometers) is comparable to that of electron microscopy, but unlike electron microscopy, it does not require a vacuum and thus can be used with living samples. However, traditional AFM requires several minutes to produce one image, so it cannot record a sequence of rapidly occurring events. <br /><br />In recent years, scientists have developed high-speed AFM techniques, but haven’t optimized them for living cells. That’s what the MIT team set out to do, building on the experience of lead author Georg Fantner, a postdoctoral associate in Belcher’s lab who had worked on high-speed AFM at the University of California at Santa Barbara.<br /><br />Atomic force microscopy makes use of a cantilever equipped with a probe tip that “feels” the surface of a sample. Forces between the tip and the sample can be measured as the probe moves across the sample, revealing the shape of the surface. The MIT team used a cantilever about 1,000 times smaller than those normally used for AFM, which enabled them to increase the imaging speed without harming the bacteria.<br /><br />The measurements are performed in a liquid environment, another critical factor in keeping the bacteria alive. <br /><br />With the new setup, the team was able to take images every 13 seconds over a period of several minutes following treatment with an AMP known as CM15. They found that AMP-induced cell death appears to be a two-step process: a short incubation period followed by a rapid “execution.” They were surprised to see that the onset of the incubation period varied from 13 to 80 seconds.<br /><br />“Not all of the cells started dying at the exact same time, even though they were genetically identical and were exposed to the peptide at the same time,” says Roberto Barbero, a graduate student in biological engineering and an author of the paper.<br /><br />Most AMPs act by puncturing bacterial cell membranes, which destroys the delicate equilibrium between the bacterium and its environment. Others appear to target machinery inside the cell. There has been a great deal of interest in developing AMPs as drugs that could supplement or replace traditional antibiotics, but none have been approved yet. <br /><br />Until a few years ago, it was thought that bacteria could not become resistant to AMPS, but recent studies have shown that they can. The new MIT work could help researchers understand how that resistance develops. <br /><br />The research was funded by an Erwin-Schrodinger Fellowship, the National Institutes of Health, Army Research Office and Austrian Research Promotion Agency.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Unraveling silks’ secrets]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/spider-silk-0315.html</link>
<story_id>15102</story_id>
<featured>0</featured>
<description><![CDATA[A new analysis of the structure of silks explains the paradox at the heart of their super-strength, and may lead to even stronger synthetic materials.]]></description>
<postDate>Mon, 15 Mar 2010 04:00:00 EDT </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100312125010-3.png</thumbURL>
<smallURL width='140' height='123'>http://web.mit.edu/newsoffice/images/article_images/w140/20100312125010-3.jpg</smallURL>
<fullURL width='368' height='325'>http://web.mit.edu/newsoffice/images/article_images/20100312125010-3.jpg</fullURL>
</image>
<otherImages>
<image>
<fullURL width='368' height='147'>http://web.mit.edu/newsoffice/images/article_images/20100312115111-1.jpg</fullURL>
<imageCredits><![CDATA[Figure courtesy M. Buehler]]></imageCredits>
<imageCaption><![CDATA[Structure of silk. The yellowish regions are the key cross-linking domains in silk, beta-sheet crystals. Spider web photograph courtesy Nicolas Demars.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='173'>http://web.mit.edu/newsoffice/images/article_images/20100312115112-2.jpg</fullURL>
<imageCredits><![CDATA[Figure courtesy M. Buehler]]></imageCredits>
<imageCaption><![CDATA[Breaking mechanisms of silk nanocrystals. The left part of the image shows a small crystal, which fails gracefully as a strand is being pulled out. The larger crystal on the right fails catastrophically as a crack forms at the left part.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Spiders and silkworms are masters of materials science, but scientists are finally catching up. Silks are among the toughest materials known, stronger and less brittle, pound for pound, than steel. Now scientists at MIT have unraveled some of their deepest secrets in research that could lead the way to the creation of synthetic materials that duplicate, or even exceed, the extraordinary properties of natural silk.<br /><br />Markus Buehler, the Esther and Harold E. Edgerton Associate Professor in MIT’s Department of Civil and Environmental Engineering, and his team study fundamental properties of materials and how those materials fail. With silk, that means using computer models that help determine the molecular and atomic mechanisms responsible for the material’s remarkable mechanical properties. The models can simulate not just the structures of the molecules but also how they move and interact in relation to each other.<br /><br />Silk’s combination of strength and ductility — its ability to bend or stretch without breaking — results from an unusual arrangement of atomic bonds that are inherently very weak, Buehler and his team found. Doctoral student Sinan Keten, postdoctoral associate Zhiping Xu and undergraduate student Britni Ihle are co-authors of a paper on the research that was published on March 14 in the journal <em>Nature Materials</em>.<br /><br />Silks are made from proteins, including some that form thin, planar crystals called beta-sheets. These sheets are connected to each other through hydrogen bonds — among the weakest types of chemical bonds, and a far cry from the much stronger covalent bonds found in most organic molecules. Buehler’s team carried out a series of atomic-level computer simulations that investigated the molecular failure mechanisms in silk. “Small yet rigid crystals showed the ability to quickly re-form their broken bonds, and as a result fail ‘gracefully’ — that is, gradually rather than suddenly,” graduate student Keten explains. <br /><br />“In most engineered materials” — ceramics, for instance — “high strength comes with brittleness,” Buehler says. “Once ductility is introduced, materials become weak.” But not silk, which has high strength despite being built from inherently weak building blocks. It turns out that’s because these building blocks — the tiny beta-sheet crystals, as well as filaments that join them — are arranged in a structure that resembles a tall stack of pancakes, but with the crystal structures within each pancake alternating in their orientation. This particular geometry of tiny silk nanocrystals allows hydrogen bonds to work cooperatively, reinforcing adjacent chains against external forces, which leads to the outstanding extensibility and strength of spider silk. <br /><br />One surprising finding from the new work is that there is a critical dependence of the properties of silk on the exact size of these beta-sheet crystals within the fibers. When the crystal size is about three nanometers (billionths of a meter), the material has its ultra-strong and ductile characteristics. But let those crystals grow to five nanometers, and the material becomes weak and brittle. <br /><br />Buehler says the work has implications far beyond just understanding silk. He notes that the findings could be applied to a broader class of biological materials, such as wood or plant fibers, and bio-inspired materials, such as novel fibers, yarns and fabrics or tissue replacement materials, to produce a variety of useful materials out of simple, commonplace elements. For example, he and his team are looking at the possibility of synthesizing materials that have a similar structure to silk, but using molecules that have inherently greater strength, such as carbon nanotubes. <br /><br />The long-term impact of this research, Buehler says, will be the development of a new material design paradigm that enables the creation of highly functional materials out of abundant, inexpensive materials. This would be a departure from the current approach, where strong bonds, expensive constituents, and energy intensive processing (at high temperatures) are used to obtain high-performance materials.<br /><br />This work was supported by the Office of Naval Research, with additional funding from the National Science Foundation, the Army Research Office, the MIT Energy Initiative, and MIT’s UROP and MISTI-Germany programs.<br /><br />Peter Fratzl, professor in the department of biomaterials in the Max Planck Institute of Colloids and Interfaces in Potsdam, Germany, who was not involved in this work, says that “the strength of this team is their pioneering multi-scale theoretical approach” to analyzing natural materials. He adds that this is “the first evidence from theoretical modeling of how hydrogen bonds, as weak as they might be, can provide high strength and toughness if arranged in a suitable way within the material.”<br /><br />Professor of biomaterials Thomas Scheibel of the University of Bayreuth, Germany, who was also not involved in this work, says Buehler’s work is of the “highest caliber,” and will stimulate much further research. The MIT team’s approach, he says, “will provide a basis for better understanding of certain biological phenomena so far not understood.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[In The World: Nanotech on the farm]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/nanotech-farm-0312.html</link>
<story_id>15099</story_id>
<featured>0</featured>
<description><![CDATA[MIT chemical engineer Paula Hammond lends her nanotechnology expertise to farmers in Africa.]]></description>
<postDate>Fri, 12 Mar 2010 04:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100311155330-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100311155330-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100311155330-1.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy of Paula Hammond]]></imageCredits>
<imageCaption><![CDATA[Women in Ghana work with cassava, which is cut, milled, pressed and dried (sometimes fermented first).]]></imageCaption>
</image>
<body><![CDATA[Cassava is a tropical root vegetable and staple crop for millions of people in sub-Saharan Africa. However, it’s tricky to handle: Once the root is removed from the ground, it spoils within one to three days, so farmers must get it to processing centers as soon as possible after harvesting it. If they don’t, the crop goes to waste. <br /><br />A simple way to prolong cassava’s shelf life could help farmers avoid that waste and sell their crop beyond their local region. Paula Hammond, MIT professor of chemical engineering, and other scientists are now working on an innovative way to help them do that, using nanotechnology — technology that controls material at a molecular or atomic scale. Their idea is to design a plastic storage bag lined with nanoparticles that would react with oxygen, preventing the roots’ oxygen-induced rotting.<br /><br />“That would enable farmers to harvest and store and process at times convenient to them,” says Hammond, who traveled to Kenya and Ghana last summer with an international group of scientists to meet with farmers and come up with new ways to improve agricultural efficiency.<br /><br />It may seem odd to send Hammond, a chemical engineer who focuses on nanotechnology, into rural Africa to help farmers. But that’s exactly the point, says Todd Barker, a partner for the <a href="http://www.merid.org/" target="_blank">Meridian Institute</a>, which organized the trip with funding from the Bill &amp; Melinda Gates Foundation.<br /><br />Organizers were looking for scientists who specialize in fields not traditionally involved in international development. And they wanted people who knew little or nothing about agriculture, says Barker. “We wanted to get them to look at these particular problems in Africa with a fresh set of eyes.”<br /><br /><strong>‘An important problem’</strong><br /><br />After the Meridian Institute identified three agricultural chains where farmers needed help — cassava, dairy and maize (corn), Barker enlisted Jeffrey Carbeck PhD ’96, a chemical engineer and entrepreneur, to identify scientists who would fit in with the mission. “I was looking for people who had a deep technical background but had shown they could apply it in multiple areas,” says Carbeck, who knew Hammond from their graduate school days at MIT.<br /><br />Carbeck thought that Hammond, an expert in designing polymers for drug delivery, sensors and energy, would fit perfectly. Hammond, in turn, was intrigued by the idea. “It sounded like such an important problem, and I had never been to Africa. This was a chance to see it from a very unique perspective,” she says.<br /><br />Equipped with Land Rovers and digital video cameras, the group of a dozen scientists from around the world traveled to farms throughout the two African nations, talking with farmers to find out the biggest obstacles they face. <br /><br />For Hammond, the trip was enlightening. “These working families have very immediate problems and have neither the resources, nor perhaps the voice, to express them to groups of elite scientists, and that’s what this allowed them to do,” she says. “These are really exciting problems outside the realm of what we might normally encounter in academia.”<br /><br />The team found that dairy farmers have a similar problem to cassava farmers — getting their milk to processing centers before it spoils. Most farms don’t have their own refrigeration facilities, so the farmers have to carry their milk in plastic jugs, usually on foot or bicycle, to the nearest cooling center.<br /><br />If the cooling centers are far from the farm, the farmers might make only one trip a day, so any milk produced after that trip is in danger of spoiling before the next day’s trip. Milk that goes bad is rejected at the center and dumped out. <br /><br />To avoid that waste, Hammond and other scientists in the group came up with the idea to design a milk container with a nanopatterned, antimicrobial coating that would preserve milk longer than a plain plastic jug. <br /><br />The African dairy farmers are also interested in a way to easily test their cows to see if they’re pregnant or in heat. Cows must be bred and produce calves in order to produce milk, but if a cow runs dry, it’s difficult to tell whether it’s due to lack of pregnancy or a common udder infection known as mastitis.<br /><br />There is no simple test for cow pregnancy as there is for humans, but scientists who went on the trip came up with the idea to adapt existing nanopatterned paper sensors to detect bovine pregnancy. <br /><br />The Gates Foundation originally planned to allocate funding for two or three ideas that came out of the trip, but there were so many (more than 200, later consolidated into 22 concepts), that the foundation is encouraging the scientists to pursue as many as possible. The Meridian Institute will initially focus on diagnostic tools for mastitis, the new milk container, tick-borne disease and other livestock diseases, safety tests for milk, a modified plastic tank for maize storage, and a new way to dry cassava.<br /><br />The Meridian Institute is now working on starting up a foundation that would serve as an “incubator” to help develop, test and bring these ideas to commercialization, according to Barker. “The major challenge now is to make sure the ideas that came out of the trip reach the farmers in Africa,” he says.<br /><br /><em><a href="http://web.mit.edu/newsoffice/topic/in-the-world.html">In The  World</a> </em><em>is a column that explores the ways members of the MIT  community are developing technology — from the appropriately simple to  the cutting edge — to help meet the needs of communities around the  planet, especially those in the developing world. If you have  suggestions for future columns, please e-mail <a href="mailto:newsoffice@mit.edu" target="_blank">newsoffice@mit.edu</a>.</em><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Wind resistance]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/climate-wind-0312.html</link>
<story_id>15098</story_id>
<featured>0</featured>
<description><![CDATA[MIT analysis suggests generating electricity from large-scale wind farms could influence climate — and not necessarily in the desired way.]]></description>
<postDate>Fri, 12 Mar 2010 04:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100311141900-1.png</thumbURL>
<smallURL width='140' height='133'>http://web.mit.edu/newsoffice/images/article_images/w140/20100311141900-1.jpg</smallURL>
<fullURL width='368' height='351'>http://web.mit.edu/newsoffice/images/article_images/20100311141900-1.jpg</fullURL>
<imageCredits><![CDATA[Image: istockphoto]]></imageCredits>
</image>
<body><![CDATA[Wind power has emerged as a viable renewable energy source in recent years — one that proponents say could lessen the threat of global warming. Although the American Wind Energy Association estimates that only about 2 percent of U.S. electricity is currently generated from wind turbines, the U.S. Department of Energy has said that wind power could account for a fifth of the nation’s electricity supply by 2030. <br /><br />But a new MIT analysis may serve to temper enthusiasm about wind power, at least at very large scales. Ron Prinn, TEPCO Professor of Atmospheric Science, and principal research scientist Chien Wang of the Department of Earth, Atmospheric and Planetary Sciences, used a climate model to analyze the effects of millions of wind turbines that would need to be installed across vast stretches of land and ocean to generate wind power on a global scale. Such a massive deployment could indeed impact the climate, they found, though not necessarily with the desired outcome.<br /><br />In a <a href="http://www.atmos-chem-phys.net/10/2053/2010/acp-10-2053-2010.html" target="_blank">paper published online</a> Feb. 22 in <em>Atmospheric Chemistry and Physics</em>, Wang and Prinn suggest that using wind turbines to meet 10 percent of global energy demand in 2100 could cause temperatures to rise by one degree Celsius in the regions on land where the wind farms are installed, including a smaller increase in areas beyond those regions. Their analysis indicates the opposite result for wind turbines installed in water: a drop in temperatures by one degree Celsius over those regions. The researchers also suggest that the intermittency of wind power could require significant and costly backup options, such as natural gas-fired power plants.<br /><br />Prinn cautioned against interpreting the study as an argument against wind power, urging that it be used to guide future research that explores the downsides of large-scale wind power before significant resources are invested to build vast wind farms. “We’re not pessimistic about wind,” he said. “We haven’t absolutely proven this effect, and we’d rather see that people do further research.”<br /><br />Daniel Kirk-Davidoff, a chief scientist for MDA Federal Inc., which develops remote sensing technologies, and adjunct professor of meteorology at the University of Maryland, has examined the climate impacts of large-scale wind farms in previous studies. To him, the most promising result of the MIT analysis is that it indicates that the large-scale installation of wind turbines doesn’t appear to slow wind flow so much that it would be impossible to generate a desirable amount of energy. “When you put the wind turbines in, they are generating the kind of power you’d hope for,” he said. <br /><br /><strong>Tapping the wind resource</strong><br /><br />Previous studies have predicted that annual world energy demand will increase from 14 terawatts (trillion watts) in 2002 to 44 terawatts by 2100. In their analysis, Prinn and Wang focus on the impact of using wind turbines to generate five terawatts of electric power.<br /><br />Using a climate model developed by the U.S. National Center for Atmospheric Research, the researchers simulated the aerodynamic effects of large-scale wind farms — located both on land and on the ocean — to analyze how the atmosphere, ocean and land would respond over a 60-year span.<br /><br />For the land analysis, they simulated the effects of wind farms by using data about how objects similar to turbines, such as undulating hills and clumps of trees, affect surface “roughness,” or friction that can disturb wind flow. After adding this data to the model, the researchers observed that the surface air temperature over the wind farm regions increased by about one degree Celsius, which averages out to an increase of .15 degrees Celsius over the entire global surface.<br /><br />According to Prinn and Wang, this temperature increase occurs because the wind turbines affect two processes that play critical roles in determining surface temperature and atmospheric circulation: vertical turbulent motion and horizontal heat transport. Turbulent motion refers to the process by which heat and moisture are transferred from the land or ocean surface to the lower atmosphere. Horizontal heat transport is the process by which steady large-scale winds transport excessive heat away from warm regions, generally in a horizontal direction, and redistribute it to cooler regions. This process is critical for large-scale heat redistribution, whereas the effects of turbulent motion are generally more localized.<br /><br />In the analysis, the wind turbines on land reduced wind speed, particularly on the downwind side of the wind farms, which reduced the strength of the turbulent motion and horizontal heat transport processes that move heat away from the Earth’s surface. This resulted in less heat being transported to the upper parts of the atmosphere, as well as to other regions farther away from the wind farms. The effect is similar to being at the beach on a windy summer day: If the wind weakened or disappeared, it would get warmer.<br /><br />In contrast, when examining ocean-based wind farms, Prinn and Wang found that wind turbines cooled the surface by more than one degree Celsius. They said that these results are unreliable, however, because in their analysis, they modeled the effects of wind turbines by introducing surface friction in the form of large artificial waves. But they acknowledge that this is not an accurate comparison, meaning that a better way of simulating marine-based wind turbines must be developed before reliable conclusions can be made.<br /><br />In addition to changes in temperatures and surface heat fluxes, they also observed changes in large-scale precipitation, particularly at the mid-latitudes in the Northern Hemisphere. Although these changes exceeded 10 percent in some areas, the global total changes were not very large, according to Prinn and Wang. <br /><br />To investigate the effect of wind variability on the intermittency in wind power generation, the researchers used the climate model to estimate the monthly-mean wind power consumption and electrical generation for each continent, concluding that there are very large and geographically extensive seasonal variations, particularly over North and South America, Africa and the Middle East. They explain that this unreliability means that an electrical generation system with greatly increased use of wind turbines would still require backup generation even if continental-scale power lines enabled electrical transmission from windy to non-windy areas.<br /><br />Although Prinn and Wang believe their results for the land-based wind farms are robust, Wang called their analysis a “proof-of-concept” study that requires additional theoretical and modeling work, as well as field experiments for complete verification. <br /><br />Their next step is to address how to simulate ocean-based wind farms more accurately. They plan to collaborate with aeronautical engineers to develop parameters for the climate model that will allow them to simulate turbines in coastal waters.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[In MIT visit, Miliband presses for Afghan peace deal]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/compton-miliband-0311.html</link>
<story_id>15095</story_id>
<featured>0</featured>
<description><![CDATA[British foreign secretary uses Compton Lecture to urge Afghanistan’s government to reach out to insurgents.]]></description>
<postDate>Thu, 11 Mar 2010 04:00:04 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100310200740-0.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100310200740-0.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100310200740-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Justin Knight]]></imageCredits>
</image>
<body><![CDATA[British Foreign Secretary David Miliband on Wednesday urged the Afghanistan government to consider bringing Taliban supporters into its political system, telling an MIT audience that the prompt pursuit of a political deal among Afghanistan’s warring factions was necessary to build a lasting peace in that country.<br /><br />“My argument today is that now is the time for the Afghans to pursue a political settlement with as much vigor and energy as we are pursuing the military and civilian effort,” said Miliband SM ’90, delivering the Karl Taylor Compton Lecture in Kresge Auditorium. <br /><br />Miliband’s talk came as U.S. and British troops make a renewed push to secure the Taliban stronghold of Helmand Province in southern Afghanistan. The Western troops have been attempting to control the town of Marjah, and aim to secure the region’s major city, Kandahar, this summer.<br /><br />While acknowledging the necessity of the military action, Miliband struck a different point of emphasis in his talk, stressing the importance of a simultaneous process of political reconciliation. “Afghanistan will never achieve a sustainable peace unless many more Afghans are inside the political system, and the neighbors [nearby countries] are onside with the political settlement,” said Miliband, whose Labour Party is expected to face a strong challenge from the opposition Conservatives in Britain’s forthcoming general election later this spring. <br /><br />The lecture was intended in part to send a message to Afghan president Hamid Karzai, who has suggested his country hold a jirga in April — a kind of grand peace negotiation about the country’s political future. Western governments want to make clear to Karzai, re-elected in controversial circumstances in 2009, that they expect improvements in governance to accompany improvements in security.<br /><br />“The international community will judge him by his actions, not his words,” Miliband warned on Wednesday, later adding, “The Afghans themselves must own, lead and drive such political engagement.” <br /><br /><strong>Talking to the Taliban</strong><br /><br />Miliband’s speech laid out a framework for the “political outreach” that he thinks will be useful in reconstructing Afghanistan. As Miliband sees it, a sustainable Afghan government will need to be more inclusive of ethnic Pashtuns, who have often supported the Taliban; will need to decentralize and provide better support for regional governors and governing councils; should give parliament a greater voice in political affairs; and must address the pervasive problem of corruption in the Afghan government.<br /><br />Adding these elements to the Afghan political system would mean supplementing the Bonn Agreement, the accords worked out after the successful capture of Kabul, Afghanistan’s capital, by the United States and its allies, following the Sept. 11, 2001, terrorist attacks. Those attacks were carried out by al-Qaida terrorists given protection and resources by the Taliban government in Afghanistan.<br /><br />In turn, extending the tribal and ethnic scope of Afghanistan’s government would inevitably mean bringing former Taliban fighters or supporters into the fold. “The idea of political engagement with those who would directly or indirectly attack our troops is difficult,” Miliband acknowledged. However, he added, “dialogue is not appeasement and political space is not the same as veto power or domination.”<br /><br />To be sure, Miliband said, he has no expectation that all militants would themselves be willing to participate in a process of reconciliation. “Some insurgents are committed to al-Qaida’s violent extremist agenda,” Miliband observed. “There will never be reconciliation with them — they must be beaten back.”<br /><br /><strong>Upward trajectory in ‘two to five years’</strong><br /><br />A portion of Miliband’s lecture was devoted to acknowledging the complications of getting Afghanistan’s many neighbors to agree on a path forward for the country. Pakistan, the country regarded as having the most influence inside Afghanistan, “holds many of the keys to security and dialogue,” Miliband said. “It clearly has to be a partner in finding solutions in Afghanistan.” <br /><br />Miliband noted that like other nations, “Pakistan will only act according to its own sense of its national interest.” That said, Miliband also said he believes there has been a “significant change” in Pakistan during the last 18 months under President Asif Ali Zardari, and he expressed optimism that countries with vested interests in Afghanistan — including India, Russia, Turkey and China — will recognize a basic fact about the region: “The status quo in Afghanistan hurts all.” <br /><br />And Miliband did, at times, re-emphasize the importance of military progress to the possibility of civil reconstruction in the country. “Only if the scale of the insurgency itself is reduced will the Afghan authorities be able to govern their land in sustainable or acceptable ways,” he said. Fewer than half of regional governors even have an office; fewer than a quarter of them have electricity. <br /><br />Still, Miliband suggested, if both military and political progress take place in 2010, then “within two to five years it is realistic to aspire to see a country on an upward trajectory, still poor but with a just peace, with democracy and inclusive politics bedding down at all levels and with incomes growing.” <br /><br />After his talk, Miliband, who earned his master’s in political science from MIT, was presented a school ring onstage, from the Graduate Student Council. While on campus two decades ago, he remarked at the outset of his speech, “I would never have believed that I would be British Foreign Secretary in a Labour government explaining a war in Afghanistan.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[An open, collaborative space]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/media-lab-research.html</link>
<story_id>15096</story_id>
<featured>0</featured>
<description><![CDATA[New Media Lab building is designed to foster collaboration, sharing, communication and experimentation.]]></description>
<postDate>Thu, 11 Mar 2010 04:00:03 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100310131010-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100310131010-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100310131010-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Andy Ryan]]></imageCredits>
<imageCaption><![CDATA[An interior shot of the new Media Lab complex, which officially opened last week.]]></imageCaption>
</image>
<body><![CDATA[The design of MIT’s new Media Lab building, which formally opened last week, is not only aesthetically pleasing, but also serves the interests of the lab’s own unique culture.<br /><br />“It’s a building that very specifically was intended to support a particular kind of community and a particular research style,” explains William Mitchell, the Alexander W. Dreyfoos, Jr. (1954) Professor of Architecture and Professor of Media Arts and Sciences at MIT. Specifically, he says, it is intended to foster a serendipitous exchange of ideas among the “intensely cross-disciplinary” researchers by making the laboratory spaces exceptionally open, with broad and often surprising sight lines from one working space to another.<br /><br />Cross-disciplinary research is a hallmark of MIT in general, but at the Media Lab, this gets carried to extremes. The lab features research teams working on projects that range from designing new artificial limbs to new banking systems, from cuddly, personable robots to new ways of storing and retrieving medical records, from educational electronic toys and computer languages to fanciful futuristic musical instruments.<br /><br />The Media Lab, now celebrating its 25th anniversary, is part of MIT’s School of Architecture and Planning. The new complex will also house some of this school’s other departments, including a new Art, Culture and Technology Program. <br /><br />The seven lab spaces that house various combinations of working groups were modeled after “the cube,” one of the laboratory spaces in the original I.M. Pei-designed Media Lab building (now connected to the new one on several floors). Each of these labs consists of a two-story-high open central workshop area, surrounded on two or three sides by offices that open onto a balcony overlooking the central space. All of these working spaces, in turn, are arrayed around a central atrium that spans five of the building’s six stories. The atrium is crisscrossed by ramps and stairways and also served by glass-enclosed elevators.<br /><strong><br />Open space</strong><br /><br />All of this open space lets in abundant natural light — a trademark of the architect, Japan’s Fumihiko Maki — while affording the occupants grand views of the Charles River and Boston skyline. To avoid excess heat in summertime, aluminum bars extend over much of the window area to filter the light.<br /><br />The building also “reverses the traditional logic of the way you do academic buildings,” Mitchell says, by putting the public spaces on the top floor. “These become attractions that draw people up through the building,” he says, and then reward them with impressive views. And as visitors ascend through the central space, they have open views into lab spaces; this “puts the building on display, to make the work of the various groups in the Media Lab visible,” Mitchell says.<br /><br />“This is important,” he adds, “because you can’t have effective cross-disciplinary work unless you can see what other people are doing.” That approach, he says, is “an absolutely radical departure from traditional MIT spaces.”<br /><br />In some cases, the combinations of groups working together in the new lab spaces have obvious connections — for example, one lab now houses the Biomechatronics group developing prosthetic and ability-enhancing attachments for the human body, the New Media Medicine group, which is developing new ways of collecting, storing and retrieving medical data, and the Affective Computing group, which includes new robots designed to help teach interactive behavior to autistic children. Other combinations are a bit more surprising, such as the pairing of Mitchell’s Smart Cities group, which is developing a variety of foldable, stackable electric vehicles, with composer Tod Machover’s Opera of the Future group and its collection of high-tech musical instruments.<br /><br />Andrew Lippman, co-director of MIT’s Communications Futures Program at the Media Lab, says the original building was designed to foster such collaborative research, and “the new building does that in spades. It raises the stakes.” The openness of the building, he says, “allows you to open your thinking.” The open atrium with its views into all the different lab spaces means that “you can stand there and literally feel the hum of what goes on. So the physical design will contribute to the style of the work that goes on there. I hope what will happen is that the space will enhance the opportunities for collaboration and cross-fertilization.”<br /><br />But some of the building’s features are decidedly retro for a hotbed of new, cutting-edge technology. For example, all the offices have windows that can be opened for ventilation. Mitchell says this “makes the control systems a lot more complicated” for heating and cooling, but is worth it because it’s something that makes a big difference to people. “If you build a building around specific technology, it’ll be obsolete quickly. But fundamental human attitudes haven’t changed.” The new building doesn’t incorporate much in the way of specific new technology, but is designed to allow for experimentation with new electronics and communications systems.<br /><br />But it is still the Media Lab. There will be “an information architecture that mirrors the accessibility and openness embodied in the architecture,” Lippman says. For example, there will be display stations strategically placed around the building and in the labs, partly to provide information about what’s going on “to optimize the experience of a visitor."<br /><br />While many labs or businesses may have systems designed to display details of projects in progress for the use of their own people, he says, “that doesn’t necessarily reflect what anyone else in the world is interested in.” At the Media Lab, “by optimizing the systems both for ourselves and for sponsors and guests and families,” he says, these systems “will become a part of the culture of this place.” The displays will “give you a window into what’s going on in the labs and where your colleagues are, and you can communicate with them with just a gesture.”<br /><br />For example, even things as simple as the signs on office doors will be electronic interactive displays. David Small, associate professor of media arts and sciences, is concentrating on the development of interactive displays in the lab. He says that if you arrive at his office for a meeting, instead of a printed sign that just gives his name, the display might say “Dave is upstairs, but he knows you have a meeting now and he’ll be right down” — and perhaps allow you to send him a text saying you’re there.<br /><br />The development of these communication systems will be an ongoing part of the life of the lab, Lippman says. “It’s not going to hatch, it’s going to evolve.”<br /><br />The display systems will also provide real-time data on the building’s lighting, heating and cooling systems. “We want to make the place a living laboratory for energy efficiency research,” Lippman says.<br /><br />Small adds that these displays will show people “detailed information about real-time electrical usage. We’re interested in how you might use that to change people’s behavior.”<br /><br />But most of all, the systems are designed to grow. Displays and controls are designed “to be as hackable as possible,” Small says, so that students can experiment with setting up different kinds of displays and interactive systems as technology develops.<br /><br />Mitchell says the building really needs to be seen in person to be appreciated. “I think people are going to be astounded by the qualities of the interior space,” he says. Maki’s buildings tend not to be flashy on the outside, he says, but have interiors that provide “an explosion of space and lightness. There’s a ‘wow!’ moment when you go inside.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: David Miliband on Afghanistan’s future]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/3q-miliband.html</link>
<story_id>15097</story_id>
<featured>0</featured>
<description><![CDATA[Before delivering the Compton Lecture, Britain’s foreign secretary sat down with MIT News to discuss the state of the war in Afghanistan]]></description>
<postDate>Thu, 11 Mar 2010 04:00:02 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100310135442-1.png</thumbURL>
<smallURL width='140' height='106'>http://web.mit.edu/newsoffice/images/article_images/w140/20100310135442-1.jpg</smallURL>
<fullURL width='368' height='280'>http://web.mit.edu/newsoffice/images/article_images/20100310135442-1.jpg</fullURL>
</image>
<body><![CDATA[<em>As U.S.-led forces step up their military efforts in Afghanistan,  British Foreign Secretary David Miliband has begun urging Afghan  President Hamid Karzai and his government to push for a peace settlement  with Taliban insurgents. Before delivering the Karl Taylor Compton  Lecture on Wednesday, Miliband — who has an MA in political science from  MIT — sat down with MIT News to discuss Afghanistan.</em><br /><br /><strong>Q.</strong> You’re here to help try to spur a peace process forward in Afghanistan.  You’ve met Hamid Karzai, you were at his inauguration along with other  Western officials in November. At this time, does he have the legitimacy  and stature to be central to a peace process and stable government?<br /><br /><strong>A.</strong> Well, he’s the elected leader of Afghanistan, the elected president of  Afghanistan. There were very serious allegation of fraud in respect to  his election, but I don’t think many people doubt that he got more votes  than his opponent, even though he didn’t get the 50 percent absolute  majority in the first round. A recent poll by the BBC suggests he has  quite a lot of support from the Afghan people, who believe him to be  their elected president, legitimately elected president. So I think the  answer to that has got to be, ‘Yes.’ He is the president of the country,  and he has huge responsibilities. And we have to work with him.<br /><br /><strong>Q.</strong> U.S. Secretary of Defense Robert Gates was just in Kabul, and said  about possible negotiations, “The timing of this, really I think, in  many respects depends on the conditions on the ground in terms of when  people, particularly the more senior [Taliban] commanders, realize that  the odds against their success are no longer in their favor.” To what  extent is military success a precondition of diplomatic success, or to  what extent, and for what reasons, can they co-exist at the same time? <br /><br /><strong>A.</strong> Well, I think that military and civilian effort can together create the  conditions for the political process. And the military effort now is  significant, the philosophy is right, to protect the population, the  numbers are right, in terms of the surge, the effort from the Pakistanis  is right, in terms of what they’re doing on the Pakistan side of the  border. So that means that it is the right time to have the sort of  political drive that’s going to bring this insurgency to an end. <br /><br /><strong>Q.</strong> Over the last several weeks we’ve seen concerted military action [by  American and British forces] in Marjah. But there have been some  temporary military successes in the Helmand Province before, which  displaced Taliban leaders, who were then able to come back later. So  what might make this time different?  <br /><br /><strong>A.</strong> Afghan capacity:  capacity on security, through the Afghan national army and the Afghan  national police; capacity in governance, through the support of  provincial and district governments; and capacity for politics to emerge  that brings disaffected Pashtuns who are not part of global jihad into  the political process. That’s the difference. In the end, the population  are the best protection against the insurgency. To make that possible,  we’ve got to protect the population. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Radiative forcing]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/explained-radforce-0309.html</link>
<story_id>15093</story_id>
<featured>0</featured>
<description><![CDATA[When there’s more energy radiating down on the planet than there is radiating back out to space, something’s going to have to heat up]]></description>
<postDate>Wed, 10 Mar 2010 04:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100309160043-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100309160043-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100309160043-1.jpg</fullURL>
<imageCredits><![CDATA[Image: NASA Johnson Space Center (NASA-JSC)]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='408'>http://web.mit.edu/newsoffice/images/article_images/20100310094520-2.png</fullURL>
<imageCredits><![CDATA[Chart: IPCC]]></imageCredits>
<imageCaption><![CDATA[This summary chart from the IPCC’s Fourth Assessment Report shows the main components of radiative forcing that affect the Earth’s climate. The values represent the amount these factors have changed (as of 2005) since 1750, in the pre-industrial era, along with the amount of uncertainty associated with each number. Note that by far the largest uncertainties are in the effect of aerosols.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<em>This is the first of a two-part “Explained” on the scientific concepts underlying the concept of the greenhouse effect and global climate change. </em><br /><br />When people talk about global warming or the greenhouse effect, the main underlying scientific concept that describes the process is radiative forcing. And despite all the recent controversy over leaked emails and charges of poorly sourced references in the last Intergovernmental Panel on Climate Change report, the basic concept of radiative forcing is one on which scientists — whatever their views on global warming or the IPCC — all seem to agree. Disagreements come into play in determining the actual value of that number.<br /><br />The concept of radiative forcing is fairly straightforward. Energy is constantly flowing into the atmosphere in the form of sunlight that always shines on half of the Earth’s surface. Some of this sunlight (about 30 percent) is reflected back to space and the rest is absorbed by the planet. And like any warm object sitting in cold surroundings — and space is a very cold place — some energy is always radiating back out into space as invisible infrared light. Subtract the energy flowing out from the energy flowing in, and if the number is anything other than zero, there has to be some warming (or cooling, if the number is negative) going on.<br /><br />It’s as if you have a kettle full of water, which is at room temperature. That means everything is at equilibrium, and nothing will change except as small random variations. But light a fire under that kettle, and suddenly there will be more energy flowing into that water than radiating out, and the water is going to start getting hotter.<br /><br />In short, radiative forcing is a direct measure of the amount that the Earth’s energy budget is out of balance. <br /><br />For the Earth’s climate system, it turns out that the level where this imbalance can most meaningfully be measured is the boundary between the troposphere (the lowest level of the atmosphere) and the stratosphere (the very thin upper layer). For all practical purposes, where weather and climate are concerned, this boundary marks the top of the atmosphere. <br /><br />While the concept is simple, the analysis required to figure out the actual value of this number for the Earth right now is much more complicated and difficult. Many different factors have an effect on this balancing act, and each has its own level of uncertainty and its own difficulties in being precisely measured. And the individual contributions to radiative forcing cannot simply be added together to get the total, because some of the factors overlap — for example, some different greenhouse gases absorb and emit at the same infrared wavelengths of radiation, so their combined warming effect is less than the sum of their individual effects.<br /><br />In its most recent report in 2007, the IPCC produced the most comprehensive estimate to date of the overall radiative forcing affecting the Earth today. Ronald Prinn, the TEPCO Professor of Atmospheric Science and director of MIT’s Center for Global Change Science, was one of the lead authors of that chapter of the IPCC’s Fourth Assessment Report. Radiative forcing “was very small in the past, when global average temperatures were not rising or falling substantially,” he explains. For convenience, most researchers choose a “baseline” year before the beginning of world industrialization — usually either 1750 or 1850 — as the zero point, and compute radiative forcing in relation to that base. The IPCC uses 1750 as its base year and it is the changes in the various radiative forcing agents since then that are counted.<br /><br />Thus radiative forcing, measured in watts per square meter of surface, is a direct measure of the impact that recent human activities — including not just greenhouse gases added to the air, but also the impact of deforestation, which changes the reflectivity of the surface — are having on changing the planet’s climate. However, this number also includes any natural effects that may also have changed during that time, such as changes in the sun’s output (which has produced a slight warming effect) and particles spewed into the atmosphere from volcanoes (which generally produce a very short-lived cooling effect, or negative forcing).<br /><br />Although all of the factors that influence radiative forcing have uncertainties associated with them, one factor overwhelmingly affects the uncertainty: the effects of aerosols (small airborne particles) in the atmosphere. That’s because these effects are highly complex and often contradictory. For example, bright aerosols (like sulfates from coal-burning) are a cooling mechanism, whereas dark aerosols (like black carbon from diesel exhausts) lead to warming. Also, adding sulfate aerosols to clouds leads to smaller but more abundant droplets that increase cloud reflectivity, thus cooling the planet.<br /><br />“The error bars in the greenhouse gas forcing are very small,” Prinn says. “The biggest uncertainty in defining radiative forcing comes from aerosols.”<br /><br />So, given all these factors and their range of errors, what’s the answer? The current level of radiative forcing, according to the IPCC AR4, is 1.6 watts per square meter (with a range of uncertainty from 0.6 to 2.4). That may not sound like much, Prinn says, until you consider the total land area of the Earth and multiply it out, which gives a total warming effect of about 800 terawatts — more than 50 times the world’s average rate of energy consumption, which is currently about 15 terawatts.<br /><br />Part two of this series will examine the concept of climate sensitivity, which determines how much the planet’s temperature will change due to a given radiative forcing.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Four professors named 2010 MacVicar Faculty Fellows]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/macvicar-awards.html</link>
<story_id>15084</story_id>
<featured>0</featured>
<description><![CDATA[Hosoi, Rajagopal, Ram and Richards honored for excellence in undergraduate teaching]]></description>
<postDate>Tue, 09 Mar 2010 22:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100310091423-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100310091423-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100310091423-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: ©suzanne camarata photography]]></imageCredits>
<imageCaption><![CDATA[2010 MacVicar fellows, from left to right, Rajeev Ram, Krishna Rajagopal, Norvin Richards and Anette (Peko) Hosoi.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100310091424-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: ©suzanne camarata photography]]></imageCredits>
<imageCaption><![CDATA[Anette (Peko) Hosoi and Dean for Undergraduate Education Daniel Hastings laugh during the MacVicar fellows ceremony Tuesday night.]]></imageCaption>
</image>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100310091424-3.jpg</fullURL>
<imageCredits><![CDATA[Photo: ©suzanne camarata photography]]></imageCredits>
<imageCaption><![CDATA[Standing, from left to right: Provost L. Rafael Reif, Daniel Hastings, Victoria MacVicar, Krishna Rajagopal. Seated, from left to right: Rajeev Ram, MIT President Susan Hockfield, Peko Hosoi and Norvin Richards.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Four professors were honored for outstanding undergraduate teaching, mentoring and educational innovation when they were named as the 2010 MacVicar Faculty Fellows: Anette (Peko) Hosoi, of mechanical engineering, Krishna Rajagopal, of physics, Rajeev Ram, of electrical engineering and computer science, and Norvin Richards, of linguistics and philosophy.<br /><br />Provost L. Rafael Reif formally announced honorees to the MIT faculty during a reception at Gray House on Tuesday. “Appointment as a MacVicar Fellow recognizes professors who have made exemplary and sustained contributions to the teaching and complete education of MIT undergraduates,” said Reif, “which includes their dedication inside the classroom and beyond.” The provost’s advisory committee is chaired by Daniel Hastings, dean for undergraduate education, and includes faculty and students who assist the provost in selecting new fellows.  <br /><br />The MacVicar Faculty Fellows Program was established in 1992 to honor the life and devotion to teaching excellence of <a href="http://web.mit.edu/provost/macvicar/biography.html" target="_blank">Margaret MacVicar</a> ‘64, ScD ‘67, MIT's first dean for undergraduate education and founder of UROP (the Undergraduate Research Opportunities Program). The 10-year fellowship provides an annual scholar’s allowance to support faculty efforts to enrich undergraduate learning experiences.<br /><br />To celebrate undergraduate education on MacVicar Day, Wednesday, March 10, <a href="http://www.johnseelybrown.com/" target="_blank">John Seely Brown</a> will present a lecture titled "Blended Learning Revisited" at 2:15 p.m. in 32-141. The event is open to the entire MIT community. <br /><br /><strong>Anette (Peko) Hosoi</strong><br /><br />Hosoi earned her MA (1994) and PhD (1997) from the University of Chicago after completing her BA at Princeton University. She was an assistant professor at Harvey Mudd College and an instructor in the Department of Mathematics at MIT before joining the mechanical engineering faculty in 2002.<br /><br />Hosoi’s principal fields of interest are free surface flows, thin films, surface tension, particle-laden flows and complex fluids. In addition, she has designed a Mechanical Crawler (MIT TLO Case No. 11298): a Peristaltic Crawler based on marine pastropod locomotion.  <br /><br />Evidence of Hosoi’s remarkable impact on the tone and vibrancy of undergraduate education in MechE is gathered by standing in the hall outside her office where the rule (not the exception) is an open door, noted one of her colleagues. There are frequently many undergraduates in roll-up-your-sleeves blackboard discussions punctuated by regular interruptions of collective laughter. It is what undergraduate education is supposed to be.<br /><br />“Professor Hosoi is amazing!” raved one of her students. “I enjoyed every recitation and left more knowledgeable each time. She is both an incredible teacher and role model for her students, especially women.”<br /><br /><strong>Norvin Richards</strong><br /><br />Richards received his BA from Cornell University in 1993 and his PhD from MIT in 1997. He is a professor in the Department of Linguistics and Philosophy. <br /><br />He is interested in a variety of topics in syntax, including the syntax of wh-movement and the syntax-phonology interface, and has done fieldwork on a number of languages. He is the author of several books and papers, including the recently published book <em>Uttering Trees</em> (MIT Press, March 2010).<br /><br />“Every conceivable virtue is evident in Norvin’s teaching,” explains one of his colleagues. “His planning is extensive and perfect. He comes to class, lays out the issues, data and analysis with clarity and beauty. Norvin is the kind of teacher who makes his audience think and ask questions because they find it fun to do so.” <br /><br />“Professor Richards is easily one of the best instructors I’ve had in my life,” one of his students told the selection committee. “His lectures are entertaining, interesting and content-packed. The care and attention he pays to his students is very evident and quite inspirational.”<br /><br /><strong>Krishna Rajagopal</strong><br /><br />Rajagopal, professor and associate department head for education in the Department of Physics, did his undergraduate work at Queen's University in Kingston, Canada. He obtained his doctorate at Princeton University in 1993 and then spent three years at Harvard as a junior fellow. He then spent one year at Caltech before coming to MIT in 1997.<br /><br />In the Center for Theoretical Physics, Rajagopal studies QCD in extreme conditions, which requires linking usually disparate strands of theoretical physics, including particle and nuclear physics, cosmology, astrophysics, condensed matter physics and string theory.<br /><br />According to colleagues, his mastery of the subjects he teaches and the clarity with which he presents subtle and challenging concepts demonstrates his excellent skill and his sensitivity as a teacher. He uses clear visual and analytical representations and repeatedly shows how the material in his lectures is connected with other courses.<br /><br />“Fantastic, coherent, sophisticated lectures and presentations” were described by one of his students. Rajagopal builds understanding from the ground up, helping students to “learn” instead of “watch and wonder.”<br /><br /><strong>Rajeev Ram</strong><br /><br />Rajeev J. Ram is an associate director of the Research Laboratory of Electronics (RLE), and director of RLE's affiliated Center for Integrated Photonic Systems (CIPS). He received the BS in applied physics from Caltech in 1991 and the PhD in electrical engineering from the University of California, Santa Barbara, in 1997. He joined the MIT faculty in the Department of Electrical Engineering and Computer Science in 1997. <br /><br />Ram's research focuses on physical optics and electronics, including the development of novel components and systems for communications and sensing, novel semiconductor lasers for advanced fiber optic communications, and studies of fundamental interactions between electronic materials and light.<br /><br />Ram brings his skill, clarity, personal integrity and passion directly into the classroom and the teaching laboratory, according to his colleagues. His lectures linger in the mind’s eye and stay with you. The excitement in the lab is palpable, and students often stay long after assignments are complete to play with apparatuses they have constructed and can take home. <br /><br />“To further our understanding and enjoyment of the material, he took us into the lab and helped us build our own LCD cells that we could keep,” explained one of Ram’s satisfied students. “My cell worked out so well that I dragged my parents into the lab when they came to visit to show them my wonderful LCD in operation.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Analyze this]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/sloan-sports-0309.html</link>
<story_id>15081</story_id>
<featured>0</featured>
<description><![CDATA[MIT Sloan sports analytics conference reaches mainstream]]></description>
<postDate>Tue, 09 Mar 2010 04:00:02 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100308130047-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100308130047-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100308130047-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: John Marcus/www.jwmarcus.com]]></imageCredits>
<imageCaption><![CDATA[Dallas Mavericks owner Mark Cuban, left, speaks with Houston Rockets General manager Daryl Morey MBA '00, right, during the MIT Sloan Sports Analytics Conference.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100308130047-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: John Marcus/www.jwmarcus.com]]></imageCredits>
<imageCaption><![CDATA[ESPN columnist Bill Simmons, left, listens to Jonathan Kraft, right, president of The Kraft Group, which owns the New England Patriots, at the MIT Sloan Sports Analytics Conference.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Once, if you wanted to become the general manager of a professional sports team, you had to have been a great athlete. For decades, sports teams were almost exclusively run by former players.<br /><br />Times have changed. Today, an MBA can be a route into the NBA. Take Houston Rockets General Manager Daryl Morey, who has the height and bearing of a basketball star, but never played professionally. Instead, Morey graduated from the MIT Sloan School of Management in 2000, and parlayed his analytical skills into his current job. <br /><br />“All else equal, it is preferable to have played the sport,” Morey said on Saturday, during a panel at the fourth annual MIT Sloan Sports Analytics Conference, which he co-founded. But all else is not equal: Sports are awash in misguided conventional wisdom, and scores of former players have blatantly mismanaged franchises. So Morey is in the vanguard of general managers applying the analytical techniques of academia to basketball.<br /><br />In practice, that means Morey’s staff has been dissecting the sport, doing things like pinpointing the most efficient shot location (the three-pointer from the corner), and slicing defensive performance into small, measurable elements, in an attempt to quantify how effective Houston’s players are. Their forward Chuck Hayes, for example, would be considered too small for his position, at a mere 6’6,” according to the conventions of coaches and scouts, but new-school metrics indicate that Hayes is a defensive ace. “You have to have a culture where there are no bad ideas,” said Morey, meaning he encourages his staff to develop new ways of assessing talent. As a result, a year ago, unheralded Houston pushed the eventual champion Los Angeles Lakers to the seven-game limit in their playoff series.  <br /><br />To be sure, the field of sports analytics has existed for years: The baseball writer Bill James’ pioneering annual book, “The Baseball Abstract,” began reaching a national audience in 1982. The subject gained new popularity through Michael Lewis’ best-seller, <em>Moneyball</em> (Norton, 2003), which chronicled how the Oakland Athletics were using James’ principles to find undervalued players. <br /><br />The Sloan conference, which featured panels examining analytical techniques, and research papers on subjects like blocked shots in basketball (not all of them are equally valuable), reflects this wave of interest. Saturday’s event, held at the Boston Convention &amp; Exhibition Center, drew more than 1,000 attendees, up from 400 last year; half the NBA’s teams had a representative present.<br /><br /><strong>Is Plus/Minus a plus?</strong><br /><br />The current state of analytics varies widely among sports. Baseball is the most developed, because it largely consists of a series of individual confrontations between pitchers and hitters, whose results can be easily isolated. As a morning session on “Baseball Analytics” made clear, defense is the last statistical frontier of the game, and even there, statistician John Dewan estimated, observers know “60 percent” of everything they can.<br /><br />Baseball analytics are so thorough, “Now I don’t think you even have to watch baseball” to dissect it, quipped ESPN.com columnist Bill Simmons during an afternoon panel. Indeed, he added, you may not even “need to know how to hold a bat.” <br /><br />But other sports feature the simultaneous interaction of many athletes at once. Isolating an individual’s performance in these sports remains problematic.<br /><br />“Unlike baseball where you have a lot of discrete events, in football there is a lot of interplay, so it’s more difficult to analyze,” said Parag Marathe, a San Francisco 49ers executive, at a panel on “Emerging Analytics.” Consider a 25-yard run. How much of the credit goes to the running back, his blockers, or to defense lapses? “The NFL is a little bit behind” in analytics, Marathe suggested. <br /><br />To work around the problem of complex interactions in basketball, analysts are refining the concept of “Plus/Minus,” which records how many points a team scores and allows when a particular player is on the court, per 100 possessions. One winner of the conference’s research-paper contest this year attempted to improve the concept; Dallas Mavericks owner Mark Cuban has tried to use Plus/Minus, while recognizing its flaws. <br /><br />“There are all these qualifications you need to keep in mind,” Cuban said in an interview with MIT News on Saturday after he spoke on two panels. A player’s Plus/Minus can depend on the quality of his teammates, the quality of opponents, the tempo of play, and more. Currently, Miami’s Dwyane Wade leads the league in Plus/Minus, relative to how his team fares when he is not on the court, but that may just mean that he has worse teammates than Cleveland’s LeBron James.<br /><br />That said, Cuban thinks the metric works well in evaluating the success of different five-man lineups, not just single players. “We’ve adjusted lineups in the playoffs based on our Plus/Minus numbers,” Cuban said. In 2005, Dallas lost the first two games of its first-round series to Houston, which was using a smaller, quicker lineup. The Mavericks studied the Plus/Minus numbers, reduced lumbering center Erick Dampier’s minutes, and rallied to win the series in 7 games. <br /><br />“Mark Cuban helped break me out of that mold of looking at traditional statistics,” recounted Avery Johnson, the Mavericks’ coach at the time, while speaking on a “Coaching Analytics” panel. “Using Plus/Minus helped me out a lot in terms of my substitutions.”<br /><br />Well, until the team faltered. In 2007, Dallas entered the playoffs with a league-best 67-15 record. But as Johnson recounted, the numbers showed that the Mavericks fared worse against their first-round opponent, the small-but-quick Golden State Warriors, with Dampier on the court. Johnson benched Dampier, the team’s starting center, for the series’ first game. “It was the right thing to do,” Johnson said. But his players did not like the adjustment; the Warriors quickly knocked out the Mavericks in a stunning upset. <br /><br /><strong>The limits of metrics</strong><br /><br />As the Mavericks’ experience suggests, analytics have limitations. General tendencies may not be borne out in specific situations. Moreover, “I think there is an onus on whoever is dispensing that information” to explain it clearly and persuasively to everyone else, asserted Simmons, whose own recent tome, <em>The Book of Basketball</em> (ESPN 2009), mixes empirical data and subjective impressions while judging players and teams in NBA history. <br /><br />And Morey noted another problem: In a business with short careers, changing circumstances may make some sports analysis irrelevant. “I think there are fundamental things that can be solved,” said Morey. “But by the time you have enough confidence in them, the world has changed.” <br /><br />What has also changed, though, is that savvy sports fans now envision a future in the business. Take Matthew Martell, a senior associate at Octothorpe Software, a Vancouver firm that designs decision-making programs. Martell, capable of talking knowledgeably about sports-analytics problems in basketball, football, and soccer, made a 12-hour trip from British Columbia on Thursday, changing planes twice, to attend the event. “This is where you want to be, to meet and see the people who really know analytics,” said Martell. “It’s incredible to be here.” <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Action needed to save climate, create jobs]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<link>http://web.mit.edu/newsoffice/2010/energy-conference-0309.html</link>
<story_id>15082</story_id>
<featured>0</featured>
<description><![CDATA[MIT Energy Conference speakers see need to boost clean-energy businesses through a price on carbon and incentives for manufacturing]]></description>
<postDate>Tue, 09 Mar 2010 04:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100308203257-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100308203257-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100308203257-0.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy of the MIT Energy Conference]]></imageCredits>
<imageCaption><![CDATA[Democratic Sen. Jeff Bingaman of New Mexico delivers a keynote address at the annual MIT Energy Conference.]]></imageCaption>
</image>
<body><![CDATA[Clean-energy technologies offer the promise of revitalizing a dwindling base of manufacturing jobs in the United States while also addressing the problems of climate change and energy security, said Democratic Sen. Jeff Bingaman of New Mexico in a keynote speech at the annual MIT Energy Conference. But as great as the potential may be, it won't be realized unless substantial new policies and regulations are put in place — and the chances of that happening anytime soon are slim, he said. <br /><br />While Congress and the Obama administration have taken some first steps, he said, "the policies that have been enacted to date are clearly not sufficient to establish the U.S. as the leader in clean technology." Right now, he said, "90 percent of the production capacity for new clean technology is outside the United States."<br /><br />Bingaman added that "China is moving ahead very aggressively," and the United States needs to act soon to reverse the present tide. For example, while lithium-ion battery technology was developed in this country, only 1 percent of the manufacturing of these batteries — now used mostly in portable electronics devices, but seen as a key to the next generation of electric vehicles — takes place in the U.S.<br /><br />That view of great potential but political stagnation was echoed by several speakers at the conference, which was held on March 6 at the Sheraton Boston. Speakers representing various levels of government, industry, academic research, international organizations, and the financial sector, among others, tended to agree that government action will play a crucial and decisive role in determining how the world responds to the challenges of growing energy demand and the risks of climate change, and how different nations' economies fare as a result.<br /><br />There were two areas where clear government action was seen as being especially important: first and foremost, setting in place a clear and predictable system that puts a price on emissions of carbon, whether it be in the form of a cap-and-trade system, as the U.S. Congress has been considering, or simply a direct tax on carbon, which many consider to be a better option but not a politically feasible one; and second, offering financial support for new energy technologies, not only at the research stage but also in establishing manufacturing capacity.<br /><br /><strong>Regaining the competitive edge</strong><br /><br />Globally, the trend toward non-fossil-fuel energy is clear: In 2008, Bingaman said, for the first time global investments in clean energy technology exceeded those for fossil fuel technology. But for U.S. competitiveness, the trend is not encouraging. For many years, he said, the U.S. made the technological breakthroughs, while other countries, especially Japan, provided the follow-through. But now, other countries are joining in the follow-through, and "the U.S. no longer has a monopoly on the breakthroughs."<br /><br />There are ways to turn that around, suggested Bingaman, who chairs the Senate Energy and Natural Resources Committee, but only with substantial policy changes. Clean technology "offers the opportunity to revitalize our manufacturing sector," he said, but in the past the kinds of incentives the government has provided "were concentrated downstream," on the consumers or suppliers rather than on the manufacturing end of the spectrum, and the policies have tended to come and go with changing political tides, resulting in "government-driven boom-and-bust cycles."<br /><br />To change that, several senators and congressmen, with President Obama's support, are urging the creation of a substantial loan-guarantee program for clean-tech manufacturing. The biggest impact of all, Bingaman said, could come from improvements in energy efficiency, which could both produce a dramatic lowering of greenhouse emissions and oil imports, and at the same time create large numbers of long-term jobs. But to make that happen requires some form of price on carbon-emitting fuels, he said.<br /><br />That's not likely in this country anytime soon, he added. "Getting comprehensive climate-change legislation is not that promising this year," he said, though he still has hope for some steps in that direction. And what might be possible next year depends on the outcome of the fall elections, he said.<br /><br /><strong>Looking for innovation — and consistency</strong><br /><br />"The government's role is vital, and temporary," said David Anthony, managing partner of the investment company 21Ventures, at the conference's closing panel discussion about the financing of energy technology. He stressed that the government's main role is to invest in basic research and development, at the early stages where private financing is too difficult to secure. "The government needs to fix the problem, and then get out of the way," he said.<br /><br />But while the legislative process is moving slowly, many segments of industry are moving ahead. "It's easy for me to be pro-climate legislation — it is in my economic self-interest," said John Rowe, CEO of Exelon, the nation's largest electric utility company and owner of the nation's largest fleet of nuclear power plants. Rowe, one of the conference's keynote speakers, explained that the greatest danger, from a business point of view, lies in "continuing to deal with energy in ways that are haphazard," as opposed to setting a clear policy in place that businesses can base their plans on.<br /><br />Rowe, whose Chicago-based electric utility holding company has already closed many of its coal-burning plants and plans to eliminate all of its 15 million tons of greenhouse gas emissions by 2020, said that addressing the problems of greenhouse-gas emissions will depend on putting a price on carbon, either through a cap-and-trade system, or a carbon tax.<br /><br />"We ought to have a predictable, confident and decisive policy on climate change," he said. Public resistance is largely based on incorrect assumptions, he suggested, because polls show people oppose carbon taxes or cap-and-trade systems because they believe those will cost them money, but they support renewable energy standards — requiring utilities to provide a set percentage of their power from renewable energy — because they believe those are cost-free. That kind of free lunch, he suggested, is an illusion.<br /><br />But if policies are put in place that set a realistic price on carbon emissions, he said, the marketplace will do the rest. With such a policy, "you'll be surprised at how much can happen in 10 years."<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Revolutionizing medicine, one chip at a time]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/biomed-diag-0309.html</link>
<story_id>15083</story_id>
<featured>0</featured>
<description><![CDATA[Low-power computer chips allow engineers to design wearable and implantable devices to monitor patients.]]></description>
<postDate>Tue, 09 Mar 2010 04:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100308160138-1.png</thumbURL>
<smallURL width='140' height='137'>http://web.mit.edu/newsoffice/images/article_images/w140/20100308160138-1.jpg</smallURL>
<fullURL width='368' height='360'>http://web.mit.edu/newsoffice/images/article_images/20100308160138-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Eric Winokur]]></imageCredits>
<imageCaption><![CDATA[MIT engineers have designed this wearable ECG monitor that runs on very little power and could replace cumbersome devices now used to monitor heart patients.]]></imageCaption>
</image>
<body><![CDATA[In the past several decades, microchips have transformed consumer electronics, enabling new products from digital watches and pocket-sized calculators to laptop computers and digital music players.<br /><br />The next wave of this electronics revolution will involve biomedical devices, say electrical engineers in MIT’s Microsystems Technology Laboratory (MTL) who are working on tiny, low-power chips that could diagnose heart problems, monitor patients with Parkinson’s disease or predict seizures in epileptic patients. Such wearable or implantable devices could transform the way medicine is practiced and help cut the costs of expensive diagnostic tests, says Dennis Buss, former vice president of silicon technology development at Texas Instruments.<br /><br />“Microelectronics have the potential to reduce the cost of health care in the same way they reduced the costs of computing in the 1980s and communications in the 1990s,” says Buss, a visiting scientist at MIT. On a limited scale, this is already taking place. For example, one of the first successful applications of microelectromechanical systems (MEMS) to medicine was the development of $10 disposable blood pressure sensors, which have been in use for over a decade and replaced sensors that cost hundreds of dollars.<br /><br />Professor Charles Sodini, one of the MIT researchers involved in the effort, says the burgeoning field holds great potential for MIT and the greater Boston area because of the opportunities for collaboration between engineers, physicians and industry. “I want to see Boston become the Silicon Valley of medical electronic systems,” he says.<br /><br />The market for MEMS for biomedical applications is more than $1 billion, and that could grow close to 100-fold by 2015, according to a 2006 market report from MedMarket Diligence.<br /><br /><strong>Beating hearts</strong><br /><br />The key to developing small wearable and implantable medical monitors is an ultra-low-power chip for interfacing to biomedical sensors, signal processing, energy processing and communications, developed by the research group of MTL Director Anantha Chandrakasan.<br /><br />Ultimately, Sodini and others at MTL hope to use that chip as the core of a device that can monitor a range of vital signs — heart rate, breathing rate, blood pressure, pulse oxygenation and temperature. For now, they’re starting with a monitor that measures and records electrocardiograms (ECGs).<br /><br />An unobtrusive, comfortable ECG monitor that patients could wear as they go about their normal lives might offer a doctors a more thorough picture of heart health than the lab tests now used, says Collin Stultz, an MIT associate professor of electrical engineering and health sciences and technology and a cardiologist working on the project. Cardiologists can order up treadmill stress tests, MRIs and CT scans, among other diagnostics, but “all of these tests are done in contrived settings,” says Stultz. “Data obtained from more realistic, ‘at home’ settings may provide added information that can reveal potential problems.” Furthermore, standard tests can cost from a few hundred to a few thousand dollars.<br /><br />Doctors often ask recent heart attack victims, and other patients suspected of having heart issues, to wear an ECG monitor as a Holter monitor for a few days. However, the device, which consists of several electrodes that stick to the chest, plus a bulky battery pack carried at the hip, is cumbersome and doesn’t have the memory to store much data.<br /><br />In contrast, the new MIT monitor is an L-shaped device, about 4 inches along each side, that sticks to the chest and can be worn comfortably, with no external wires protruding. It can store up to two weeks of data in flash memory, and requires just two milliwatts of power. Eventually, the researchers hope to build chips that can harvest energy from the body of the person wearing the device, eliminating the need for a battery.<br /><br />Doctors can use ECG data — which provides information on the electrical health of the heart — to help spot future problems. Stultz, working with MIT Professor John Guttag and recent PhD recipient Zeeshan Syed, has designed a computer algorithm that uses ECG data to assess risk of death in heart patients. They found that higher variability in heartbeat shapes in data recorded the day after a heart attack correlates with an eightfold increase in the risk of cardiac death within 90 days in some patient populations.<br /><br />Currently that analysis can only be done after the data is downloaded from the chip, but eventually Stultz hopes to incorporate the algorithm into the chip itself. He envisions that the device could be equipped with an alarm that would alert the patient and/or doctor that a heart attack is imminent. It could also serve as an early detection system for longer-term problems, letting doctors know they may need to perform additional tests, alter the patient’s medication or perform surgery.<br /><br />The researchers have built a prototype and plan to start testing the device in healthy subjects this spring, followed by trials in patients with cardiovascular disease.<br /><br /><strong>New directions</strong><br /><br />While Stultz and colleagues are focusing on wearable devices, other MIT engineers are working on implantable electronics for medical monitoring. To do that, they need to overcome a significant challenge: how to run the device indefinitely without a battery that needs recharging. To solve that problem, Associate Professor Joel Dawson is working on a device that stores energy in an ultracapacitor, which doesn’t wear out like batteries do. He hopes to use the device, which would be about the size of a grain of rice, to measure tremors and shaking in patients with Parkinson’s disease. <br /><br />Dawson is working on that project with neurologist Seward Rutkove of Beth Israel Hospital. That kind of collaboration between engineer and physician is exactly what Sodini would like to see happen with all of MTL’s biomedical projects. “We start out working with physicians so they can help define the problem, and they can start testing the devices in the clinic early in the process,” he says.<br /><br />Other projects underway at MTL include tiny ultrasound devices and “lab on a chip” devices that can perform diagnostic tests on body fluids. Engineers are also working on the best ways to wirelessly transmit data from wearable or implanted devices to a cell phone or computer. <br /><br />While those applications are promising, the future of biomedical electronics likely holds even more potential than we can imagine, says Buss.<br /><br />“We will be using electronics in medical ways we don’t even conceive of yet,” he says. “When we started using cell phones, we had no idea we would be playing games and watching TV and surfing the Internet the way we do now.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Listening in on single cells]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/cell-sensor-0308.html</link>
<story_id>15076</story_id>
<featured>0</featured>
<description><![CDATA[A novel sensor array is the first to detect single molecules produced by living cells.]]></description>
<postDate>Mon, 08 Mar 2010 04:00:02 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100305162710-0.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100305162710-0.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100305162710-0.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[Michael Strano, MIT associate professor of chemical engineering]]></imageCaption>
</image>
<body><![CDATA[MIT researchers have built the first sensor array that can detect single molecules emitted by a living cell. Their sensor targets hydrogen peroxide and could help scientists learn more about that molecule’s role in cancer.<br /><br />Hydrogen peroxide has long been known to damage cells and their DNA, but scientists have recently uncovered evidence that points to a more beneficial role: it appears to act as a signaling molecule in a critical cell pathway that stimulates cell growth, among other functions.<br /><br />When that pathway goes awry, cells can grow out of control and become cancerous, so understanding hydrogen peroxide’s role could lead to new targets for potential cancer drugs, says Michael Strano, MIT associate professor of chemical engineering and leader of the research team. Strano and his colleagues describe their new sensor array, which is made of carbon nanotubes, in the March 7 online edition of <em>Nature Nanotechnology</em>. <br /><br />Strano’s team is also working on carbon nanotube sensors for other molecules, and within the past year has successfully tested and published sensors for nitric oxide and ATP (the molecule that carries energy within a cell). <br /><br />“The list of biomolecules that we can now detect very specifically and selectively is growing rapidly,” says Strano, who also points out that the ability to detect and count single molecules sets carbon nanotubes apart from many other nanosensor platforms, including electrochemical, electromechanical cantilevers and surface acoustic wave sensors.<br /><br /><strong>Nanotube array</strong><br /><br />In the new study, Strano’s team used the carbon nanotube array to study the flux of hydrogen peroxide that occurs when a common growth factor called EGF activates its target, a receptor known as EGFR, which is located on cell surfaces. For the first time, the team showed that hydrogen peroxide levels more than double when EGFR is activated.<br /><br />EGF and other growth factors induce cells to grow or divide through a complex cascade of reactions inside the cell. It’s still unclear exactly how hydrogen peroxide affects this process, but Strano speculates that it may somehow amplify the EGFR signal, reinforcing the message to the cell. Because hydrogen peroxide is a small molecule that doesn’t diffuse far, the signal would be limited to the cell where it was produced.<br /><br />The team also found that in skin cancer cells, believed to have overactive EGFR activity, the hydrogen peroxide flux was 10 times greater than in normal cells. Because of that dramatic difference, Strano believes this technology could be useful in building diagnostic devices for some types of cancer.<br /><br />“You could envision a small handheld device, for example, which your doctor could use to assay tissue in a minimally invasive manner and tell if this pathway is corrupted,” he says.<br /><br />The sensor consists of a film of carbon nanotubes embedded in collagen. Cells can grow on the collagen surface, and the collagen also attracts and traps hydrogen peroxide released by the cell. When the nanotubes come in contact with the trapped hydrogen peroxide, their fluorescence flickers.  By counting the flickers, one can obtain an accurate count of the incident single molecules.<br /><br />The new sensor represents “an excellent example of the application of nanotechnology to address fundamental questions in biology,” says Ravi Kane, professor of chemical and biological engineering at Rensselaer Polytechnic Institute. <br /><br />Strano points out that this is the first time an array of sensors with single-molecule specificity has ever been demonstrated.  He and his colleagues derived mathematically that such an array could distinguish “near field” molecular generation from that which takes place far from the sensor surface.  <br /><br />“Arrays of this type have the ability to distinguish, for example, if single molecules are coming from an enzyme located on the cell surface or from deep within the cell,” says Strano.<br /><br />In future work, researchers in Strano’s lab plan to study different forms of the EGF receptor to better characterize the hydrogen peroxide flux and its role in cell signaling. They have already discovered that molecules of oxygen are consumed to generate the peroxide.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Insulators made into conductors]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/heat-nanofibers-0308.html</link>
<story_id>15077</story_id>
<featured>0</featured>
<description><![CDATA[MIT team coaxes polymers to line up, transforming them into materials that could dissipate heat.]]></description>
<postDate>Mon, 08 Mar 2010 04:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100305154751-0.png</thumbURL>
<smallURL width='140' height='164'>http://web.mit.edu/newsoffice/images/article_images/w140/20100305154751-0.jpg</smallURL>
<fullURL width='368' height='432'>http://web.mit.edu/newsoffice/images/article_images/20100305154751-0.jpg</fullURL>
<imageCredits><![CDATA[Illustration courtesy of Gang Chen]]></imageCredits>
<imageCaption><![CDATA[The new method involves pulling a thin thread of material (top) from a liquid solution (bottom), and in the process the individual polymer filaments, which start out as a tangled mass, become very highly aligned.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='734'>http://web.mit.edu/newsoffice/images/article_images/20100305144540-2.jpg</fullURL>
<imageCredits><![CDATA[Illustration courtesy of Gang Chen]]></imageCredits>
<imageCaption><![CDATA[At top, an illustration of the tangled nature of the polymer filaments, with heat-stopping voids indicated as dark blobs. When drawn and heated into a thin thread (bottom), the molecules line up and the voids are compressed, making the material a good conductor. ]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Most polymers — materials made of long, chain-like molecules — are very good insulators for both heat and electricity. But an MIT team has found a way to transform the most widely used polymer, polyethylene, into a material that conducts heat just as well as most metals, yet remains an electrical insulator.<br /><br />The new process causes the polymer to conduct heat very efficiently in just one direction, unlike metals, which conduct equally well in all directions. This may make the new material especially useful for applications where it is important to draw heat away from an object, such as a computer processor chip. The work is described in a paper published on March 7 in <em>Nature Nanotechnology</em>.<br /><br />The key to the transformation was getting all the polymer molecules to line up the same way, rather than forming a chaotic tangled mass, as they normally do. The team did that by slowly drawing a polyethylene fiber out of a solution, using the finely controllable cantilever of an atomic force microscope, which they also used to measure the properties of the resulting fiber.<br /><br />This fiber was about 300 times more thermally conductive than normal polyethylene along the direction of the individual fibers, says the team’s leader, Gang Chen, the Carl Richard Soderberg Professor of Power Engineering and director of MIT’s Pappalardo Micro and Nano Engineering Laboratories. <br /><br />The high thermal conductivity could make such fibers useful for dissipating heat in many applications where metals are now used, such as solar hot water collectors, heat exchangers and electronics.<br /><br />Chen explains that most attempts to create polymers with improved thermal conductivity have focused on adding in other materials, such as carbon nanotubes, but these have achieved only modest increases in conductivity because the interfaces between the two kinds of material tend to add thermal resistance. “The interfaces actually scatter heat, so you don’t get much improvement,” Chen says. But using this new method, the conductivity was enhanced so much that it was actually better than that of about half of all pure metals, including iron and platinum.<br /><br />Producing the new fibers, in which the polymer molecules are all aligned instead of jumbled, required a two-stage process, explains graduate student Sheng Shen, the lead author of the paper. The polymer is initially heated and drawn out, then heated again to stretch it further. “Once it solidifies at room temperature, you can’t do any large deformation,” Shen says, “so we heat it up twice.”<br /><br />Even greater gains are likely to be possible as the technique is improved, says Chen, noting that the results achieved so far already represent the highest thermal conductivity ever seen in any polymer material. Already, the degree of conductivity they produce, if such fibers could be made in quantity, could provide a cheaper alternative to metals used for heat transfer in many applications, especially ones where the directional characteristics would come in handy, such as heat-exchanger fins (like the coils on the back of a refrigerator or in an air conditioner), cell-phone casings or the plastic packaging for computer chips. Other applications might be devised that take advantage of the material’s unusual combination of thermal conductivity with light weight, chemical stability and electrical insulation. <br /><br />So far, the team has just produced individual fibers in a laboratory setting, Chen says, but “we’re hoping that down the road, we can scale up to a macro scale,” producing whole sheets of material with the same properties.<br /><br />Ravi Prasher, an engineer at Intel, says that “the quality of the work from Prof. Chen’s group has always been phenomenal,” and adds that “this is a very significant finding” that could have many applications in electronics. The remaining question, he says, is “how scalable is the manufacturing of these fibers? How easy is it to integrate these fibers in real-world applications?”<br /><br />This work, which also included Chen’s former graduate students Asegun Henry and Jonathan Tong, was supported by the National Science Foundation and the Department of Energy’s Office of Basic Energy Sciences.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Big power from tiny wires]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/thermopower-waves-0308.html</link>
<story_id>15078</story_id>
<featured>0</featured>
<description><![CDATA[New discovery shows carbon nanotubes can produce powerful waves that could be harnessed for new energy systems.]]></description>
<postDate>Mon, 08 Mar 2010 04:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100305145954-1.png</thumbURL>
<smallURL width='140' height='99'>http://web.mit.edu/newsoffice/images/article_images/w140/20100305145954-1.jpg</smallURL>
<fullURL width='368' height='262'>http://web.mit.edu/newsoffice/images/article_images/20100305145954-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
<imageCaption><![CDATA[A carbon nanotube (shown in illustration) can produce a very rapid wave of power when it is coated by a layer of fuel and ignited, so that heat travels along the tube.]]></imageCaption>
</image>
<body><![CDATA[A team of scientists at MIT have discovered a previously unknown phenomenon that can cause powerful waves of energy to shoot through minuscule wires known as carbon nanotubes. The discovery could lead to a new way of producing electricity, the researchers say.<br /><br />The phenomenon, described as <a href="http://web.mit.edu/newsoffice/2010/explained-thermoelectricity-0427.html" target="_blank">thermopower</a> waves, “opens up a new area of energy research, which is rare,” says Michael Strano, MIT’s Charles and Hilda Roddey Associate Professor of Chemical Engineering, who was the senior author of a paper describing the new findings that appeared in <em>Nature Materials</em> on March 7. The lead author was Wonjoon Choi, a doctoral student in mechanical engineering.<br /><br />Like a collection of flotsam propelled along the surface by waves traveling across the ocean, it turns out that a thermal wave — a moving pulse of heat — traveling along a microscopic wire can drive electrons along, creating an electrical current. <br /><br />The key ingredient in the recipe is carbon nanotubes — submicroscopic hollow tubes made of a chicken-wire-like lattice of carbon atoms. These tubes, just a few billionths of a meter (nanometers) in diameter, are part of a family of novel carbon molecules, including buckyballs and graphene sheets, that have been the subject of intensive worldwide research over the last two decades. <br /><strong><br />A previously unknown phenomenon</strong><br /><br />In the new experiments, each of these electrically and thermally conductive nanotubes was coated with a layer of a reactive fuel that can produce heat by decomposing. This fuel was then ignited at one end of the nanotube using either a laser beam or a high-voltage spark, and the result was a fast-moving thermal wave traveling along the length of the carbon nanotube like a flame speeding along the length of a lit fuse. Heat from the fuel goes into the nanotube, where it travels thousands of times faster than in the fuel itself.  As the heat feeds back to the fuel coating, a thermal wave is created that is guided along the nanotube. With a temperature of 3,000 kelvins, this ring of heat speeds along the tube 10,000 times faster than the normal spread of this chemical reaction. The heating produced by that combustion, it turns out, also pushes electrons along the tube, creating a  substantial electrical current. <br /><br />Combustion waves — like this pulse of heat hurtling along a wire — “have been studied mathematically for more than 100 years,” Strano says, but he was the first to  predict that such waves could be guided by a nanotube or nanowire and that this wave of heat could push an electrical current along that wire.<br /><br />In the group’s initial experiments, Strano says, when they wired up the carbon nanotubes with their fuel coating in order to study the reaction, “lo and behold, we were really surprised by the size of the resulting voltage peak” that propagated along the wire.<br /><br />After further development, the system now puts out energy, in proportion to its weight, about 100 times greater than an equivalent weight of lithium-ion battery. <br /><br />The amount of power released, he says, is much greater than that predicted by thermoelectric calculations. While many semiconductor materials can produce an electric potential when heated, through something called the Seebeck effect, that effect is very weak in carbon. “There’s something else happening here,” he says. “We call it electron entrainment, since part of the current appears to scale with wave velocity.”<br /><br />The thermal wave, he explains, appears to be entraining the electrical charge carriers (either electrons or electron holes) just as an ocean wave can pick up and carry a collection of debris along the surface. This important property is responsible for the high power produced by the system, Strano says.<br /><br /><strong>Exploring possible applications</strong><br /><br />Because this is such a new discovery, he says, it’s hard to predict exactly what the practical applications will be. But he suggests that one possible application would be in enabling new kinds of ultra-small electronic devices — for example, devices the size of  grains of rice, perhaps with sensors or treatment devices that could be injected into the body. Or it could lead to “environmental sensors that could be scattered like dust in the air,” he says.<br /><br />In theory, he says, such devices could maintain their power indefinitely until used, unlike batteries whose charges leak away gradually as they sit unused. And while the individual nanowires are tiny, Strano suggests that they could be made in large arrays to supply significant amounts of power for larger devices.<br /><br />The researchers also plan to pursue another aspect of their theory: that by using different kinds of reactive materials for the coating, the wave front could oscillate, thus producing an alternating current. That would open up a variety of possibilities, Strano says, because alternating current is the basis for radio waves such as cell phone transmissions, but present energy-storage systems all produce direct current. “Our theory predicted these oscillations before we began to observe them in our data,” he says.<br /><br />Also, the present versions of the system have low efficiency, because a great deal of power is being given off as heat and light. The team plans to work on improving that efficiency.<br /><br />Ray Baughman, director of the Nanotech Institute at the University of Texas at Dallas, who was not involved in this work, calls the research “stellar.”<br /><br />The work, Baughman says, “started with a seminal initial idea, which some might find crazy, and provided exciting experimental results, the discovery of new phenomena, deep theoretical understanding, and prospects for applications.” Because it uncovered a previously unknown phenomenon, he says, it could open up “an exciting new area of investigation.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Financial aid budget to increase more sharply than tuition and fees]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/financial-aid.html</link>
<story_id>15075</story_id>
<featured>0</featured>
<description><![CDATA[Students to be asked to contribute more from summer and part-time earnings]]></description>
<postDate>Fri, 05 Mar 2010 16:14:05 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100305121530-1.png</thumbURL>
<smallURL width='140' height='138'>http://web.mit.edu/newsoffice/images/article_images/w140/20100305121530-1.jpg</smallURL>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100305121530-1.jpg</fullURL>
</image>
<body><![CDATA[MIT will raise undergraduate tuition and fees next year by 3.8 percent — among the smallest percentage increases in more than a quarter-century — while also increasing the undergraduate financial aid budget by 6.7 percent. <br /><br />The moves, which come as MIT is poised to meet its two-year target of reducing its operating budget by around $120 million, mark the 11th consecutive year in which the increase in the Institute’s financial aid budget has greatly outpaced the rise in tuition — a record that underscores MIT’s continued efforts to cushion the impact of price increases on families with need. <br /><br />The changes in tuition and financial aid were announced at the MIT Corporation meeting on Friday, March 5.<br /><br />Dean for Undergraduate Education Daniel Hastings said that in light of continued weakness in the U.S. and global economies, MIT recognizes the importance of maintaining its core admissions and financial aid policies. Currently, 64 percent of MIT students receive need-based financial aid, and 35 percent of MIT students receive sufficient scholarship funding that they pay no tuition. Hastings noted that MIT will continue to make all undergraduate admission decisions without regard to family financial circumstances, will award all aid based on financial need, and will meet the full need of each student. But he also said that even as MIT increases the undergraduate financial aid budget, students will be expected to increase their contributions from summer and part-time work. For most students, this increase will range from $550 to $1,050. <br /><br />“Given MIT’s long-standing commitment to need-based financial aid, we will maintain our strong partnership with families in the financing of an MIT education. However, we will ask our students to contribute to their education to a slightly greater degree through their own earnings,” Hastings said. “We will continue to ensure access and affordability because we want students from all economic backgrounds to know they can afford to attend MIT — and that once they are here, they will thrive.”<br /><br />The 2010-2011 undergraduate financial aid budget will total $87 million, up from $81.5 million in 2009-2010. Tuition and fees will total $39,212, versus $37,782 in the current academic year. Estimated undergraduate student expenses for tuition, fees, housing and dining costs, will rise 2.7 percent to $50,446 in 2010-2011.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT opens new Media Lab Complex]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/media-lab-0304.html</link>
<story_id>15067</story_id>
<featured>0</featured>
<description><![CDATA[Fumihiko Maki-designed building will support technological innovation]]></description>
<postDate>Fri, 05 Mar 2010 13:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100304124958-1.png</thumbURL>
<smallURL width='140' height='123'>http://web.mit.edu/newsoffice/images/article_images/w140/20100304124958-1.jpg</smallURL>
<fullURL width='368' height='325'>http://web.mit.edu/newsoffice/images/article_images/20100304124958-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Andy Ryan]]></imageCredits>
<imageCaption><![CDATA[The new Media Lab complex at night.]]></imageCaption>
</image>
<body><![CDATA[On Friday, March 5, MIT officially opens the Media Lab Complex, designed  by Pritzker Prize-winning architect Fumihiko Maki and Associates in  association with Leers Weinzapfel Associates. The building marks a new  era of innovation for the world-renowned Media Lab and for a range of  art, design, and technology-related programs in the School of  Architecture + Planning, of which the Media Lab is a part.  <br /><br />Located  on the corner of Amherst and Ames Streets in the heart of the MIT  campus, the six-story, 163,000-square-foot building is adjacent to and  carefully integrated into the existing home of the Media Lab, known as  the Wiesner Building, designed by MIT alumnus I.M. Pei. Together, the  two landmark buildings — connected on several floors — will create an  exceptional environment for research, creativity, and discovery.<br /><br />"In  the best MIT tradition of inventing the future, the new Media Lab  Complex expands a legendary workshop where creativity and innovation  continually transform the intersection of people and machines," said MIT  President Susan Hockfield. "This magnificent new facility unites  researchers from across our campus in advancing technologies that  amplify the human experience."<br /><br />The new six-story complex features  an open, flexible, atelier-style layout designed to support the unique  cross-disciplinary research style of the Media Lab and other academic  units that will occupy the building. Laboratories and workspaces are  arranged around light-filled central atria, with spectacular views of  the Charles River and the Boston skyline to the south. Lecture halls and  other public gathering spaces occupy the upper floors and serve to draw  visitors up through the building.<br /><br />“The magic of the Media Lab is  its ability to bring together researchers from an eclectic range of  disciplines — engineers, computer scientists, artists, musicians, and  others — who work together collaboratively to invent technologies that  improve people's lives and transform society,” says Frank Moss, Director  of the Media Lab. “The openness and transparency of this incredible new  building will support the Lab’s unique style of research and allow us  to take on the major challenges that confront the world in the 21st  century, such as human health, education and sustainable cities." <br /><br />The  building’s several double-height, glass-enclosed research laboratories —  home to research groups such as Camera Culture, Lifelong Kindergarten  and Tangible Media — are vertically offset from one another. This makes  possible long and often surprising vistas through the building  —horizontally, vertically, and diagonally — that will serve to make the  research work highly visible. It will also enhance the sense of  community in the building.  The aluminum and glass curtain walls that  surround the steel-framed building extend the feeling of openness and  transparency to the exterior and make the building appear like a  luminous jewel at night.<br /> <br />“Fumihiko Maki is one of the world’s  most elegant designers,” says Adèle Naudé Santos, dean of the MIT School  of Architecture + Planning. “The precision of his vision and his  exacting attention to detail lend his work a rare clarity and serenity.  This exquisite building is one of his best, and we are privileged to  have it on our campus.” <br /><br />Now celebrating its 25th anniversary,  the MIT Media Lab has long been at the vanguard of new technology. Many  of the Lab’s inventions — such as electronic ink, wearable computers,  and early platforms for social networking — helped ignite the digital  revolution. More recently, the Lab has expanded its focus into “human  adaptability,” with research projects involving affective computing, 6-D  imaging and the future of the automobile. <br /><br />The Media Lab Complex  will also become home to the Jerome Lemelson Center for Inventive  Thinking and to the Okawa Center for Future Children, funded by Isao  Okawa, the late chairman of CSK Corporation and SEGA Enterprises, Ltd.  The center will focus on transforming the ways in which children live,  learn and play in the digital age.<br /><br />The complex will also house  several programs in the School of Architecture + Planning, including the  newly formed Art, Culture and Technology Program. <br /><br />The Media Lab  Complex builds on an MIT tradition of architectural excellence, from  the neoclassical design of its original architect, William Welles  Bosworth, to the mid-20th-century modernism of Alvar Aalto and Eero  Saarinen. An ambitious new building program, begun a decade ago, added  nearly one million square feet to the Cambridge campus and utilized the  talents of some of the world’s finest architects and planners, including  Charles Correa, Frank Gehry, Steven Holl and Kevin Roche. Other  buildings currently under construction include the Koch Institute for  Integrative Cancer Research at MIT, designed by Ellenzweig; and a new  building for the Sloan School of Management, designed by Moore Ruble  Yudell Architects &amp; Planners in association with Bruner / Cott  Architects.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Hunt Allcott on behavioral economics and the energy crisis]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/3q-alcott-0305.html</link>
<story_id>15069</story_id>
<featured>0</featured>
<description><![CDATA[Can understanding human irrationality help solve our energy problems? An MIT researcher explains]]></description>
<postDate>Fri, 05 Mar 2010 05:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100304141044-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100304141044-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100304141044-1.jpg</fullURL>
<imageCaption><![CDATA[Behavioral economist Hunt Allcott]]></imageCaption>
</image>
<body><![CDATA[<em>Behavioral economics is used to examine how consumers make decisions about everything from their life savings to which brands of jam they select in a supermarket. Hunt Allcott, a behavioral economist with a two-year appointment as the Energy and Society Fellow in MIT’s Department of Economics and the MIT Energy Initiative, wants to apply his field’s insights to the realm of energy use. <br /><br />In the latest issue of </em>Science<em>, Allcott and co-author Sendhil Mullainathan, of Harvard, advocate passage of a bill currently in Congress that would fund more behavioral research about energy consumption. The authors also note initiatives like that of OPOWER, a Virginia company, which has found that the user-friendly energy reports it sends to consumers can influence behavior enough to reduce household energy use by 2 percent, at minimal cost (OPOWER is an affiliate of Ideas42, an MIT-linked think tank to which Allcott also belongs). MIT News spoke with Allcott about how behavioral economics addresses our energy needs. </em><br /><br /><strong>Q.</strong> Why should we invest in behavioral research pertaining to energy efficiency, and what are the specific kinds of research we can do right now?<br /><br /><strong>A.</strong> It’s an economic argument. There are lots of different technology-centered R&amp;D investments that we can and do make: Fuel cells, hybrid vehicles, wind power. But we can also invest in new social science research that can inform policies and programs that encourage people to consume energy differently. The argument Sendhil and I make is that we have to compare across all of these classes and say, “What’s cost-effective in terms of achieving our goals?” We use the results from recent large-scale energy conservation programs that were motivated by behavioral science to show that behavioral science R&amp;D is an underexplored and potentially cost-effective approach. <br /><br />Let me give you two examples of how economics can inform energy efficiency policy. First, much of the policy-oriented research in behavioral economics has been about identifying barriers in individual decision making that keep us from making the choices that, in a perfect world, we would have wanted to make for ourselves. Perhaps the leading example of this has been helping people to make better choices about how much to invest in their retirement plans, and what funds to hold. One of the things I’m interested in is to document whether consumers make similar types of mistakes when they go to buy air conditioners, or cars. It’s a complex decision, and the benefits of energy efficiency occur incrementally and in the future, so those benefits are not very salient. Depending on the types of mistakes that consumers are making — if we conclude they are indeed making mistakes — we can design policies to nudge them in ways that they would find helpful.<br /><br />Second, economists tend to think of energy consumption as driven primarily by prices. Indeed, in many domains, I think we reflexively focus on price at the expense of failing to model other important drivers of consumer choice. There’s a lot of research in behavioral economics that suggests we can influence people to conserve energy, or do other things, in many ways other than raising prices. I think an important research area is to document whether policies and programs based on these sorts of insights can increase welfare or be cost-effective in reducing carbon emissions.<br /><br /><strong>Q.</strong> To what extent will consumers make different choices if they simply have the facts about energy explained to them in a clear manner? <br /><br /><strong>A.</strong> The effect of clearer information is an empirical question that often has surprising answers. One example of this is from OPOWER, a company that our research group interacts with a lot. OPOWER sends home energy use reports to households that compare those households to their neighbors and give energy conservation tips. The information in these reports is very similar to what’s already on a utility bill: How much did you spend this month, how much did you spend this year, here’s where you can get compact fluorescent lightbulbs. But something about the way they’re presenting it — presumably the way they use comparisons to neighbors — seems to be very powerful. I’m not sure it would have been obvious to any of us 10 years ago or three years ago that this program would have large effects in the real world. <br /><br />There was an academic study by psychologist Bob Cialdini and co-authors that helped provide the proof-of-concept for the OPOWER program. In this study, the researchers left door-hangers at a group of households in California. Some of the door-hangers said, “Save money by saving energy,” some of them said, “Save the environment,” and some said, “Here’s how much your neighbors are using.” And the ones that said, “Here’s how much your neighbors are using” had a much stronger impact on energy consumption. In the last couple of years that study in particular has had a lot of influence. <br /><br /><strong>Q.</strong> Okay, so why is it that referring to neighbors is effective?<br /><br /><strong>A.</strong> Psychologists have been great at documenting that if you tell people what the social norm is, people will converge to the social norm. In my mind there are two leading economic hypotheses for why this works in energy consumption. One is called “conditional cooperation.” People may be altruistic, and they view conserving energy as contributing to the public good of reducing climate change. People are typically more willing to contribute to a public good if they are informed that other people are contributing more than they are. <br /><br />The other explanation is just social inference. It could be that I couldn’t care less about the environment, but I do want to save money. And if you tell me that I’m using twice as much energy as my neighbor, that lets me know that maybe I’ve been leaving a window open or that my furnace is inefficient. So that’s purely a self-interested, informational story. Testing between these two explanations is one of the research questions we’re interested in.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Two MIT classes focus on helping Haiti]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>5</category>
<link>http://web.mit.edu/newsoffice/2010/classes-haiti-0305.html</link>
<story_id>15070</story_id>
<featured>0</featured>
<description><![CDATA[Students and teachers lend mind and hand to the rebuilding]]></description>
<postDate>Fri, 05 Mar 2010 05:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100304163728-1.png</thumbURL>
<smallURL width='140' height='92'>http://web.mit.edu/newsoffice/images/article_images/w140/20100304163728-1.jpg</smallURL>
<fullURL width='368' height='244'>http://web.mit.edu/newsoffice/images/article_images/20100304163728-1.jpg</fullURL>
<imageCredits><![CDATA[Image: istockphoto]]></imageCredits>
</image>
<body><![CDATA[In response to the earthquake in Haiti, MIT Media Lab students have developed a service that helps communities rebuild after a crisis by indexing the skills of local residents so that NGOs like the American Red Cross and Partners In Health can quickly find and employ them.<br /><br />Since January, Greg Elliott and Aaron Zinman have been developing Konbit, a free interactive communication platform that allows Haitians, their diaspora and the international community to report their skills by phone, text message or web. In anticipation of long-term rebuilding efforts, the goal of Konbit is to index everyday skills, such as language or construction skills, that aren’t currently being advertised or tracked by sites like Craigslist or Monster.<br /><br />Composed of several hardware and software systems, Konbit allows people from multiple countries to work together to help disaster victims find employment and rebuild their economy. It includes software for web, text, phone and translation services, as well as servers located in Cambridge and custom phone hardware to be installed in Haiti.<br /><br />Konbit is language- and medium-neutral, meaning that voice and text messages can be translated through the Konbit phone, text or web interface. The voice component is crucial to Konbit because more than 60 percent of the Haitian population is illiterate, according to UNICEF.<br /><br />Messages in native Creole will be translated by volunteer Haitians and then transcribed into the database so that NGOs can search for specific skills in real-time and by location. <br /><br />In addition to aiding reconstruction, another goal of Konbit is to prevent the outsourcing of labor. When aid organizations bring non-Haitians into Haiti for relief and reconstruction work, this prevents Haitians from receiving training and experience that could be valuable once the relief teams have left. It also hurts the Haitian economy, which had a 70 percent unemployment rate before the earthquake, according to the U.S. Agency for International Development.<br /><br />Zinman and Elliott hope to launch a prototype of the Konbit platform in early March. While the phone and web interfaces are essentially working, Zinman and Elliott are trying to get the major telecommunications companies in Haiti to deploy the service as soon as possible. <br /><br />Konbit got its start in the four-day Independent Activities Period workshop sponsored by the Media Lab and the Center for Future Civic Media that was aimed at developing innovative technologies to alleviate the crisis caused by the Jan. 12 earthquake in Haiti.<br /><br />Dale Joachim, a Media Lab visiting scientist who helped run that workshop, is now teaching “New Media Projects for Haiti” with Barry Vercoe, professor of media arts and sciences in the Media Lab. The project-oriented class will explore how communications technology can help rebuilding efforts. The class has about 30 undergraduate and graduate students enrolled, as well as a handful of participants who are attending the lectures.<br /><br />The first half of the class includes lectures on topics about Haitian society, such as economics, education and language, that will help student groups choose a societal problem and devise solutions. The class will travel to Haiti during the last week of April to field test and document its solutions. Each project will have its own evaluation plan that will be discussed when the class returns to MIT.<br /><br /><strong>Crisis, management</strong><br /><br />The Sloan School of Management is also offering a class directly involved in Haitian relief efforts. Several students from that class, “Applications of System Dynamics: Global Challenges,” are helping the U.S. military quickly analyze data for humanitarian-relief needs. <br /><br />Taught by Sloan senior lecturer Anjali Sastry, the project developed after Marc Zissman, assistant head of the Communications and Information Technology Division at MIT Lincoln Laboratory, asked for help assessing the current state of health, food, shelter and water in Haiti. Zissman is helping the U.S. military Joint Task Force in Haiti coordinate a group that will survey 288 displacement camps and neighborhoods in the country to determine the needs and supplies for the overall humanitarian effort.<br /><br />On a weekly basis, Sastry’s students will analyze up-to-the-minute data collected in Haiti over the next one to four months. The results are urgently needed for planning and decision-making and will be reported to the military, NGOs and the U.N. In addition to the rapid data analysis, the students will produce a set of white papers to help frame the results within the larger picture of long-term sustainability in Haiti, and how the humanitarian efforts might play out over time.<br /><br />“This is really about putting MIT skills to the test,” Sastry said, urging the various relief and reconstruction projects within the MIT community to communicate and partner with one another so there can be a better understanding of the needs and opportunities in Haiti.<br /><br />That line of thinking will guide the March 8 retreat by the MIT/Haiti Response Advisory Group. Hosted by Vice Chancellor and Dean for Graduate Education Steve Lerman, the goal of the retreat is to identify viable MIT-led projects that could meet Haiti’s needs.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Context is ev … well, something, anyway]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/object-recognition-0305.html</link>
<story_id>15072</story_id>
<featured>0</featured>
<description><![CDATA[MIT research uses information about how frequently objects are seen together to refine the conclusions of object recognition systems.]]></description>
<postDate>Fri, 05 Mar 2010 04:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100304171502-0.png</thumbURL>
<smallURL width='140' height='146'>http://web.mit.edu/newsoffice/images/article_images/w140/20100304171502-0.jpg</smallURL>
<fullURL width='368' height='385'>http://web.mit.edu/newsoffice/images/article_images/20100304171502-0.jpg</fullURL>
<imageCredits><![CDATA[Image: Myung 'Jin' Choi]]></imageCredits>
<imageCaption><![CDATA[Standard object recognition software mistakenly detects a sofa, a cabinet, and a mirror in a street scene (right), but a new MIT system corrects those errors (left) using statistical information about how often certain types of objects occur together.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='381'>http://web.mit.edu/newsoffice/images/article_images/20100304171503-2.jpg</fullURL>
<imageCredits><![CDATA[Image: Myung 'Jin' Choi]]></imageCredits>
</image>
</otherImages>
<body><![CDATA[Today, computers can’t reliably identify the objects in digital images. But if they could, they could comb through hours of video for the two or three minutes that a viewer might be interested in, or perform web searches where the search term was an image, not a sequence of words. And of course, object recognition is a prerequisite for the kind of home assistance robot that could execute an order like “Bring me the stapler.” Now, MIT researchers have found a way to improve object recognition systems by using information about context. If the MIT system thinks it’s identified a chair, for instance, it becomes more confident that the rectangular thing nearby is a table.<br /><br />A typical object recognition system will scan a digital image for groups of pixels that differ from those around them; those pixels could define an edge, a corner or some other feature of an object. Usually, the system has been trained on a set of sample images, which teaches it how to correlate feature patterns with particular objects.<br /><br />Some researchers have tried to use context information to refine those correlations. But according to Myung “Jin” Choi, a grad student in MIT’s Laboratory for Information and Decision Systems and one of the leaders of the new project, those researchers were generally working with a standard training set that included examples of only about 20 different types of object. In that case, it was fairly straightforward to specify how frequently each object co-occurred with every other object in the set.<br /><br /><strong>Upping the ante<br /></strong><br />A system that could recognize only 20 different objects, however, wouldn’t be very useful. And with a large number of objects, it becomes computationally impractical to consider the frequency of all possible two-object combinations. In work to be presented at the IEEE Conference on Computer Vision and Pattern Recognition this summer, Choi and her colleagues — including graduate student Joseph Lim and Professors Antonio Torralba and Alan Willsky — describe a different approach. Working with a training set that included more than 4,000 images and 107 different types of objects, they created algorithms that pored through the images and automatically constructed a hierarchical map of the object categories — kind of like the organizational chart for a large company, which shows who reports to whom. In the map, each object is connected to at most one object above it in the hierarchy (everyone in the organization reports to only one person), drastically reducing the number of connections that the system has to consider. The connection between any two objects is given a weight that indicates how often the objects appear together in the training images. The map also encodes information about the typical relative locations of two connected objects: buildings generally appear above roads, for instance, not below them.<br /><br />When the system analyzes a new image, it uses standard object recognition algorithms to generate a list of candidate objects, together with each object’s “confidence score” — a statistical measure of how likely the object is to have been correctly identified. Then it revises those scores on the basis of the information encoded in the contextual map.<br /><br />In experiments, Choi compared the performance of the bare object recognition algorithms with their performance when augmented by the contextual map. In both cases, she considered the three objects per image with the highest confidence scores. The bare algorithms correctly identified all three objects roughly 14 percent of the time; with the addition of the contextual map, the success rate jumped to about 25 percent.<br /><br /><strong>Long row to hoe</strong><br /><br />Of course, that means that the system still failed to correctly identify three objects per image about 75 percent of the time, which shows just how difficult the problem of object recognition remains. “Context really is essential,” says Serge Belongie, an associate professor of computer science at the University of California, San Diego who has worked on both object recognition in general and context-based object recognition in particular. “It deserves a proper treatment, and Jin is doing that.” But Belongie cautions that context awareness will never be more than an augmentation of an underlying system that recognizes objects from visual features. “We absolutely cannot afford to take our eye off the ball of the component recognition systems that need to feed these context engines,” he says. And, he adds, to be useful, object recognition systems will need to be much more precise than today’s prototypes are. “Imagine that you take a picture of a wild mushroom while you’re hiking,” Belongie says, “and then you send it to the system to find out what it is. And it says, ‘Mushroom!’ You’re like, Thanks. That’s really useful. I knew that part.”<br /><br />Nonetheless, Choi is continuing to improve her contextual-map system, against the day when the underlying algorithms are more reliable. The next version of the system, she says, will add entries to the map that, in effect, represent higher-level scene descriptions. Street scenes, for instance, may frequently feature sky, buildings and roads, while building interiors may frequently feature floors, walls and windows. The system won’t need to explicitly label these additional map entries, however; it will simply register them as foci around which certain types of objects regularly cluster. She’s confident that this modification will make the added benefits of context awareness even more acute.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Varanasi wins Career award from NSF]]></title>
<author><![CDATA[]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/varanasi-career.html</link>
<story_id>15073</story_id>
<featured>0</featured>
<description><![CDATA[]]></description>
<postDate>Thu, 04 Mar 2010 18:20:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100323093347-1.png</thumbURL>
<smallURL width='140' height='165'>http://web.mit.edu/newsoffice/images/article_images/w140/20100323093347-1.jpg</smallURL>
<fullURL width='368' height='435'>http://web.mit.edu/newsoffice/images/article_images/20100323093347-1.jpg</fullURL>
<imageCaption><![CDATA[Kripa Varanasi, the d'Arbeloff Assistant Professor of Mechanical Engineering]]></imageCaption>
</image>
<body><![CDATA[Kripa Varanasi, the d'Arbeloff Assistant Professor of Mechanical Engineering, is among the latest recipients of Career awards, the highly selective grants that the National Science Foundation (NSF) awards to junior faculty members who are likely to become academic leaders of the future.<br /><br />Varanasi's Career project, for which he received a $400,000 award, seeks to advance research and education programs in thermal-fluid-surface interactions involving nanoengineered surfaces with an emphasis on condensation phenomena. Varanasi says this research could lead to novel nanoengineered surfaces that could be useful in various industries including energy, water, agriculture, and transportation, for major gains in system efficiencies, energy savings and reduced CO2 emissions.<br /><br />Varansi's award began Feb. 1, and lasts for five years. For more information on the award, visit the NSF's web site <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0952564" target="_blank">http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0952564</a>.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Hurricanes’ effects on ocean temperature revisited]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/hurricane-thermostate-0304.html</link>
<story_id>15058</story_id>
<featured>0</featured>
<description><![CDATA[Mixing of ocean layers by tropical cyclones may have less effect on climate than previously thought, new research reveals.]]></description>
<postDate>Thu, 04 Mar 2010 05:00:02 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100303132947-1.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100303132947-1.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100303132947-1.jpg</fullURL>
</image>
<body><![CDATA[The role of hurricanes in the global climate system has gained interest ever since scientists suggested that strong hurricanes have become more frequent in recent decades and might continue to do so as the planet warms. Because hurricanes are known to influence the oceans and overall climate system, the consequences of the increase in the frequency of hurricanes could reach further.<br /><br />When a hurricane passes over an ocean, its powerful winds stir and mix the warm surface water with the colder, deeper water. This mixing results in warm water being forced down into the deep ocean and cold water being brought to the surface layer. Scientists know that the cold water near the surface is reheated by the atmosphere to pre-hurricane temperatures within a few weeks, but they have been less clear on what happens to the warm water mixed into the deep ocean. It has been suggested that this heat is transported toward the poles by ocean currents and contributes to the ocean heat transport, the process by which oceans regulate our climate by transporting warm water away from the equator and cold water toward the equator. It has also been speculated that the heat pumped into the ocean by hurricanes strengthens subsequent storms that pass over the same part of the ocean, because ocean heat is the energy source that powers hurricanes. Stronger storms would then mix even more heat into the ocean driving a positive feedback loop for hurricane intensity. <br /><br />A new MIT analysis suggests that previous studies have overestimated the amount of hurricane-induced ocean heating and its overall impact on climate. The analysis indicates that previous estimates have failed to consider how the oceans change with the seasons.<br /><br />Most of the heat from the warm water that hurricanes mix deep into the oceans during the summer and early fall is returned to the atmosphere in the winter, meaning these “warm anomalies” don’t appear to affect the long-term state of the oceans, according to a paper published Feb. 10 in <em>Geophysical Research Letters</em> by Raffaele Ferrari, the Cecil and Ida Green Professor of Oceanography in the Department of Earth, Atmospheric and Planetary Science; graduate student Malte F. Jansen; and undergraduate student Todd Mooring. <br /><br />Jim Price, a senior scientist at Woods Hole Oceanographic Institution, believes Ferrari and his students’ estimate more accurately reflects how hurricanes affect the long-term state of the oceans. “They’ve taken a step that makes the previous estimates look a little bit excessive,” he said.<br /><br />By analyzing the paths of almost 1,000 tropical cyclones, as well as data about the sea surface temperature and height before and after each storm, Ferrari and his students estimate that only about one-quarter of the warm water that is mixed downward by a hurricane remains in the ocean for more than a season.<br /><br />Previous estimates, including a 2001 study by Kerry Emanuel, the Breene M. Kerr Professor of Atmospheric Science, suggested that on average, hurricanes contribute about one petawatt (equal to one quadrillion watts) of heat to the estimated two to four petawatts of heat that the ocean transports out of the tropics and toward the poles each year. <br /><br />Although Emanuel and a colleague concluded in 2008 that only about half of the heat pumped into the ocean by hurricanes contributes to the total ocean heat transport toward the poles, Jansen said this study did not consider the crucial role of the seasonal cycle.<br /><br /><strong>Analyzing the seasonal cycle</strong><br /><br />The ocean is made up of several layers. The uppermost layer has the highest temperatures and it is called a mixed layer, because it is continuously being mixed and thus kept in contact with the atmosphere. Below this is the thermocline layer, which gets colder as depth increases. During the summer, the mixed layer is shallow, often only about 10-20 meters deep, but during the winter, cooler atmospheric temperatures and stronger winds can cause the mixed layer to expand to more than 100 meters. As a result, the mixed layer extends into what used to be the upper part of the thermocline, called the seasonal thermocline, thereby re-absorbing any warm anomaly that was deposited in this layer. Only anomalies in the part of the thermocline that remains below the mixed layer for the entire year, known as the permanent thermocline, will remain after the winter.<br /> <br />To calculate the fraction of heat from the mixed layer that is deposited into the permanent thermocline in the tropics, the researchers relied on theoretical models, as well as data about how the oceans changed as a result of all tropical cyclones that occurred between 1998 and 2006. Previous work, not related to Emanuel’s estimates, calculated that about half a petawatt of heat is pumped into the ocean during a hurricane. Using a formula that takes into account how the mixed layer deepens during the winter, Ferrari’s team concluded that this amount is actually between zero and .3 petawatts with a most likely estimate of .15 petawatts.<br /><br />The analysis suggests that the major part of a warm anomaly sits in the seasonal thermocline until the following winter, when it is reabsorbed by the deepened mixed layer; at that point, it interacts with the atmosphere and releases its extra heat. Rather than contributing to ocean heat transport, therefore, the warm anomalies function as thermostats by transferring their heat to the atmosphere during the winter. Additional research would presumably focus on the effects of this warmer atmosphere during winter.<br /><br />Emanuel praised the result, noting that “the path is different from what we initially envisioned” almost a decade ago. He explained that the interest in ocean mixing has grown recently also because of its potential biological consequences. Because cold water contains more dissolved carbon dioxide than warm water, when hurricanes bring that cold water closer to the surface, the carbon dioxide interacts with and enters the atmosphere. At the same time, however, as the nutrient-rich cold water is pushed closer to the surface and becomes exposed to sunlight, the number of tiny plant-like microorganisms called phytoplankton multiplies. When the plankton eventually die, the carbon in their shells is deposited at the bottom of the sea. Ferrari predicts that the seasonal effects of ocean mixing will become major factors for research that seeks to determine the net balance of carbon dioxide leaving and staying in the ocean.<br /><br />Jansen added that a potential next step for research on tropical cyclones and ocean mixing could examine how hurricane-induced ocean heating affects El Niño, a weather pattern in the Pacific Ocean where most of the strongest tropical cyclones occur, that is related to large climate disturbances. Whether the warm water mixed deep into the Pacific during a hurricane somehow affects El Niño remains unknown, partly because there isn’t enough quality data, according to Jansen.<br /><br />]]></body>
</item>
<item>
<title><![CDATA[3 Questions: Eduardo Kausel on Chile’s massive earthquake]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/3q-kausel-0304.html</link>
<story_id>15059</story_id>
<featured>0</featured>
<description><![CDATA[How stringent building codes limited damage in Chile, and why there’s cause for concern in parts of the U.S.]]></description>
<postDate>Thu, 04 Mar 2010 05:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100303152044-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100303152044-0.JPG</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100303152044-0.JPG</fullURL>
<imageCredits><![CDATA[Photo: Wikipedia]]></imageCredits>
<imageCaption><![CDATA[Only slight damage can be seen on buildings in Santiago de Chile on the morning after the earthquake in February.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='506'>http://web.mit.edu/newsoffice/images/article_images/20100304093806-1.jpg</fullURL>
<imageCaption><![CDATA[Eduardo Kausel, a professor in MIT's Department of Civil and Environmental Engineering.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[<em>Santiago native Eduardo Kausel, a professor in MIT's Department of Civil and Environmental Engineering (CEE), is an expert in structural dynamics and earthquake engineering. In an interview with MIT News, Kausel explains why Chile’s stronger earthquake led to less-catastrophic damage than the earthquake that struck Haiti in January. He also explains some of the risks that could be associated with a sizable earthquake in Boston, and why the philosophy behind building codes may be changing.</em><br /><br /><strong>Q.</strong> Even though the 8.8-magnitude earthquake that rocked Chile was 500 times stronger than the 7.0-magnitude earthquake that struck Haiti in January, the scope of damage was significantly less. Describe for us some of the differences between Chile and Haiti that helped limit loss of life and property. <br /><br /><strong>A.</strong> Chile possesses an educated middle class and ranks among the most developed nations in South America whereas Haiti is the poorest country in the Western Hemisphere. Clearly, there is a strong correlation between poverty and quality of housing and infrastructure. It is this difference more than the proximity of the epicenter of the earthquake that led to massive damage in Haiti. <br /><br />The most fundamental reason for the difference in damage and casualties is that Chile has been taking into account the effect of earthquakes and designing for them since at least the beginning of the 20th century. Chile's building codes are comparable to those of the United States, Japan, Turkey or Mexico, and rank among the most stringent and demanding. They have to, because strong earthquakes are a fact of life in Chile.<br /><br />In Haiti, however, virtually no construction is earthquake-proof, not even government buildings or the houses of the affluent. This may relate to the fact that although Haiti is in a seismically active zone, strong earthquakes there are much less frequent than in Chile and have return periods measured in centuries, not decades. <br /><br /><strong>Q.</strong> In the U.S., earthquakes tend to be associated with the West Coast, but strong earthquakes have struck the Midwest and Northeast over the last few hundred years. In your view, are these areas of the country prepared to withstand an earthquake of considerable magnitude?<br /><br /><strong>A.</strong> Considerable research is being carried out at present on how to make the Midwest safe against earthquakes. Fortunately, paleoseismology seems to suggest that mammoth intra-plate earthquakes such as the four that took place in New Madrid, Missouri, between Dec. 16, 1811 and Feb. 7, 1812, which rank among the strongest in the Midwest in historic times, may be rare — although strong quakes on the order of magnitude 6 or so could be expected to occur sometime this century. Still, the problem is not only technical, but also economical, for any upgrading of the large inventory of old, low-rise, unreinforced masonry structures from Chicago through St. Louis to Memphis would entail enormous costs, which the public would have to weigh against the low risk.<br /><br />A somewhat different story is that of quakes in the Northeast from Boston to Canada. Although not as strong or frequent as those in California, they may not be so rare either. For example, some strong earthquakes have recently taken place in the Quebec province, but having occurred in largely uninhabited areas, they have been inconsequential. On the other hand, a repeat of the 1755 Cape Ann earthquake some 50 miles to the northeast of Boston could conceivably produce substantial damage to the unreinforced red stone buildings of Back Bay, an area that was reclaimed from the Atlantic Ocean and has very soft ground conditions. But probably the more important risk factor there may be the gas lines embedded in that soft infill soil whose possible rupture could lead to fires. <br /><br />It is worth keeping in mind that modern high rise buildings in Boston and elsewhere in the East and Midwest are very safe indeed, for they not only account for seismic considerations, but have been designed to resist the enormous overturning forces caused by strong winds or even hurricanes, which are much more frequent than earthquakes in this region.<br /><br /><strong>Q.</strong> What can we to do to prevent significant damage in the U.S.? How can building codes and processes be improved further?<br /><br /><strong>A.</strong> Over time, all codes continuously evolve, reflecting the lessons learned from past design mistakes, most of which were not a-priori obvious. Until recently, the goal of seismic codes was to protect human life, not the buildings themselves. This philosophy may be gradually changing now. It has been argued that the economic loss to the affected region or nation can be far greater than the aggregate of the physical losses. An example is the massive damage to the port facilities in 1995 by an earthquake in Kobe, Japan, which caused much of that port's shipping commerce to move elsewhere. Societal and economic considerations such as these may begin to affect seismic codes yet to come.<br /><br />In Chile's case, the various bridges that failed on the highway to the south are not only a loss to the local municipalities or the Chilean highway administration, but the damage to the local economy may vastly exceed the cost of the bridges, which could have been made safer if constructed at a modest additional expense. The stricken area is the heart of the Chilean agriculture, akin to California's San Joaquin Valley. Much of the fruit consumed in winter in the U.S., not to mention the wine, comes from that area. If trucks cannot take these to the ports — many of which where also destroyed — then the produce cannot make it to our markets. Thus, the seismic codes that govern the infrastructure may be in need of upgrading. Expect the fruit and vegetable prices in the U.S. to rise sharply in the weeks ahead.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT Medical proposes community care model]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/medical-care.html</link>
<story_id>15062</story_id>
<featured>0</featured>
<description><![CDATA[]]></description>
<postDate>Wed, 03 Mar 2010 19:49:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100303161729-0.png</thumbURL>
<smallURL width='140' height='90'>http://web.mit.edu/newsoffice/images/article_images/w140/20100303161729-0.jpg</smallURL>
<fullURL width='368' height='238'>http://web.mit.edu/newsoffice/images/article_images/20100303161729-0.jpg</fullURL>
</image>
<body><![CDATA[MIT Medical is proposing the establishment of a new Community Care Center that would lead to significant improvements in patient care by better utilizing our health care resources.<br /><br />This proposal would involve a transition from MIT Medical’s current model of providing on-site overnight urgent and inpatient care to a major expansion and enhancement of our care management program, with a focus on coordinating on-campus and community resources to ensure that patients receive the care they need from hospital or doctor’s office to dorm or home.<br /><br />MIT Medical's leadership has met and will continue to meet with numerous faculty and student groups around MIT to explain the proposal and get feedback. If you’d like to suggest that the leadership meet with a certain group, please use <a href="http://medweb.mit.edu/about/news/article/comment-030410.html" target="_blank">this form</a>. MIT Medical also welcomes your comments and opinions (anonymous if desired) via that same form.<br /><br />To read more about the proposal, <a href="http://medweb.mit.edu/about/news/article/community-care-proposal-100303.html" target="_blank">visit the MIT medical web site</a>.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Lerman named provost at George Washington University]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/lerman-provost-gw.html</link>
<story_id>15061</story_id>
<featured>0</featured>
<description><![CDATA[Praised as a ‘remarkable friend and servant of MIT,’ he will leave the Institute after more than 40 years]]></description>
<postDate>Wed, 03 Mar 2010 19:48:49 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100303155459-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100303155459-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100303155459-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Donna Coveney]]></imageCredits>
<imageCaption><![CDATA[MIT Vice Chancellor and Dean for Graduate Education Steven R. Lerman has been named provost at The George Washington University.]]></imageCaption>
</image>
<body><![CDATA[MIT Vice Chancellor and Dean for Graduate Education Steven R. Lerman '72, SM '73, PhD '75 has been named George Washington University’s next provost and executive vice president for academic affairs, GW President Steven Knapp announced on Wednesday. Lerman is scheduled to take up his new responsibilities on July 1.<br /><br />"Dr. Lerman understands George Washington's aspirations, its unique position in Washington and the world, and its extraordinary opportunities,” Knapp said in a statement. “He has a strong record of engaging students and working collaboratively and effectively with colleagues; those skills will help us build an ever stronger faculty, raise the university's stature, and continue to enhance the academic experience of our students."<br /><br />A loyal and enthusiastic member of the MIT community for more than 40 years, Lerman has built a reputation for fresh thinking, technical knowledge, skillful management and great prowess in pulling various stakeholders together to advance MIT goals. From leading Project Athena to chairing the committee on MIT OpenCourseWare and coordinating the work of the Task Force for Institute-wide Planning, Lerman has been a force for innovation at MIT, his friends and colleagues said.<br /><br />MIT President Susan Hockfield said, “With integrity, thoughtfulness, enthusiasm and unfailing good cheer, Steve Lerman has lived the life of MIT in every dimension — as an undergraduate and graduate student, as a professor, as chair of the faculty, as a housemaster, as dean for graduate education, as vice chancellor, and as leader of vital educational and research initiatives, from Project Athena and the Singapore-MIT Alliance to the Center for Educational Computing Initiatives and the iLabs program. We are very sorry to lose this remarkable friend and servant of MIT.”<br /><br />Lerman, who first arrived at MIT as a freshman in 1969 and who has effectively been here ever since, said the decision to leave the Institute was a difficult one, but that the time had come for new challenges.<br /><br />“As much as I have loved being both a student and a professor here, the opportunity to serve in a leadership role at George Washington was an opportunity that I found extraordinarily compelling. Any success I am fortunate to have there will be because of what I have learned from all the experiences I was privileged to have here,” he said.<br /><br />In addition to his new administrative duties at GW, Lerman will serve as the A. James Clark Professor of Civil and Environmental Engineering. As at MIT, Lerman and his wife, Lori, will live on campus. <br /><br />“I look forward to getting to know the entire GW community over the coming months and years. My wife Lori and I will live on the Mount Vernon campus, and we plan to participate fully in the intellectual and social life of the community,” he said.<br /><br />Lerman has authored more than 50 publications, including two books. He has held several chairs at MIT and has been recognized with awards for his teaching, including the chair he now holds. His department and the Graduate Student Council have both honored him for excellence in teaching. He recently was given the National Association of Graduate and Professional Students Advisor of the Year award.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[The Institute-wide Planning Task Force moves from ideas into action]]></title>
<author><![CDATA[]]></author>
<link>http://web.mit.edu/newsoffice/2010/planning-task-action.html</link>
<story_id>15065</story_id>
<featured>0</featured>
<description><![CDATA[]]></description>
<postDate>Wed, 03 Mar 2010 19:47:00 EST </postDate>
<otherImages>
<image>
<fullURL width='368' height='364'>http://web.mit.edu/newsoffice/images/article_images/20100303165632-1.jpg</fullURL>
</image>
</otherImages>
<body><![CDATA[To the members of the MIT community:<br /><br />On December 16 the Institute-wide Planning Task Force released its <a href="http://web.mit.edu/instituteplanning/TaskForceFinalReport.pdf" target="_blank">final report</a>. Today we write to describe how we will act on and advance the ideas presented in this report, which marked the culmination of an intense period of collaboration that began one year ago. The final report proves, once again, that MIT is at its best when it comes together to solve problems. This extraordinary effort brought together approximately 200 faculty, students, and staff to develop and assess ideas, and to explore new ways of reducing costs and enhancing revenues while promoting MIT's historic excellence. Many of these ideas grew out of submissions to the online <a href="http://ideabank.mit.edu/" target="_blank">Idea Bank</a>, which encouraged and facilitated the participation of the entire MIT-wide community. In addition, the Task Force was able to build on and accelerate many long-standing efforts across the Institute aimed at improving our operations and strengthening MIT's global leadership. <br /><br />We have seriously considered all ideas presented by the Task Force Working Groups while planning for Fiscal 2011. The ideas and recommendations put forward by the Task Force respond to our original charge, and range from operational improvements to strategic opportunities, from relatively simple changes to very complex implementations, and from incremental actions to bold and ambitious statements. All of these recommendations are proposals for change. In evaluating these proposals, we must carefully consider the impact of possible changes on the MIT culture, and reaffirm our commitment to the guiding principles that have helped to make MIT a great institution. Progress, at this or any time, requires change. <br /><br />Our guiding principles include commitment to a unified structure of one faculty, staff and student population that operates under a common set of policies and procedures, and to working and communicating with the entire MIT community. The Task Force outcomes are a reflection of these principles in action. We have witnessed with pride this collective work over the past year, and we are inspired by the outcome of this impressive effort. Before we move forward to act on the Task Force recommendations, we want to express our most sincere gratitude to every member of the MIT community for their dedication to this effort and to the Institute.<br /><strong><br />Moving Forward: Implementing and Advancing Task Force Ideas</strong><br /><br />After careful consideration of the ideas and recommendations included in the report, and guided by feedback from the MIT community and discussions with the Task Force Coordinating Team and Working Group Co-Chairs, we propose a roadmap for action focused on three objectives:<br /><ol>
<li>Impacting the Fiscal 2011 budget; </li>
<li>Promoting and investing for the future; and </li>
<li>Understanding implementation paths and options. </li>
</ol><strong>1. Impacting the Fiscal 2011 budget</strong><br /><br />The work to balance the financial picture of MIT is unwavering. Thanks to the determined work of the MIT units, much has been accomplished in Fiscal 2010, but more is needed in Fiscal 2011. The planning objective for Fiscal 2011 is to be able to absorb the effects of the financial and economic crises and balance the general Institute budget (GIB). The Fiscal 2011 pro-forma budget includes a planned budget reduction of $61 million, in addition to the $58 million already achieved in the current Fiscal 2010. Additional information can be found at the <a href="http://web.mit.edu/instituteplanning/" target="_blank">Institute-wide Planning website</a>, and a budget planning update can be found <a href="http://web.mit.edu/instituteplanning/budgetupdate.html" target="_blank">here</a>.<br /><br />The following Task Force ideas are estimated to contribute $12 million in budget savings or revenues and are incorporated in our Fiscal 2011 pro-forma budget.<br /> 
<ul>
<li>Advancement of procurement and sourcing recommendations and travel modernization. Examples include: (i) Combining campus purchasing volume with Lincoln Laboratory; (ii) Negotiating volume and market share discounts in exchange for upgraded supplier status in MIT's tools; (iii) Renegotiating travel services for airlines, hotels and cars, focusing on frequently traveled segments; and (iv) Introduction of MIT travel credit card, online booking tool, electronic expense reporting and direct deposit for out-of-pocket reimbursements;</li>
<li>Investments and concrete actions in energy efficiencies. For example, modeling energy needs according to time of day, weather conditions, equipment constraints and predicted pricing for electricity in order to optimally deploy assets at the utility plant to meet demand in the most cost effective way;</li>
<li>Increase in visitor fees to cover more of the associated costs; </li>
<li>Modest increase in total undergraduate enrollment (target up by 50 students to 4,288); and</li>
<li>Reduction in the student expense budget and adjustment of financial aid to better reflect true board costs.</li>
</ul>
In addition to these concrete ideas and following on the salary actions  implemented in Fiscal 2010, we are also planning to adopt the  recommendation to moderate the salary growth across all categories in  Fiscal 2011. A number of ideas were offered related to the softening of  faculty salaries and changing the research assistant (RA) tuition  subsidy, but these ideas will not be implemented in Fiscal 2011.<br /><br /><strong>2. Promoting and investing for the future</strong><br /><br />Equally important to improving the short-term financial picture of the Institute, is investing for the future of MIT. Many ideas and recommendations from the Task Force can be instrumental in creating a sustainable MIT, both physically and financially; we are making a commitment to promote, invest in and advance them.<br /><br />We have asked the Task Force Coordinating Team — Vice President for Finance Israel Ruiz, Associate Provost Martin Schmidt, and Vice Chancellor and Dean for Graduate Education Steven Lerman — to continue in their role as we move from idea generation into the implementation phase of this work. We have also asked the Task Force Working Group Co-Chairs to continue to act in an advisory capacity to the Coordinating Team as ideas are further evaluated, so that we can be sure to retain the spirit and intent of their recommendations. We have asked them to develop a process to inform the selection of ideas and investment opportunities that will better position MIT to operate in a more sustainable way in the future in time to inform the final Fiscal 2011 budget. <br /><br />The areas we consider of most interest are:<br /> 
<ul>
<li>Deferred maintenance and energy efficiency: balancing the need to constrain annual expenditures while investing in preventive maintenance to achieve long-term savings;</li>
<li>Accelerating the Digital MIT concept: electronic paystubs and W2s, online graduate admissions, paperless solutions for document management and current printed materials, digitization of existing labor-intensive paper-based systems, process simplification and improved adoption of electronic tools; and</li>
<li>New educational and revenue opportunities: expanding the traditional model to include "3+2" degrees with other institutions, OCW certificates, e-learning, and investing in resource development activities and programs.</li>
</ul>
We also want to endorse the following ideas and themes expressed by several of the Working Groups:<br /> 
<ul>
<li>Sharing resources and efforts across units and between MIT campus and Lincoln Laboratory;</li>
<li>Supporting standards in renovations and purchasing while allowing choice at a cost that departments, laboratories and centers should appropriately bear;</li>
<li>Measuring and understanding MIT to inform opportunities to gain efficiencies;</li>
<li>Bundling project renovations to economize costs and minimize academic and research disruption; and</li>
<li>Supporting location-independent work to improve productivity and produce space savings.</li>
</ul>
<strong>3. Understanding implementation paths and options</strong><br /><br />The Task Force recommended that a number of opportunities identified by the Working Groups be assigned to individual units and existing committees for further evaluation and to determine next steps. We recognize the value in pursuing additional ideas, and have followed this recommendation. A number of these ideas are already being explored by units across MIT, and some are in the early stages of implementation and will impact Fiscal 2011. These include improving the utilization of summer student housing, addressing expenditures for Athena clusters and printing, and providing electronic paystubs for the MIT community. In addition, we are charging several groups with continuing the work of the Task Force. <br /><br />First, in order to effectively and efficiently evaluate the implementation possibilities and the timeline for these activities, we are asking the Task Force Coordinating Team to collect responses from unit leaders with assigned ideas by April 2nd, and to work together with the Working Group Co-Chair Advisory Group to determine next steps and prioritize ideas for implementation. As we work to prioritize this important work, we must evaluate each opportunity based on its potential impact to the MIT mission, financial sustainability, compatibility with our overall values and core principles, and with sensitivity to the particular groups affected, while also considering alternative approaches. <br /><br />Second, in response to the feedback received on the ideas related to MIT's benefits program, we recently charged the <a href="http://web.mit.edu/instituteplanning/benefitsgroup.html" target="_blank">Benefits Advisory Group</a> to deepen the understanding of different proposals and to advise on potential options and implementation paths. The Group, which is chaired by Vice President for Human Resources Alison Alden, is expected to report its recommendations to MIT's senior leadership in June. <br /><br />Third, we will appoint the following groups to further understand the opportunities described below and, if deemed desirable, to inform the appropriate level of investment. The groups will advise the Task Force Coordinating Team on possible implementation paths with preliminary reports due shortly after Commencement. <br /> 
<ul>
<li>Expanding Educational Opportunities: this group will take an in-depth look at expanding educational opportunities as presented by the Working Groups with two thrusts: 1) Residential opportunities ranging from professional education to offering existing class capacity to special students and visitors; and 2) Hybrid educational models combining the residential experience with on-line and asynchronous methods for education.</li>
<li>E-learning Opportunities: this group will further evaluate new educational opportunities around e-learning. These scalable educational platforms are educational offerings that use on-line, asynchronous tools to create new learning opportunities and could reach a greater number of students. In particular the group will be asked to consider: 1) How MIT OpenCourseWare could be extended to offer certificates; and 2) Opportunities to provide complete asynchronous education or offerings that are totally on-line.</li>
<li>Printing and Digital Archiving: this group will continue to work on the area of infrastructure to enable effective printing strategies and reduce costs and distribution of printed materials, develop record retention policies, and pursue electronic storage systems to enable the elimination of paper records and archives.</li>
<li>Measuring and Understanding - Education Model: this group will work to refine the undergraduate cost of education model and extend the model to incorporate graduate education and research.</li>
<li>Measuring and Understanding - Space Economy: this group will examine our academic frameworks for accountability and develop metrics and improved cost models for space. These metrics will improve accountability and transparency and should ultimately lead to sustained savings through increased efficiency of our operations.</li>
</ul>
Throughout the next phase of the Task Force work, we remain committed to an open and transparent communication strategy with the entire community. Again, we are most grateful for your participation in this important planning effort on behalf of the Institute, which will strengthen MIT's operations while meeting its fiscal challenges.<br /><br />Sincerely,<br /><br />L. Rafael Reif, Provost<br />Phillip L. Clay, Chancellor<br />Terry Stone, Executive Vice President &amp; Treasurer<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Ecological balancing act]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/phytoplankton-0303.html</link>
<story_id>15042</story_id>
<featured>0</featured>
<description><![CDATA[Phytoplankton diversity depends on balance between competition and the ocean’s physical dynamics, new research suggests]]></description>
<postDate>Wed, 03 Mar 2010 05:00:02 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100302171346-2.png</thumbURL>
<smallURL width='140' height='110'>http://web.mit.edu/newsoffice/images/article_images/w140/20100302171346-2.jpg</smallURL>
<fullURL width='368' height='290'>http://web.mit.edu/newsoffice/images/article_images/20100302171346-2.jpg</fullURL>
<imageCredits><![CDATA[Image: National Science Foundation]]></imageCredits>
<imageCaption><![CDATA[An image of diatoms, a major group of eukaryotic algae that are one of the most common types of phytoplankton.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='230'>http://web.mit.edu/newsoffice/images/article_images/20100301130247-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Oliver Jahn, Chris Hill, Stephanie Dutkiewicz and Mick Follows (MIT) and the ECCO2 Project (MIT/NASA)]]></imageCredits>
<imageCaption><![CDATA[Daily, global image of phytoplankton diversity from a high resolution ocean and ecosystem model for Oct. 19, 1999. Colors represent the number of types of phytoplankton.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Phytoplankton are single-celled organisms that serve as the base of the marine food web and provide half the oxygen we breathe on Earth. They also play a key role in global climate change by removing carbon from the atmosphere and injecting it deep into the oceans.<br /><br />Scientists study phytoplankton to understand how the tiny plants help transport elements like carbon through the environment. Although they understand much of what phytoplankton do, less is understood about why particular plankton live in particular environments and what maintains the diversity of phytoplankton. <br /><br />Previous research has suggested that more diverse ecosystems may be more efficient at utilizing resources, meaning that the diversity of phytoplankton could be important for regulating the cycles of carbon and other elements in the ocean. But scientists need a better understanding of that diversity before they can understand how much carbon the ocean ultimately removes from the atmosphere.<br /><br />Researchers from MIT’s cross-disciplinary Darwin Project, a collaboration between the Earth System Initiative (ESI) and the Computational and Systems Biology Initiative (CSBi) and funded by the Gordon and Betty Moore Foundation’s Marine Microbiology Initiative and NASA, have developed a computer model to simulate ecosystems in a virtual ocean, a model that could guide future field surveys of phytoplankton. They suggest that the diversity of phytoplankton species at a given location depends on the balance between the removal of species through competition for limited nutrient resources and their replacement by ocean currents, according to a paper published online Feb. 25 in <em>Science Express</em>. <br /><br />In order to grow, phytoplankton need sunlight and nutrients like carbon, some of which comes from the carbon dioxide in the atmosphere. When phytoplankton die, some of their cells sink to the ocean floor, taking carbon away from the atmosphere and injecting it deep into the ocean through a process known as the “biological pump.” To understand the global scale of this process, scientists must learn more about the diversity of phytoplankton species.<br /><br />“We feel this paper is a step toward understanding what the phytoplankton diversity is at different places in the ocean and what regulates that diversity,” said lead author Andrew D. Barton, a graduate student in the Department of Earth, Atmospheric and Planetary Sciences (EAPS).<br /><br />Although future studies will have to make a more explicit link between phytoplankton diversity and the climate, Barton hopes that his group’s models could be used as a tool to inform future sampling surveys that try to map phytoplankton diversity in the ocean.<br /><br /><strong>Building an ecosystem</strong><br /><br />Barton and his colleagues used a computer model developed in 2007 by co-author Mick Follows, a senior research scientist in EAPS, to study the distribution of particular phytoplankton types, as well as to observe how phytoplankton help move different elements through the oceans. <br /><br />To study these cycles, Barton’s team plugged information about the traits of nearly 80 phytoplankton species, such as how fast they grow and what temperature they prefer to live in, into the computer model, which also simulates the physical circulation and currents of the ocean. After the computer progressed the virtual ocean forward for a decade, certain patterns began to appear, with more species appearing in the warm tropics and Gulf Stream regions than at colder, higher latitudes. <br /><br />Barton’s team then hypothesized why those patterns occur, taking into account the circulation of the ocean in different regions, as well as the fact that growth rates depend on changes in temperature, light and nutrient concentration. They conclude that the amount of species in a given location is based on how rapidly species are removed because of competition for limited resources, and the rate at which species are returned to that location by the ocean’s currents — a balance that is also affected by the nature of the environment.<br /><br />In the tropics, seasonal variations are weak, and different species can coexist for long periods. But there is less diversity at higher latitudes, where the changing seasons vary the amount of light and nutrients that phytoplankton can consume throughout the year. Here, a few highly specialized phytoplankton rapidly outcompete all others during the strong spring blooms, and this effect outweighs the rate at which the ocean’s currents can return species to these latitudes. <br /> <br />Barton and his colleagues also explain that a relatively large variety of phytoplankton coexist in the Gulf Stream and similar currents that constantly move and mix different species from different regions. In this case, the variability of the environment doesn’t matter, because the intensity of the currents prevents more dominant species from outcompeting other species for food. <br /><br /><strong>Future mapping</strong><br /><br />Princeton ecologist Simon Levin called the research “highly original and exciting” for scaling microscopic details of the ocean to macroscopic patterns by combining fluid dynamics, ecology and evolutionary biology data into one model. He also thinks the research will be useful for planning future studies where phytoplankton are collected. <br /><br />Barton and his colleagues hope their interpretation will help inform future mapping surveys of the ocean by guiding oceanographers where to look for particular patterns in phytoplankton diversity. They need new field data to test and refine their hypotheses and are currently speaking to scientists at Woods Hole, MIT and the University of Hawaii about collecting data on upcoming long-distance scientific explorations in the Pacific Ocean.<br /><br />Barton’s next step is to evaluate the diversity patterns using a very high resolution version of the current computer model to examine how the ocean’s complex range of structures — small eddies, currents and fronts — provide small habitats that could enhance diversity. <br /><br />Future research should also examine how the processes of extinction and evolution help maintain the diversity patterns, he said.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[A more democratic Internet]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>6</category>
<link>http://web.mit.edu/newsoffice/2010/fcc-broadband-0303.html</link>
<story_id>15051</story_id>
<featured>0</featured>
<description><![CDATA[At a forum on campus this week, the FCC discussed how its new broadband plan will help citizens engage in democratic deliberation.]]></description>
<postDate>Wed, 03 Mar 2010 05:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100302180358-0.png</thumbURL>
<smallURL width='140' height='105'>http://web.mit.edu/newsoffice/images/article_images/w140/20100302180358-0.jpg</smallURL>
<fullURL width='368' height='276'>http://web.mit.edu/newsoffice/images/article_images/20100302180358-0.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Patrick Gillooly; image: Google Maps]]></imageCredits>
</image>
<body><![CDATA[On March 17, the Federal Communications Commission will present the U.S. Congress with its National Broadband Plan, a set of recommendations for bringing high-speed Internet access to the millions of Americans who don’t yet have it. The plan is likely to determine the allocation of the $7.2 billion in stimulus money intended to bring broadband to rural and underserved areas, and many observers believe that the government is already planning to augment that investment with billions more in discretionary spending.<br /><br />In anticipation of the plan’s unveiling, the FCC is holding a series of regional forums on particular aspects of the plan. On Monday evening, in an event co-hosted by the MIT Media Lab’s Center for Future Civic Media, Eugene Huang, director of government performance and civic engagement for the National Broadband Plan, spoke at MIT on the plan’s provisions for better engaging the public in the democratic process.<br /><br />Huang began by describing the FCC’s attempts to engage the public in the creation of the broadband plan itself, describing the agency’s 35 public workshops on the topic, held between August of last year and the end of January, some of which had as many as 1,000 live attendees and another 5,000 online participants. He also commended the launch, last May, of data.gov, an online index of publicly available government data maintained by the White House. “But,” he added, “data.gov includes only a small amount of federal-government data, and we believe that all data and information that the government treats as public should be made available online, in machine-readable formats.” Huang said that the broadband plan recommends that data from the legislative and judiciary branches, too, be freely accessible online, pointing out, for example, that the federal courts’ Public Access to Court Electronic Records system, or PACER, charges for access to judicial decisions. In fact, Huang said, “The U.S. federal courts themselves pay private contractors $150 million annually for electronic access to judicial documents.”<br /><br />Huang also said that the plan urges the creation of a public archive of historically significant video — a kind of YouTube for policy deliberations and news footage. To that end, he said, the FCC will call on Congress to revise copyright law to make it easier for news organizations to donate historical footage to the archive.<br /><br />In addition, he emphasized that the federal government should make better use of social media, pointing to the success of the Centers for Disease Control in using Twitter, YouTube, podcasts, and other social-networking technologies to disseminate information about the H1N1 flu outbreak.<br /><br />When he turned to the topic of how government can draw citizens into the deliberative process, rather than simply providing them with better information, Huang became a little more vague: “Government is just beginning to think about these types of issues,” he acknowledged. But in thinking about how to use digital tools to directly engage the citizenry, he said, the government is using digital tools to directly engage the citizenry. The White House’s Open Government Initiative, Huang said, has used what he described as “public brainstorming blogs, a wiki, and a collaborative drafting tool” to solicit public participation in determining just what its project should be.<br /><br />Huang grew more specific, however, in describing how Internet access could facilitate the process of voter registration and make voters’ records more portable. “One recent study estimates that voter registration problems resulted in more than two millions voters’ being unable to vote in the 2008 general election,” Huang said. “Providing broadband to more Americans provides an important opportunity to fix the problems in the existing process.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[In Profile: Alex Slocum]]></title>
<author><![CDATA[David L. Chandler, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/profile-slocum-0302.html</link>
<story_id>15041</story_id>
<featured>0</featured>
<description><![CDATA[Teaching and tinkering, and teaching how to tinker, are the driving forces for longtime MIT mechanical engineer]]></description>
<postDate>Tue, 02 Mar 2010 05:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100301125327-1.png</thumbURL>
<smallURL width='140' height='109'>http://web.mit.edu/newsoffice/images/article_images/w140/20100301125327-1.jpg</smallURL>
<fullURL width='368' height='287'>http://web.mit.edu/newsoffice/images/article_images/20100301125327-1.jpg</fullURL>
<imageCredits><![CDATA[Photo courtesy of the White House]]></imageCredits>
<imageCaption><![CDATA[Professor Alex Slocum demonstrates the principle behind anchoring proposed energy-storage tanks to the seafloor, as President Barack Obama and MIT President Susan Hockfield look on.]]></imageCaption>
</image>
<body><![CDATA[It’s Wednesday evening, so Alexander Slocum is hard at work cooking up a huge batch of spaghetti, green beans and garlic bread in a cramped kitchen on the top floor of MIT’s Building 24. That might not be most people’s image of how an MIT professor spends part of his workday, but it’s something Slocum does every Wednesday for a hungry and appreciative group of a few dozen freshmen.<br /><br />“Food is a great conversation catalyst,” Slocum explains with a smile as he lays out the dinner on a set of long tables and the students eagerly line up.<br /><br />The students are all part of the Experimental Study Group, a program celebrating its 40th year at MIT. As an alternative that allows about 50 freshmen to satisfy their General Institute Requirements in an atmosphere of small, personalized classes and peer learning in small study groups, ESG generally provides a sense of community and camaraderie more typical of a small college. Slocum, director of the program since 2002, is also an alumnus of ESG himself.<br /><br />Slocum ’82, MS ’83, PhD ’85, the Neil and Jane Pappalardo Professor of Mechanical Engineering, loves to tinker and build, but most of all he loves to ignite that same passion for creating new devices in other people. His enthusiasm is one reason he was named Massachusetts’ “professor of the year” in 2000, among many awards he has garnered for both research and teaching.<br /><br />For more than a decade, his was the funny and effervescent voice announcing the play-by-play for MIT’s famed annual student competition of remotely operated robots, held every year as the culmination of class 2.007, “Introduction to Design and Manufacturing.” And when President Barack Obama came to campus last fall and toured labs, Slocum was the researcher who greeted the president wearing (as he usually does) a vibrant Hawaiian shirt and who explained to the commander-in-chief his novel concept to store some of the energy harvested by offshore wind farms. <br /><br /><strong>Devices for doctors</strong><br /><br />One of the programs Slocum has been especially interested in is a class that he evolved from a class originally created by Prof. Guttag with CIMIT, the Boston-based Center for Integration of Medicine and Innovative Technology. Each year in that class (2.75, “Precision Machine Design”), clinicians and doctors from area hospitals propose to CIMIT a device they wish someone would invent to deal with problems they encounter in their practice.<br /><br />Students can then pick a problem they would like to tackle, and form teams to develop devices to fill that need. Thanks to U.S. Army sponsorship, each team then gets a budget of about $5,000 to pursue the project. “I have weekly design-review and problem-solving meetings with the student/doctor teams, and the students’ task is to work with the doctor they’ve selected, create strategies and concepts, do a patent search, then do the research and engineering needed to build and test their solution,” Slocum explains.<br /><br />Several devices developed in that class over the years have won awards (including the MIT $100K business plan competition), and some are being developed as commercial products. Sometimes, the students will advance their designs in a follow-up mechanical engineering class.<br /><br />Each year, about eight new devices are developed as part of that class, and typically one ends up being described in a paper for a major journal article, and two or three are presented at medical device-design conferences. The class has produced “very important contributions to medical technology,” he says, including a low–cost, single-use robotic system for laparoscopic procedures, a novel needle for injections that can penetrate precisely to the right depth within a blood vessel or organ, and a pressure-sensing syringe.<br /><br />Robert Sheridan, a doctor at Massachusetts General Hospital who has worked with Slocum’s class, recalls how students in one 2.75 class responded to a problem involving the need for negative-pressure wound therapy — creating a low-pressure atmosphere over a wound to promote healing — with a device that could be used in developing-world locations with limited resources.<br /><br />“They grasped the clinical problem and developed a creative concept to address the challenge,” Sheridan says. “I had been working in my basement workshop on a concept device, and the students opened my mind to other potential approaches, which have led to the current prototype.” That prototype, further developed by Slocum’s graduate student and original team member Danielle Zurovcik, he says, is now undergoing tests in a rural hospital in Rwanda. In fact, after Zurovcik came back from Rwanda and the Haiti earthquake struck, she garnered support to make more devices and rushed off to help the earthquake victims.  <br /><br />With more than a decade of experience with 2.75, Slocum plans to write up the lessons learned so that mechanical engineering instructors elsewhere can add elements of the class’s distinctiveness to their own. “It might be a useful model for others to adopt bits from,” he says.<br /><br />The doctors have been quite enthusiastic about the process. Tom Brady, also a doctor from Mass General, says, “the magic is a combination of motivated students and clinicians, unique prototyping facilities, and Alex’s passion for solving medical problems. Alex is the creative force that makes this class work.”<br /><br /><strong>Energy solutions</strong><br /><br />Slocum is also working with students to tackle one of the thorniest problems facing the world: meeting growing energy demand with renewable sources that don’t add greenhouse gases to the atmosphere. His major focus has been a proposal for incorporating an energy-storage system into the mooring structures for offshore floating windmill installations, connected by submerged cables to the grid onshore, so intermittent energy produced by variable winds can be stored on site and delivered just when it’s needed. <br /><br />The concept involves huge concrete tanks — spheres or cylinders attached to the seafloor beneath the floating windmills — that pump out water when the windmill is producing excess power and then allow water to flow back in, through a generating turbine, when power is needed. It’s a classic pumped hydro system similar to those used for decades with dams and lakes. That’s the concept he demonstrated to President Obama last fall, and Slocum is hopeful that the idea will catch on.<br /><br />Such a system, he says, could theoretically be used to provide enough power to meet all of the nation’s electricity needs. Offshore wind farms — far enough out to avoid objections about interfering with pristine views — could be arrayed in a few places along the coasts and in the Great Lakes, he says. Wind turbines in a total area of about 100 miles by 500 miles, he estimates, would produce as much electric power as the nation currently uses. <br /><br />He’s also looking into ways of storing solar power, using molten salts to store the thermal energy for delivery later. With the right policies in place to promote the development and deployment of these technologies along with modular nuclear plants and liquefied coal or biomass for transportation fuels, he says, “The U.S. could essentially be energy independent in 20 years, and that would do more for world peace than anything else. We built the interstate highway system in the interest of national security, so we should be able to build a National Energy System.”<br /><br />In addition to looking for innovative ways to teach students about science and engineering, and to apply that knowledge to important problems, Slocum is interested in helping them learn how to pass along their knowledge to others. “How do you ever teach someone to become a teacher?” he muses. One way, he suggests, is to have their education be as interactive as possible from the start. Most students who sign up for ESG, for example, come from smaller schools, where they are used to small classes and lots of interaction with their teachers. <br /><br />And after their freshman year of immersion in that program, many of the students come back as teaching assistants, mentors, seminar leaders and tutors. Slocum thinks having a mix of freshmen, upperclassmen, graduate students, faculty and alums helps to foster the learning process. The more different kinds of people you have around, “the more serendipity can take hold,” he says.<br /><br />The students tend to appreciate that kind of openness. Nevan Hanumara SM ’06, took Slocum’s 2.75 class a few years ago and now, while working on his doctorate at MIT, has returned as one of the class’s two Teaching Assistants. “He’s a sharer,” he says, “he does not hoard ideas. That’s unusual.” And Slocum encourages an open, exploratory approach: “He’s extremely creative, and also extremely nonlinear and chaotic. You can try stuff really easily.” Those who have taken his classes tend to stay in touch, Hanumara says. “He’s very much interested in our development as people, not just as students.”<br /><br />One result, he says, is that Slocum has created “a lab full of people who play well together.”<br /><br />Mechanical engineering graduate student Conor Walsh says simply that 2.75 was “the most interesting class I took at MIT,” and he has continued to work on the project he started in that class as part of his thesis. Working with Slocum and the doctor who suggested that device, he says, “we are developing lots of other devices in the same area as the original product. So in a way, the class spawned a new research area!”<br /><br />The classes he teaches, Slocum says, where students get a chance to work on something whose real-world application to meet a significant need is clear from the outset, are based on “hard-core mechanical design.” As he finishes scarfing down a plateful of the spaghetti he just prepared, he explains the importance of this kind of hands-on approach to learning: “You can read about cooking, but then you’re still hungry,” he says. “To really satisfy your hunger, you need to get in the kitchen.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[MIT welcomes British foreign secretary to campus this week]]></title>
<author><![CDATA[Rebecca Tyler]]></author>
<link>http://web.mit.edu/newsoffice/2010/miliband-compton.html</link>
<story_id>15017</story_id>
<featured>0</featured>
<description><![CDATA[David Miliband SM ’90 to deliver Compton Lecture on Wednesday]]></description>
<postDate>Mon, 01 Mar 2010 21:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100223140308-0.png</thumbURL>
<smallURL width='140' height='122'>http://web.mit.edu/newsoffice/images/article_images/w140/20100223140308-0.jpg</smallURL>
<fullURL width='368' height='322'>http://web.mit.edu/newsoffice/images/article_images/20100223140308-0.jpg</fullURL>
<imageCaption><![CDATA[British Foreign Secretary David Miliband will deliver the March 2010 Karl Taylor Compton Lecture. ]]></imageCaption>
</image>
<body><![CDATA[MIT welcomes British Foreign Secretary David Miliband to campus this week to deliver the March 2010 Karl Taylor Compton Lecture.<br /><br />Miliband, who received a master of science in political science from MIT in 1990, will present a talk titled “The War in Afghanistan: How to End It” at 3:30 p.m. on Wednesday in <a href="http://whereis.mit.edu/?go=W16" target="_blank">Kresge  Auditorium</a>. MIT President Susan Hockfield will introduce Miliband, and the program will include a brief question and answer period. The lecture is free and open to the MIT community; no tickets are required. Video of Miliband's remarks will be available at <a href="http://amps-webflash.amps.ms.mit.edu/public/MIT/2009-2010/Miliband/" target="_blank">http://amps-webflash.amps.ms.mit.edu/public/MIT/2009-2010/Miliband/</a>.<br /><br /><strong>Arrival procedures</strong><br /><br />Entry to Kresge Auditorium will begin at 3 p.m. from the doors on <a href="http://whereis.mit.edu/?go=G8" target="_blank">Kresge Oval only</a>. Please note that metal detectors will be in use and that lecture attendees are permitted to carry one small personal item (such as a purse or briefcase) into the auditorium; no backpacks or laptop computers will be permitted. The U.S. Department of State reserves the right to inspect personal items on a case-by-case basis.<br /><br />The Karl Taylor Compton lecture series is sponsored by the MIT president in conjunction with the Office of the Provost. The lecture series, named in honor of Karl Taylor Compton, MIT’s ninth president, began in 1957 and aims to give the MIT community direct contact with the important ideas of our times and with people who have contributed much to modern thought. For more information, please visit the <a href="http://compton.mit.edu/" target="_blank">Compton Lecture Series web site</a>.<br /><br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[New technique offers a more detailed view of brain activity]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/brain-imaging-0301.html</link>
<story_id>15035</story_id>
<featured>0</featured>
<description><![CDATA[‘Cleverly designed' MRI sensors detect dopamine, offering a high-resolution look at what’s happening inside the brain.]]></description>
<postDate>Mon, 01 Mar 2010 05:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100226175623-0.png</thumbURL>
<smallURL width='140' height='116'>http://web.mit.edu/newsoffice/images/article_images/w140/20100226175623-0.jpg</smallURL>
<fullURL width='368' height='307'>http://web.mit.edu/newsoffice/images/article_images/20100226175623-0.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Patrick Gillooly]]></imageCredits>
</image>
<body><![CDATA[For neuroscientists, one of the best ways to study brain activity is with a scanning technique called functional magnetic resonance imaging (fMRI), which reveals blood flow in the brain. <br /><br />However, although fMRI is a powerful tool for identifying brain regions that are active during a particular task, it offers only an indirect view of what’s happening. Measuring a more direct indicator of neural activity, such as concentrations of neurotransmitters (brain chemicals that carry messages between neurons) could be much more valuable.<br /><br />Now, for the first time, MIT and Caltech researchers have come up with a new type of fMRI sensor that can do just that. The two sensors, described in the Feb. 28 online edition of <em>Nature Biotechnology</em>, detect dopamine — a neurotransmitter involved in learning, movement control and many other brain processes.<br /><br />“This new tool connects molecular phenomena in the nervous system with whole-brain imaging techniques, allowing us to probe very precise processes and relate them to the overall function of the brain and of the organism,” says Alan Jasanoff, an associate professor of biological engineering at MIT and senior author of the paper.<br /><br />Dopamine holds particular interest for neuroscientists because of its role in motivation, reward, addiction and several neurodegenerative conditions, including Parkinson’s disease. The new sensors could help scientists learn more about how dopamine acts in the brain and in other organs, says Andrew Alexander, co-director of the Brain Imaging Core at the University of Wisconsin at Madison.<br /><br />“Previously we really haven’t had specific biomarkers for looking at things like dopamine or other chemical neurotransmitters” with MRI, says Alexander.<br /><br /><strong>Designing a new sensor</strong><br /><br />Conventional fMRI measures blood flow in the brain by tracking hemoglobin, the molecule that carries oxygen. Hemoglobin has an iron atom at its core that binds to oxygen. When bound to oxygen, hemoglobin’s magnetic properties change in a way that can be detected with MRI.<br /><br />“fMRI is an extremely powerful technique for studying how the brain functions, and it’s the only way to obtain spatial information and information about when things are happening,” says Jasanoff, who also has appointments in the Departments of Brain and Cognitive Sciences and Nuclear Science and Engineering, and in the McGovern Institute for Brain Research at MIT.<br /><br />However, the spatial and temporal information is imprecise. Researchers can detect increased activity in a certain area, but they can’t see what the activity is, nor can they get a high-resolution picture of which neurons are involved.<br /><br />A more detailed picture of brain activity could emerge with MRI sensors specific to particular neurotransmitters. The MIT team designed sensors specifically for dopamine, but their technique could be used to create sensors for other neurotransmitters or even unrelated molecules of biological interest.<br /><br />To build the new sensors, the MIT team worked with chemical engineers at Caltech, using an approach called “directed evolution.” They started with a protein called cytochrome P450, an enzyme found in most organisms that is paramagnetic (meaning it can become weakly magnetic when exposed to a magnetic field). Using a technique called error-prone PCR, which is a faulty version of the way cells naturally replicate their genes, they generated a large collection of different mutated forms of the gene. <br /><br />Each mutated gene was placed into an E. coli bacterium, which produced the mutated protein. The researchers then tested each protein for its ability to bind dopamine. At the end of each round, they took the best candidate and mutated it again for a new round of improvement. At the end of five rounds, they had two sensors that would bind strongly to dopamine but not to other neurotransmitters.<br /><br />“You want it to be specific to dopamine — you don’t want it to bind to dopamine and half a dozen other things,” says Jasanoff.<br /><br />In studies of rats, the researchers showed that the sensor can effectively detect dopamine in the brain. However, in its current form, the dopamine probe must be injected into the brain, and the imaging is limited to the site of injection. <br /><br />Bruce Jenkins, director of neurochemical imaging at the Martinos Center for Biomedical Imaging at MGH, says the new probe is “very cleverly designed,” but points out that an important challenge is yet to come: getting the molecule to cross the layer of cells that separates the brain from circulating blood. “Trying to get a charged protein across the blood-brain barrier is very tricky,” he says.<br /><br />The MIT team hopes to overcome that obstacle by applying barrier disruption techniques used historically to deliver chemotherapeutic agents to the brain. They will also try to genetically program brain cells to express the sensor, so it doesn’t have to be injected.<br /><br />They plan to adapt the directed evolution strategy to look for sensors for other neurotransmitters as well. If successful, that could help researchers in Jasanoff’s lab and elsewhere create a better wiring diagram of how different brain regions and neurotransmitters work together to yield behavior such as learning, memory, addiction and movement.<br /><br />“We hope to develop probes that target different parts of the mechanism, allowing us to piece these systems together in a way that’s noninvasive,” says Jasanoff. <br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Second opinion? Diagnosing doctors]]></title>
<author><![CDATA[Peter Dizikes, MIT News Office]]></author>
<category>3</category>
<link>http://web.mit.edu/newsoffice/2010/good-doctors-0301.html</link>
<story_id>15037</story_id>
<featured>0</featured>
<description><![CDATA[All physicians can save lives. But great ones tend to save time and money, too, an MIT economist finds.]]></description>
<postDate>Mon, 01 Mar 2010 05:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100226155525-1.png</thumbURL>
<smallURL width='140' height='104'>http://web.mit.edu/newsoffice/images/article_images/w140/20100226155525-1.jpg</smallURL>
<fullURL width='368' height='274'>http://web.mit.edu/newsoffice/images/article_images/20100226155525-1.jpg</fullURL>
<imageCredits><![CDATA[Image: istockphoto]]></imageCredits>
</image>
<body><![CDATA[What difference does a great doctor make to your health? Patients everywhere would love to know the answer. <br /><br />A recent study co-authored by Joseph Doyle, an economist at the MIT Sloan School of Management, offers a subtle conclusion to this question. Treatment by a highly rated physician does not necessarily change the outcome of a serious medical problem. Instead, the best doctors typically offer an accurate diagnosis more quickly than moderately rated doctors, leading to hospital stays for patients that are 10 percent shorter and less expensive — an average that increases to 25 percent for certain medical specialties. <br /><br />“As a patient myself, I always hope to go to a prestigious hospital, but I wonder how much more of an advantage that is,” says Doyle. “It turns out that if you don’t have access to the most prestigious teams, the less prestigious ones will eventually make the same types of interventions, but it just takes them longer to get there, and it’s more costly.” These findings figure to resonate at a time when the cost of health care is a major political preoccupation.<br /><br />To reach this conclusion, Doyle — along with his colleagues Steven Ewer of the University of Wisconsin and Todd Wagner of Stanford University — examined roughly 70,000 treatment episodes involving 30,000 patients, spread over 13 years, at a Veterans Affairs hospital in a large city in the United States. The hospital’s practices naturally lent themselves to a comparison of doctor quality since the institution randomly assigned patients to two separate teams of physicians and residents, which had markedly different medical backgrounds. <br /><br />One of these teams (dubbed “Program A” by the researchers) consisted of members trained at an elite U.S. medical school, which sometimes boasts the nation’s highest average MCAT scores among its incoming students. The other group (“Program B”) has members trained at a middle-ranked medical school. Medical residents with Program A had medical board-certification scores that on average placed them in the top quarter of the national results, while the Program B doctors had scores placing them in the bottom fifth of U.S. residency programs. (The researchers agreed to keep the identities of the VA hospital and medical schools anonymous.)<br /><br /><strong>Money, not mortality</strong><br /><br />Despite these differences, in some ways the bottom-line results for patients were similar regardless of whether they were treated by doctors in Program A or Program B. The mortality rates for the two programs were within a percentage point of each other, as measured over 30 days, one year and five years from the time each patient was treated. <br /><br />“I find that to be a feel-good result,” Doyle says. <br /><br />As Doyle, Ewer, and Wagner see it, the major difference between the teams involved the ease and confidence with which the more highly regarded doctors made their diagnoses. The doctors in Program B, the ones from the lower-ranked medical school, ordered 8 percent more tests than their counterparts in Program A, and on average took 8 percent longer to request an initial test for a patient. These differences were more pronounced within certain specialties. For instance, the Program B doctors took 21 percent longer to order heart exams, 51 percent longer to request an angiography, and 32 percent longer to order a cardiac stress test, for patients with congestive heart failure. Such delays have a direct impact on the overall cost of treatment, since they result in longer hospital stays for patients. Moreover, laboratory expenditures were 13 percent higher or patients in program B.<br /><br />The doctors in Program B also consulted with specialists more often, which can also prolong the duration of a hospital visit. “Sometimes people look at the use of specialists as waste or excessive cost,” says Doyle. “But maybe these [lesser-ranked] physicians need specialists to achieve the same outcomes.”<br /><br />Other health-care economists find the study a useful look at a complex question. “I think this has been on the minds of people trying to fix health care: What goes on in that black box inside the heads of doctors?” says Jonathan Skinner, an economist who teaches at both Dartmouth College and Dartmouth Medical School. “It speaks more broadly to why we see greater medical costs in some areas — it may be the difficulties physicians are having making a diagnosis.” <br /><br /><strong>Only so much room at the top</strong><br /><br />To be sure, there will always be differences among doctors; in any group of physicians, some will have better training or be more highly regarded than others. By quantifying the disparities among doctors, however, and linking them to particular practices, Doyle’s work provides a yardstick for medical professionals who would like to reduce the gap between excellent and average doctors in absolute terms. <br /><br />As Doyle notes, a basic caveat to the study is that it examines just one VA hospital (the researchers are looking for others featuring the same arrangements). Nonetheless, Skinner, for one, thinks the paper will have a significant impact among policymakers. “The larger point this speaks to in terms of policy is that measurement is really important in understanding what physicians do,” he says. “If you take a bunch of physicians and train them, you cannot assume they will all do the same things. It would be nice to have feedback mechanisms where doctors and residents could sit down and observe what is going on. They might change what they do.”<br /><br />]]></body>
</item>
<item>
<title><![CDATA[Ancient hurricanes]]></title>
<author><![CDATA[Morgan Bettex, MIT News Office]]></author>
<category>2</category>
<link>http://web.mit.edu/newsoffice/2010/hurricane-pliocene-0226.html</link>
<story_id>15029</story_id>
<featured>0</featured>
<description><![CDATA[Intense hurricane activity millions of years ago may have caused and sustained warmer climate conditions, new research suggests]]></description>
<postDate>Fri, 26 Feb 2010 04:00:02 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100225170623-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100225170623-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100225170623-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Chris Brierley and Google Earth]]></imageCredits>
<imageCaption><![CDATA[When ocean mixing from hurricanes is included in an early Pliocene global climate model simulation, the number and intensity of hurricanes increases, as well as the warm pool in the Pacific Ocean.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100225170623-0.jpg</fullURL>
<imageCredits><![CDATA[Image: Chris Brierley and Google Earth]]></imageCredits>
<imageCaption><![CDATA[A simulation with no hurricane-induced mixing shows a year of modern hurricane tracks and sea surface temperatures colored by intensity (blue is weaker, red is stronger).]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[A question central to research on global warming is how warmer temperatures caused by increased greenhouse gases could influence climate. Probing the past for clues about this potential effect, MIT and Yale climate scientists examined the Pliocene period, which began five million years ago and which some consider to be a potential analog to modern greenhouse conditions. They found that hurricanes influenced by weakened atmospheric circulation — possibly related to high levels of carbon dioxide — contributed to very warm temperatures in the Pacific Ocean, which in turn led to more frequent and intense hurricanes. The research indicates that Earth’s climate may have multiple states based on this feedback cycle, meaning that the climate could change qualitatively in response to the effects of global warming.<br /><br />Although scientists know that the early Pliocene had carbon dioxide concentrations similar to those of today, it has remained a mystery what caused the high levels of greenhouse gas and how the Pliocene’s warm conditions, including an extensive warm pool in the Pacific Ocean and temperatures that were roughly 4 degrees C higher than today’s, were maintained. <br /><br />In a paper published Feb. 25 in <em>Nature</em>, Kerry Emanuel, the Breene M. Kerr Professor of Atmospheric Science in the Department of Earth, Atmospheric and Planetary Science, and two colleagues from Yale University’s <a href="http://www.geology.yale.edu/ " target="_blank">Department of Geology and Geophysics</a> suggest that a positive feedback between tropical cyclones — commonly called hurricanes and typhoons — and the circulation in the Pacific could have been the mechanism that enabled the Pliocene’s warm climate.<br /><br />The Pliocene ended around three million years ago with the onset of large ice sheets in the Northern Hemisphere. There has been a slow reduction in carbon dioxide levels in the atmosphere for about 15 million years, and it is thought that the start of the glacial cycles was the climate’s response once those levels reached a certain threshold, according to co-author Chris Brierley. While that level remains unknown, this research indicates that by increasing carbon dioxide levels, humans could reach the threshold that would induce a Pliocene-like climate.<br /><br />By combining a hurricane model and coupled ocean-atmosphere general circulation model to investigate the early Pliocene, Emanuel, Brierley and co-author Alexey Fedorov observed how vertical ocean mixing by hurricanes near the equator caused shallow parcels of water to heat up and later resurface in the eastern equatorial Pacific as part of the ocean wind-driven circulation. The researchers conclude from this pattern that frequent hurricanes in the central Pacific likely strengthened the warm pool in the eastern equatorial Pacific, which in turn increased hurricane frequency — an interaction described by Emanuel as a “two-way feedback process.”<br /><br />The researchers believe that in addition to creating more hurricanes, the intense hurricane activity likely created a permanent El Niño-like state in which very warm water in the eastern Pacific near the equator extended to higher latitudes. The El Niño weather pattern, which is caused when warm water replaces cold water in the Pacific, can impact the global climate by intermittently altering atmospheric circulation, temperature and precipitation patterns. <br /><br />The research suggests that Earth’s climate system may have at least two states — the one we currently live in that has relatively few tropical cyclones and relatively cold water, including in the eastern part of the Pacific, and the one during the Pliocene that featured warm sea surface temperatures, permanent El Niño conditions and high tropical cyclone activity.<br /><br />Although the paper does not suggest a direct link with current climate models, Fedorov said it is possible that future global warming could cause Earth to transition into a different equilibrium state that has more hurricanes and permanent El Niño conditions. “So far, there is no evidence in our simulations that this transition is going to occur at least in the next century. However, it’s still possible that the condition can occur in the future.”<br /><br />Whether our future world is characterized by a mean state that is more El Niño-like remains one of the most important unanswered questions in climate dynamics, according to Matt Huber, a professor in Purdue University’s Department of Earth and Atmospheric Sciences. He praised the research, saying it is the “very specific predictions it makes about how cyclones can warm the eastern equatorial Pacific that is the most unique and exciting.”<br /><br /><strong>Reconstructing the Pliocene</strong><br /><br />To investigate the hurricane activity of the Pliocene, the researchers relied on proxy ocean temperatures based on variations in several chemical tracers in drill cores of the ocean floor. The exact value of these chemical properties is known to correlate well with sea surface temperatures in the modern ocean and are used as proxies for past temperatures. Using these proxy temperatures, the researchers reconstructed the global distribution of sea surface temperatures. They then plugged these sea surface temperatures into an ocean-atmosphere general circulation model, which is a computer-based mathematical model used for climate forecasting, to determine the Pliocene’s atmospheric conditions. This showed that the early Pliocene had a weakened atmospheric circulation, and therefore, reduced vertical wind shear, which is favorable for tropical cyclone growth.<br /><br />Next, Emanuel entered data from the large-scale climate model of the Pliocene into a Statistical DownScaling Model (SDSM), which is software used to derive regional climate information, such as hurricane activity, based on global climate data. By producing synthetic hurricane tracks with the SDSM, researchers can study the effects of hurricanes on ocean temperatures in different regions.<br /><br />Their observations included nearly twice the number of tropical cyclones than  occur in our current climate system, including storms with lifespans that averaged two to three days longer than our current system. The hurricanes appeared in places, such as Hawaii, that differ from where they typically occur today, and also occurred throughout the seasons.<br /><br />The researchers traced this storm activity to the expansion of the warm pool in the eastern equatorial Pacific that resulted from resurfaced parcels of warm water created by the hurricanes — essentially, both the cause and effect of the observed increased tropical cyclones.<br /><strong><br />Fine-tuning the theory</strong><br /><br />Additional research will focus on why the Pliocene was so warm at higher latitudes, including an iceless North Pole, and whether this resulted from moisture produced by the tropical cyclones, Fedorov said.<br /><br />Brierley hopes to develop an interactive model to strengthen the group’s theory. Rather than examining individual components, such as sea surface temperatures, and then imposing that data onto a model to figure out potential ocean mixing and hurricane activity, the researchers would like to include everything in the same interactive model.<br /><br />Resolving other issues, such as how to more precisely estimate the contribution of tropical cyclones to ocean mixing, will not only help improve the early Pliocene climate model, but also help predict future climate change for which the feedback between hurricanes and the ocean circulation could be crucial.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Explained: Linear and nonlinear systems]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/explained-linear-0226.html</link>
<story_id>15031</story_id>
<featured>0</featured>
<description><![CDATA[Much scientific research across a range of disciplines tries to find linear approximations of nonlinear behaviors. But what does that mean?]]></description>
<postDate>Fri, 26 Feb 2010 04:00:01 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100225172319-1.png</thumbURL>
<smallURL width='140' height='125'>http://web.mit.edu/newsoffice/images/article_images/w140/20100225172319-1.jpg</smallURL>
<fullURL width='368' height='330'>http://web.mit.edu/newsoffice/images/article_images/20100225172319-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<otherImages>
<image>
<fullURL width='368' height='485'>http://web.mit.edu/newsoffice/images/article_images/20100225151835-1.jpg</fullURL>
<imageCredits><![CDATA[Photo: Patrick Gillooly]]></imageCredits>
<imageCaption><![CDATA[Pablo Parrilo, the Finmeccanica Career Development Professor of Engineering MIT’s Laboratory for Information and Decision Systems]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Spend some time browsing around the web site of MIT’s Computer Science and Artificial Intelligence Laboratory, and you’ll find hundreds if not thousands of documents with titles like “On Modeling Nonlinear Shape-and-Texture Appearance Manifolds” and “Non-linear Drawing systems,” or, on the contrary, titles like “Packrat Parsing: Simple, Powerful, Lazy, Linear Time” and “Linear-Time-Encodable and List-Decodable Codes.”<br /><br />The distinction between linear and nonlinear phenomena is everywhere in the sciences and engineering. But what exactly does it mean?<br /><br />Suppose that, without much effort, you can toss a tennis ball at about 20 miles per hour. Now suppose that you’re riding a bicycle at 10 miles per hour and toss a tennis ball straight ahead. The ball will travel forward at 30 miles per hour. Linearity is, essentially, the idea that combining two inputs — like the velocity of your arm and the velocity of the bike — will yield the sum of their respective outputs — the velocity of the ball.<br /><br />Now suppose that, instead of tossing a tennis ball, you toss a paper airplane. Depending on the airplane’s design, it might sail straight ahead, or it might turn loops. Some paper planes seem to behave more erratically the harder you throw them: the bike’s added velocity might make it almost impossible to get the plane to do anything predictable. That’s because airflow over a paper plane’s wings can be very nonlinear.<br /><br />If the bicycle had built-in sensors and an onboard computer, it could calculate the velocity of the tennis ball in a fraction of a second. But it could never hope to calculate all the airflows over the paper plane’s wing in time to do anything useful. “I think that it’s a reasonable statement that we mostly understand linear phenomena,” says Pablo Parrilo, the Finmeccanica Career Development Professor of Engineering MIT’s Laboratory for Information and Decision Systems.<br /><br />To make the distinction between linearity and nonlinearity a bit more precise, recall that a mathematical equation can be thought of as a function — something that maps inputs to outputs. The equation y = x, for instance, is equivalent to a function that takes as its input a value for x and produces as its output a value for y. The same is true of y = x<sup>2</sup>.<br /><br />The equation y = x is linear because adding together inputs yields the sum of their respective outputs: 1 = 1, 2 = 2, and 1 + 2 = 1 + 2. But that’s not true of y = x<sup>2</sup>: if x is 1, y is 1; if x is 2, y is 4; but if x is 3, y is not 5.<br /><br />This example illustrates the origin of the term “linear”: the graph of y = x is a straight line, while the graph of y = x<sup>2</sup> is a curve. But the basic definition of linearity holds for much more complicated equations, such as the differential equations used in engineering to describe dynamic systems.<br /><br />While linear functions are easy enough to define, the term “nonlinear” takes in everything else. “There’s this famous quote — I’m not sure who said it first — that the theory of nonlinear systems is like a theory of non-elephants,” Parrilo says. “It’s impossible to build a theory of nonlinear systems, because arbitrary things can satisfy that definition.” Because linear equations are so much easier to solve than nonlinear ones, much research across a range of disciplines is devoted to finding linear approximations of nonlinear phenomena.<br /><br />Russ Tedrake, for example, the X Consortium Associate Professor of Electrical Engineering and Computer Science at MIT, has adapted <a href="http://web.mit.edu/newsoffice/2010/parrilo-convergence.html" target="_blank">theoretical work done by Parrilo</a> to create novel control systems for robots. A walking robot’s gait could be the result of a number of mechanical systems working together in a nonlinear way. The collective forces exerted by all those systems might be impossible to calculate on the fly. But within a narrow range of starting conditions, a linear equation might describe them well enough for practical purposes. Parrilo’s theoretical tools allow Tedrake to determine how well a given linear approximation will work within how wide a range of starting conditions. His control system thus consists of a whole battery of linear control equations, one of which is selected depending on the current state of the robot.<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Mechanical devices stamped on plastic]]></title>
<author><![CDATA[Larry Hardesty, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/printable-mems-0226.html</link>
<story_id>15032</story_id>
<featured>0</featured>
<description><![CDATA[Microelectromechanical devices gave us the Wii and the digital movie projector. MIT researchers have found a new way to make them.]]></description>
<postDate>Fri, 26 Feb 2010 04:00:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100225170336-1.png</thumbURL>
<smallURL width='140' height='110'>http://web.mit.edu/newsoffice/images/article_images/w140/20100225170336-1.jpg</smallURL>
<fullURL width='368' height='291'>http://web.mit.edu/newsoffice/images/article_images/20100225170336-1.jpg</fullURL>
<imageCredits><![CDATA[Image: Corinne Packard and Apoorva Murarka]]></imageCredits>
<imageCaption><![CDATA[To test a new technique for creating micromachines, MIT researchers deposited films of gold on a sheet of plastic; grooves in the plastic are visible as a series of horizontal lines.]]></imageCaption>
</image>
<otherImages>
<image>
<fullURL width='368' height='326'>http://web.mit.edu/newsoffice/images/article_images/20100225170337-2.jpg</fullURL>
<imageCredits><![CDATA[Photo: Corinne Packard]]></imageCredits>
<imageCaption><![CDATA[Unlike existing micromachines, the researchers' gold films are flexible, so they could be used to coat irregular surfaces.]]></imageCaption>
</image>
</otherImages>
<body><![CDATA[Microelectromechanical devices — tiny machines with moving parts — are everywhere these days: they monitor air pressure in car tires, register the gestures of video game players, and reflect light onto screens in movie theaters. But they’re manufactured the same way computer chips are, in facilities that can cost billions of dollars, and their rigidity makes them hard to wrap around curved surfaces.<br /><br />MIT researchers have discovered a way to make microelectromechanical devices, or MEMS, by stamping them onto a plastic film. That should significantly reduce their cost, but it also opens up the possibility of large sheets of sensors that could, say, cover the wings of an airplane to gauge their structural integrity. The printed MEMS are also flexible, so they could be used to make sensors with irregular shapes. And since the stamping process dispenses with the harsh chemicals and high temperatures ordinarily required for the fabrication of MEMS, it could allow MEMS to incorporate a wider range of materials.<br /><br />Conventional MEMS are built through a process called photolithography, in which different layers of material are chemically deposited on a substrate — usually a wafer of some semiconducting material — and etched away to form functional patterns. Since a wafer is at most 12 inches across, arranging today’s MEMS into large arrays requires cutting them out and bonding them to some other surface.<br /><br />Instead of using a wafer, the MIT researchers begin with a grooved sheet of a rubbery plastic, which is coated with the electrically conductive material indium tin oxide. The researchers use what they call a “transfer pad” to press a thin film of metal against the grooved plastic. Between the metal film and the pad is a layer of organic molecules that weaken the metal’s adhesion to the pad. If the researchers pull the pad away fast enough, the metal remains stuck to the plastic.<br /><br />“It’s kind of similar to if you have Scotch tape on a piece of paper,” says Corinne Packard, a postdoc in the Research Lab of Electronics at MIT who led the work, along with professors of electrical engineering Vladimir Bulovi? and Martin Schmidt. “If you peel it off slowly, you can delaminate the tape very easily. But if you peel fast, you’ll rip the paper.”<br /><br />Once the transfer pad has been ripped away, the metal film is left spanning the grooves in the plastic like a bridge across a series of ravines. Applying a voltage between the indium-tin-oxide coating and the film can cause it to bend downward, into the groove in the plastic: the film becomes an “actuator” — the moving part in a MEMS. Varying the voltage would cause the film to vibrate, like the diaphragm of a loudspeaker; selectively bending different parts of the film would cause them to reflect light in different ways; and dramatically bending the film could turn a smooth surface into a rough one. Similarly, if pressure is applied to the metal film, it will generate an electric signal that the researchers can detect. The film is so thin that it should be able to register the pressure of sound waves.<br /><br /><strong>Serendipity</strong><br /><br />The discovery of the manufacturing technique, which the MIT team describes in a forthcoming issue of the journal <em>Advanced Materials</em>, was a happy accident. The researchers were actually trying to use a printing technique to build an electrical circuit. They had created a plastic stamp with a pattern molded into it and were trying to transfer that pattern to a thin sliver film. They had expected that the plastic would pull away the silver it made contact with, leaving behind an electrode that could control an organic light-emitting diode. <br /><br />Instead, however, the stamp kept pulling away the entire silver film. “The first couple times we did this, we were like, ‘Ah! Bummer, man,’” says Bulovi?. “And then a light bulb went off, and we said, ‘Well, but we just made the world’s first printed MEM.’” The stamp was intended as a means of creating an electronic device; instead, it ended up serving as the basis for a device itself. The researchers’ ensuing work was on the ideal architecture for the device and on ways to minimize the metal film’s adhesion to the transfer pad and maximize its adhesion to the grooved plastic.<br /><br />Because the researchers hadn’t set out to make MEMS, and because, to their knowledge, their films constitute the first stamped MEMS devices, they’re still trying to determine the ideal application of the technology. Sheets of sensors to gauge the structural integrity of aircraft and bridges are one possibility; but the MEMS could also change the physical texture of the surfaces they’re applied to, altering the airflow over a wing, or modifying the reflective properties of a building’s walls or windows. A sheet of thousands of tiny microphones could determine, from the difference in the time at which sound waves arrive at different points, where a particular sound originated. Such a system could filter out extraneous sounds in a noisy room, or even perform echolocation, the way bats do. The same type of sheet could constitute a paper-thin loudspeaker; the vibrations of different MEMS might even be designed to interfere with each other, so that transmitted sounds would be perfectly audible at some location but inaudible a few feet away. The technology could also lead to large digital displays that could be rolled up when not in use.<br /><br />John Rogers, a researcher at the University of Illinois at Urbana-Champaign <a href="http://www.technologyreview.com/energy/21467/" target="_blank">who has pioneered techniques</a> for printable electrical circuits, is particularly intrigued by the idea that printable MEMS could incorporate materials that are incompatible with existing MEMS manufacturing processes. “The ability to do heterogeneous integration of different material types into micromachines is a neat capability that would be enabled by this form of manufacturing,” Rogers says. “It opens up new design opportunities because it relaxes constraints on choices of materials.” And in general, Rogers says, the idea of printing MEMS is “cool.” “What they’ve done in this paper is demonstrated, for the first time, to my knowledge, this kind of concept.”<br /><br /><br />]]></body>
</item>
<item>
<title><![CDATA[Cell-inspired electronics]]></title>
<author><![CDATA[Anne Trafton, MIT News Office]]></author>
<category>1</category>
<link>http://web.mit.edu/newsoffice/2010/cytomorphic-0225.html</link>
<story_id>15022</story_id>
<featured>0</featured>
<description><![CDATA[By mimicking cells, MIT researcher designs electronic circuits for ultra-low-power and biomedical applications.]]></description>
<postDate>Thu, 25 Feb 2010 04:30:00 EST </postDate>
<image>
<thumbURL>http://web.mit.edu/newsoffice/images/article_images/w76/20100224140544-1.png</thumbURL>
<smallURL width='140' height='140'>http://web.mit.edu/newsoffice/images/article_images/w140/20100224140544-1.jpg</smallURL>
<fullURL width='368' height='368'>http://web.mit.edu/newsoffice/images/article_images/20100224140544-1.jpg</fullURL>
<imageCredits><![CDATA[Graphic: Christine Daniloff]]></imageCredits>
</image>
<body><![CDATA[A single cell in the human body is approximately 10,000 times more energy-efficient than any nanoscale digital transistor, the fundamental building block of electronic chips. In one second, a cell performs about 10 million energy-consuming chemical reactions, which altogether require about one picowatt (one millionth millionth of a watt) of power. <br /><br />MIT's Rahul Sarpeshkar is now applying architectural principles from these ultra-energy-efficient cells to the design of low-power, highly parallel, hybrid analog-digital electronic circuits. Such circuits could one day be used to create ultra-fast supercomputers that predict complex cell responses to drugs. They may also help researchers to design synthetic genetic circuits in cells. <br /><br />In his new book, <em>Ultra Low Power Bioelectronics</em> (Cambridge University Press, 2010), Sarpeshkar outlines the deep underlying similarities between chemical reactions that occur in a cell and the flow of current through an analog electronic circuit. He discusses how biological cells perform reliable computation with unreliable components and noise (which refers to random variations in signals — whether electronic or genetic). Circuits built with similar design principles in the future can be made robust to electronic noise and unreliable electronic components while remaining highly energy efficient. Promising applications include image processors in cell phones or brain implants for the blind.<br /><br />"Circuits are a language for representing and trying to understand almost anything, whether it be networks in biology or cars," says Sarpeshkar, an associate professor of electrical engineering and computer science. "There's a unified way of looking at the biological world through circuits that is very powerful."<br /><br />Circuit designers already know hundreds of strategies to run analog circuits at low power, amplify signals, and reduce noise, which have helped them design low-power electronics such as mobile phones, mp3 players and laptop computers.<br /><br />"Here's a field that has devoted 50 years to studying the design of complex systems," says Sarpeshkar, referring to electrical engineering. "We can now start to think of biology in the same way." He hopes that physicists, engineers, biologists and biological engineers will work together to pioneer this new field, which he has dubbed "cytomorphic" (cell-inspired or cell-transforming) electronics.<br /><br /><strong>Finding connections</strong><br /><br />Sarpeshkar, an electrical engineer with many years of experience in designing low-power and biomedical circuits, has frequently turned his attention to finding and exploiting links between electronics and biology. In 2009, he designed a <a href="http://web.mit.edu/newsoffice/2009/bio-electronics-0603.html" target="_blank">low-power radio chip</a> that mimics the structure of the human cochlea to separate and process cell phone, Internet, radio and television signals more rapidly and with more energy efficiency than had been believed possible.<br /><br />That chip, known as the RF (radio frequency) cochlea, is an example of "neuromorphic electronics," a 20-year-old field founded by Carver Mead, Sarpeshkar's thesis advisor at Caltech. Neuromorphic circuits mimic biological structures found in the nervous system, such as the cochlea, retina and brain cells.<br /><br />Sarpeshkar's expansion from neuromorphic to cytomorphic electronics is based on his analysis of the equations that govern the dynamics of chemical reactions and the flow of electrons through analog circuits. He has found that those equations, which predict the reaction's (or circuit's) behavior, are astonishingly similar, even in their noise properties. <br /><br />Chemical reactions (for example, the formation of water from hydrogen and oxygen) only occur at a reasonable rate if enough energy is available to lower the barriers that prevent such reactions from occurring. A catalyst such as an enzyme can lower such barriers. Similarly, electrons flowing through a circuit in a transistor exploit input voltage energy to allow them to reduce the barrier for electrons to flow from the transistor's source to the transistor's drain. Changes in the input voltage lower the barrier and increase current flow in transistors, just as adding an enzyme to a chemical reaction speeds it up.<br /><br />Essentially, cells may be viewed as circuits that use molecules, ions, proteins and DNA instead of electrons and transistors. That analogy suggests that it should be possible to build electronic chips — what Sarpeshkar calls "cellular chemical computers" — that mimic chemical reactions very efficiently and on a very fast timescale. <br /><br />One potentially powerful application of such circuits is in modeling genetic network — the interplay of genes and proteins that controls a cell's function and fate. In a paper presented at the 2009 IEEE Symposium on Biological Circuits and Systems, Sarpeshkar designed a circuit that allows any genetic network reaction to be simulated on a chip. For example, circuits can simulate the interactions between genes involved in lactose metabolism and the transcription factors that regulate their expression in bacterial cells. <br /><br />In the long term, Sarpeshkar plans to develop circuits that mimic interactions within entire cellular genomes, which are important in enabling scientists to understand and treat complex diseases such as cancer and diabetes. Eventually, researchers may be able to use such chips to simulate the entire human body, he believes. Such chips would be much faster than computer simulations now, which are highly inefficient at modeling the effects of noise in the large-scale nonlinear circuits within cells. <br /> <br />He is also investigating how circuit design principles can help genetically engineer cells to perform useful functions, for example, the robust and sensitive detection of toxins in the environment.<br /><br />Sarpeshkar's focus on modeling cells as analog rather than digital circuits offers a new approach that will expand the frontiers of synthetic biology, says James Collins, professor of biomedical engineering at Boston University. "Rahul has nicely laid a foundation that many of us in synthetic biology will be able to build on," he says.<br /><br />]]></body>
</item>
</items>